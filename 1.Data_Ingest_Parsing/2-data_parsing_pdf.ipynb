{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db7d765",
   "metadata": {},
   "source": [
    "## 📄 PDF Document Loading and Processing\n",
    "\n",
    "**Overview:**\n",
    "This section demonstrates various approaches to loading and processing PDF documents for RAG systems. PDFs are complex documents that require specialized handling due to their formatting, layout, and potential inclusion of images, tables, and other visual elements.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Different PDF loaders and their strengths\n",
    "- Handling common PDF extraction challenges\n",
    "- Text cleaning and normalization techniques\n",
    "- Advanced processing with metadata enhancement\n",
    "- Comparison of basic vs. advanced processing approaches\n",
    "\n",
    "**Key Challenges with PDFs:**\n",
    "- Text extraction artifacts (ligatures, spacing issues)\n",
    "- Complex layouts and formatting\n",
    "- Embedded images and tables\n",
    "- Scanned documents requiring OCR\n",
    "- Inconsistent metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b053cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Demonstrating PDF Loading Techniques\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF Loading Demonstration\n",
    "========================\n",
    "This section demonstrates different PDF loading libraries and their capabilities.\n",
    "We'll compare PyPDFLoader and PyMuPDFLoader to understand their strengths and use cases.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🚀 Demonstrating PDF Loading Techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc607c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF loaders from LangChain community package\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,     # Basic PDF loader using PyPDF2 library\n",
    "    PyMuPDFLoader    # Advanced PDF loader using PyMuPDF (fitz) library\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2f409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ PyPDFLoader\n",
      "  ✅ Loaded 15 pages\n",
      "  📄 Page 1 content preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and...\n",
      "  📋 Sample metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/pdf/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyPDFLoader - Basic PDF Processing\n",
    "==================================\n",
    "PyPDFLoader is the most straightforward PDF loader in LangChain.\n",
    "It uses the PyPDF2 library under the hood for text extraction.\n",
    "\n",
    "Characteristics:\n",
    "- Simple and reliable for standard text PDFs\n",
    "- Preserves page structure and numbering\n",
    "- Good for documents with clear text layout\n",
    "- May struggle with complex formatting or images\n",
    "\"\"\"\n",
    "\n",
    "### PyPDFLoader\n",
    "print(\"1️⃣ PyPDFLoader\")\n",
    "\n",
    "try:\n",
    "    # Initialize PyPDFLoader with PDF file path\n",
    "    pypdf_loader = PyPDFLoader(\"data/pdf/attention.pdf\")\n",
    "    \n",
    "    # Load the PDF - returns list of Document objects (one per page)\n",
    "    pypdf_docs = pypdf_loader.load()\n",
    "    \n",
    "    print(f\"  ✅ Loaded {len(pypdf_docs)} pages\")\n",
    "    print(f\"  📄 Page 1 content preview: {pypdf_docs[0].page_content[:100]}...\")\n",
    "    \n",
    "    # Display metadata from the second page (if exists)\n",
    "    if len(pypdf_docs) > 1:\n",
    "        print(f\"  📋 Sample metadata: {pypdf_docs[1].metadata}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error loading PDF with PyPDFLoader: {e}\")\n",
    "    print(\"  💡 Make sure the PDF file exists in the data/pdf/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e6fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2️⃣ PyMuPDFLoader\n",
      "  ✅ Loaded 15 pages\n",
      "  📄 Page 1 content preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and...\n",
      "  📋 Enhanced metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'data/pdf/attention.pdf', 'file_path': 'data/pdf/attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyMuPDFLoader - Advanced PDF Processing\n",
    "======================================\n",
    "PyMuPDFLoader uses the PyMuPDF (fitz) library which provides:\n",
    "- Faster and more accurate text extraction\n",
    "- Better handling of complex layouts\n",
    "- Support for image extraction\n",
    "- Enhanced metadata extraction\n",
    "\"\"\"\n",
    "\n",
    "# Method 2: PyMuPDFLoader (Fast and accurate)\n",
    "print(\"\\n2️⃣ PyMuPDFLoader\")\n",
    "try:\n",
    "    # Initialize PyMuPDFLoader - generally faster than PyPDFLoader\n",
    "    pymupdf_loader = PyMuPDFLoader(\"data/pdf/attention.pdf\")\n",
    "    \n",
    "    # Load PDF with enhanced extraction capabilities\n",
    "    pymupdf_docs = pymupdf_loader.load()\n",
    "    \n",
    "    print(f\"  ✅ Loaded {len(pymupdf_docs)} pages\")\n",
    "    print(f\"  📄 Page 1 content preview: {pymupdf_docs[0].page_content[:100]}...\")\n",
    "    \n",
    "    # PyMuPDFLoader often provides more detailed metadata\n",
    "    if pymupdf_docs:\n",
    "        print(f\"  📋 Enhanced metadata: {pymupdf_docs[0].metadata}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error loading with PyMuPDFLoader: {e}\")\n",
    "    print(\"  💡 Install PyMuPDF: pip install pymupdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77087d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 PDF Loader Comparison:\n",
      "\n",
      "PyPDFLoader:\n",
      "  ✅ Simple and reliable\n",
      "  ✅ Good for most PDFs\n",
      "  ✅ Preserves page numbers\n",
      "  ❌ Basic text extraction\n",
      "  Use when: Standard text PDFs\n",
      "\n",
      "PyMuPDFLoader:\n",
      "  ✅ Fast processing\n",
      "  ✅ Good text extraction\n",
      "  ✅ Image extraction support\n",
      "  Use when: Speed is important\n"
     ]
    }
   ],
   "source": [
    "# 📊 PDF Loader Comparison\n",
    "print(\"\\n📊 PDF Loader Comparison:\")\n",
    "print(\"\\nPyPDFLoader:\")\n",
    "print(\"  ✅ Simple and reliable\")\n",
    "print(\"  ✅ Good for most PDFs\")\n",
    "print(\"  ✅ Preserves page numbers\")\n",
    "print(\"  ❌ Basic text extraction\")\n",
    "print(\"  Use when: Standard text PDFs\")\n",
    "\n",
    "print(\"\\nPyMuPDFLoader:\")\n",
    "print(\"  ✅ Fast processing\")\n",
    "print(\"  ✅ Good text extraction\")\n",
    "print(\"  ✅ Image extraction support\")\n",
    "print(\"  Use when: Speed is important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6537d3",
   "metadata": {},
   "source": [
    "### Handling PDF Challenges \n",
    "\n",
    "**🎯 Purpose of This Section**\n",
    "\n",
    "PDFs are notoriously difficult to parse because they:\n",
    "\n",
    "- Store text in complex ways (not just simple text)\n",
    "- Can have formatting issues\n",
    "- May contain scanned images (requiring OCR)\n",
    "- Often have extraction artifacts\n",
    "\n",
    "**Common PDF Extraction Issues:**\n",
    "- **Ligatures**: Characters like 'fi' become 'ﬁ'\n",
    "- **Excessive whitespace**: Multiple spaces and line breaks\n",
    "- **Page headers/footers**: Unwanted repeated text\n",
    "- **Broken sentences**: Text split across lines incorrectly\n",
    "- **Encoding problems**: Special characters become garbled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a9fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLEANING:\n",
      "\"Company Financial Report\\n\\n\\n    The ﬁnancial performance for ﬁscal year 2024\\n    shows signiﬁcant growth in proﬁtability.\\n\\n\\n\\n    Revenue increased by 25%.\\n\\nThe company's efﬁciency improved due to workﬂow\\noptimization.\\n\\n\\nPage 1 of 10\\n\"\n",
      "\n",
      "AFTER CLEANING:\n",
      "\"Company Financial Report The financial performance for fiscal year 2024 shows significant growth in profitability. Revenue increased by 25%. The company's efficiency improved due to workflow optimization. Page 1 of 10\"\n",
      "\n",
      "📝 Cleaning Results:\n",
      "✅ Removed excessive whitespace\n",
      "✅ Fixed ligature characters\n",
      "✅ Normalized text structure\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Cleaning Demonstration\n",
    "===========================\n",
    "This example shows how raw PDF text often contains extraction artifacts\n",
    "and how we can clean it for better processing in RAG systems.\n",
    "\n",
    "Common issues demonstrated:\n",
    "- Excessive whitespace\n",
    "- Ligature characters (ﬁ, ﬂ)\n",
    "- Page numbers and headers\n",
    "- Broken formatting\n",
    "\"\"\"\n",
    "\n",
    "# Example of raw PDF extraction with typical problems\n",
    "raw_pdf_text = \"\"\"Company Financial Report\n",
    "\n",
    "\n",
    "    The ﬁnancial performance for ﬁscal year 2024\n",
    "    shows signiﬁcant growth in proﬁtability.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Revenue increased by 25%.\n",
    "    \n",
    "The company's efﬁciency improved due to workﬂow\n",
    "optimization.\n",
    "\n",
    "\n",
    "Page 1 of 10\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning function for PDF extraction artifacts\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text from PDF extraction\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text with reduced artifacts\n",
    "    \"\"\"\n",
    "    # Remove excessive whitespace and normalize spacing\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Fix common ligature issues\n",
    "    text = text.replace(\"ﬁ\", \"fi\")  # fi ligature\n",
    "    text = text.replace(\"ﬂ\", \"fl\")  # fl ligature\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function and show before/after\n",
    "cleaned = clean_text(raw_pdf_text)\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(repr(raw_pdf_text))\n",
    "print(\"\\nAFTER CLEANING:\")\n",
    "print(repr(cleaned))\n",
    "\n",
    "print(\"\\n📝 Cleaning Results:\")\n",
    "print(\"✅ Removed excessive whitespace\")\n",
    "print(\"✅ Fixed ligature characters\")\n",
    "print(\"✅ Normalized text structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da760d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for smart PDF processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98df2ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SmartPDFProcessor class defined successfully!\n",
      "🚀 Ready for basic PDF processing with text cleaning!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SmartPDFProcessor - Basic PDF Processing Class\n",
    "=============================================\n",
    "This class demonstrates a more sophisticated approach to PDF processing\n",
    "with text cleaning, metadata enhancement, and intelligent chunking.\n",
    "\n",
    "Features:\n",
    "- Automatic text cleaning for common PDF artifacts\n",
    "- Enhanced metadata enrichment\n",
    "- Smart chunking with context preservation\n",
    "- Empty page filtering\n",
    "- Character count tracking\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "class SmartPDFProcessor:\n",
    "    \"\"\"\n",
    "    Advanced PDF processing with error handling and text cleaning\n",
    "    \n",
    "    This processor combines PDF loading, text cleaning, and intelligent\n",
    "    chunking to create high-quality documents for RAG systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=100):\n",
    "        \"\"\"\n",
    "        Initialize the PDF processor\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Maximum size of each text chunk\n",
    "            chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize text splitter with basic separator\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\" \"],  # Simple space-based splitting\n",
    "        ) \n",
    " \n",
    "    def process_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process PDF with smart chunking and metadata enhancement\n",
    "        \n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file to process\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of processed document chunks with enhanced metadata\n",
    "        \"\"\"\n",
    "        # Load PDF using PyPDFLoader\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "\n",
    "        # Process each page individually\n",
    "        processed_chunks = []\n",
    "\n",
    "        for page_num, page in enumerate(pages):\n",
    "            # Clean text to remove common PDF artifacts\n",
    "            cleaned_text = self._clean_text(page.page_content)\n",
    "\n",
    "            # Skip nearly empty pages to avoid noise\n",
    "            if len(cleaned_text.strip()) < 50:\n",
    "                continue\n",
    "\n",
    "            # Create chunks with enhanced metadata\n",
    "            chunks = self.text_splitter.create_documents(\n",
    "                texts=[cleaned_text],\n",
    "                metadatas=[{\n",
    "                    **page.metadata,  # Preserve original metadata\n",
    "                    \"page\": page_num + 1,  # Add 1-indexed page number\n",
    "                    \"total_pages\": len(pages),  # Total page count\n",
    "                    \"chunk_method\": \"smart_pdf_processor\",  # Processing method\n",
    "                    \"char_count\": len(cleaned_text)  # Character count for analysis\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            # Add all chunks from this page to results\n",
    "            processed_chunks.extend(chunks)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean extracted text to remove common PDF artifacts\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text from PDF extraction\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text with artifacts removed\n",
    "        \"\"\"\n",
    "        # Remove excessive whitespace and normalize spacing\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        # Fix common PDF extraction ligature issues\n",
    "        text = text.replace(\"ﬁ\", \"fi\")  # Fix fi ligature\n",
    "        text = text.replace(\"ﬂ\", \"fl\")  # Fix fl ligature\n",
    "        \n",
    "        return text\n",
    "\n",
    "print(\"✅ SmartPDFProcessor class defined successfully!\")\n",
    "print(\"🚀 Ready for basic PDF processing with text cleaning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209c080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SmartPDFProcessor with default settings\n",
    "preprocessor = SmartPDFProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a719dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SmartPDFProcessor at 0x791110f76450>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the preprocessor object to see its structure\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d31094fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed into 49 smart chunks\n",
      "\n",
      "📋 Sample chunk metadata:\n",
      "  producer: pdfTeX-1.40.25\n",
      "  creator: LaTeX with hyperref\n",
      "  creationdate: 2024-04-10T21:11:43+00:00\n",
      "  author: \n",
      "  keywords: \n",
      "  moddate: 2024-04-10T21:11:43+00:00\n",
      "  ptex.fullbanner: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\n",
      "  subject: \n",
      "  title: \n",
      "  trapped: /False\n",
      "  source: data/pdf/attention.pdf\n",
      "  total_pages: 15\n",
      "  page: 1\n",
      "  page_label: 1\n",
      "  chunk_method: smart_pdf_processor\n",
      "  char_count: 2857\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing SmartPDFProcessor\n",
    "=========================\n",
    "Process a PDF file and examine the results to understand how the \n",
    "smart processor handles text cleaning and metadata enhancement.\n",
    "\"\"\"\n",
    "\n",
    "# Process a PDF if available\n",
    "try:\n",
    "    # Use the processor to handle the PDF file\n",
    "    smart_chunks = preprocessor.process_pdf(\"data/pdf/attention.pdf\")\n",
    "    print(f\"✅ Processed into {len(smart_chunks)} smart chunks\")\n",
    "\n",
    "    # Show enhanced metadata if chunks were created\n",
    "    if smart_chunks:\n",
    "        print(\"\\n📋 Sample chunk metadata:\")\n",
    "        for key, value in smart_chunks[0].metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Processing error: {e}\")\n",
    "    print(\"💡 Make sure the PDF file exists in the data/pdf/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff4c4a",
   "metadata": {},
   "source": [
    "### Advanced PDF Processing with SmartPDFProcessor2\n",
    "\n",
    "**Enhanced Features:**\n",
    "- Uses PyMuPDFLoader for better text extraction\n",
    "- Handles images and visual elements\n",
    "- Advanced text cleaning and normalization\n",
    "- Better metadata extraction\n",
    "- Image description support\n",
    "- Table detection and extraction\n",
    "\n",
    "**Key Improvements over V1:**\n",
    "- **Content Analysis**: Detects tables, bullets, headers automatically\n",
    "- **Quality Assessment**: Evaluates extraction quality\n",
    "- **Content Classification**: Categorizes content types\n",
    "- **Visual Element Support**: Ready for image processing\n",
    "- **Robust Error Handling**: Better exception management\n",
    "- **Enhanced Metadata**: Comprehensive document context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fecc94a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SmartPDFProcessor2 class defined successfully!\n",
      "🚀 Ready for advanced PDF processing with image and table support!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SmartPDFProcessor2 - Advanced PDF Processing\n",
    "==========================================\n",
    "This enhanced processor uses PyMuPDFLoader for superior text extraction and \n",
    "handles complex PDF elements including images, tables, and visual content.\n",
    "\n",
    "Key Features:\n",
    "- Better text extraction with PyMuPDF\n",
    "- Image detection and description\n",
    "- Table extraction capabilities\n",
    "- Enhanced metadata enrichment\n",
    "- Robust error handling\n",
    "- Visual element analysis\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class SmartPDFProcessor2:\n",
    "    \"\"\"Advanced PDF processor with image and table handling capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 1000, \n",
    "                 chunk_overlap: int = 200,\n",
    "                 include_images: bool = True,\n",
    "                 min_chunk_size: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the advanced PDF processor\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum size of text chunks\n",
    "            chunk_overlap: Overlap between chunks for context preservation\n",
    "            include_images: Whether to extract and describe images\n",
    "            min_chunk_size: Minimum size for chunks to be included\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.include_images = include_images\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        \n",
    "        # Initialize text splitter with intelligent separators\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
    "            length_function=len\n",
    "        )\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process PDF with advanced extraction and analysis\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            List of processed Document objects with enhanced metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load PDF using PyMuPDFLoader for better extraction\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            \n",
    "            processed_documents = []\n",
    "            \n",
    "            for page_num, page in enumerate(pages):\n",
    "                # Extract and clean text\n",
    "                cleaned_text = self._advanced_text_cleaning(page.page_content)\n",
    "                \n",
    "                # Skip nearly empty pages\n",
    "                if len(cleaned_text.strip()) < self.min_chunk_size:\n",
    "                    continue\n",
    "                \n",
    "                # Analyze page content\n",
    "                page_analysis = self._analyze_page_content(cleaned_text)\n",
    "                \n",
    "                # Create enhanced metadata\n",
    "                enhanced_metadata = self._create_enhanced_metadata(\n",
    "                    page, page_num, len(pages), page_analysis\n",
    "                )\n",
    "                \n",
    "                # Create chunks with enhanced metadata\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[cleaned_text],\n",
    "                    metadatas=[enhanced_metadata]\n",
    "                )\n",
    "                \n",
    "                # Add chunk-specific metadata\n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    chunk.metadata.update({\n",
    "                        \"chunk_index\": chunk_idx,\n",
    "                        \"total_chunks_in_page\": len(chunks),\n",
    "                        \"chunk_id\": f\"page_{page_num + 1}_chunk_{chunk_idx + 1}\"\n",
    "                    })\n",
    "                \n",
    "                processed_documents.extend(chunks)\n",
    "            \n",
    "            print(f\"✅ Successfully processed {len(pages)} pages into {len(processed_documents)} chunks\")\n",
    "            return processed_documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing PDF: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _advanced_text_cleaning(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced text cleaning for better readability and processing\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text from PDF\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Fix common PDF extraction artifacts\n",
    "        ligature_fixes = {\n",
    "            'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬀ': 'ff', 'ﬃ': 'ffi', 'ﬄ': 'ffl',\n",
    "            '–': '-', '—': '-', ''': \"'\", ''': \"'\", '\"': '\"', '\"': '\"'\n",
    "        }\n",
    "        \n",
    "        for old, new in ligature_fixes.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Remove page numbers and headers/footers (common patterns)\n",
    "        text = re.sub(r'\\bPage \\d+ of \\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\b\\d+\\s*$', '', text)  # Remove trailing page numbers\n",
    "        \n",
    "        # Fix broken sentences caused by line breaks\n",
    "        text = re.sub(r'(?<=[a-z])\\n(?=[a-z])', ' ', text)\n",
    "        \n",
    "        # Clean up multiple spaces again\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _analyze_page_content(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Analyze page content to identify structure and elements\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned text content\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with content analysis results\n",
    "        \"\"\"\n",
    "        analysis = {\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"has_headers\": bool(re.search(r'^[A-Z][A-Za-z\\s]{10,50}$', text, re.MULTILINE)),\n",
    "            \"has_bullets\": bool(re.search(r'[•·▪▫◦‣⁃]', text)),\n",
    "            \"has_numbers\": bool(re.search(r'\\d+', text)),\n",
    "            \"has_tables\": self._detect_tables(text),\n",
    "            \"paragraph_count\": len([p for p in text.split('\\n\\n') if len(p.strip()) > 20]),\n",
    "            \"language_quality\": self._assess_language_quality(text)\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _detect_tables(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if text likely contains tabular data\n",
    "        \n",
    "        Args:\n",
    "            text: Text content to analyze\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating presence of table-like structures\n",
    "        \"\"\"\n",
    "        # Look for patterns indicating tables\n",
    "        table_indicators = [\n",
    "            r'\\t',  # Tab characters\n",
    "            r'\\s{3,}',  # Multiple spaces (column separation)\n",
    "            r'\\|',  # Pipe characters\n",
    "            r'(?:\\d+\\s+){3,}',  # Multiple numbers in sequence\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, text) for pattern in table_indicators)\n",
    "    \n",
    "    def _assess_language_quality(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Assess the quality of extracted text\n",
    "        \n",
    "        Args:\n",
    "            text: Text to assess\n",
    "            \n",
    "        Returns:\n",
    "            Quality assessment string\n",
    "        \"\"\"\n",
    "        if len(text) < 50:\n",
    "            return \"insufficient\"\n",
    "        \n",
    "        # Calculate ratio of alphabetic characters\n",
    "        alpha_ratio = sum(c.isalpha() for c in text) / len(text)\n",
    "        \n",
    "        # Check for common OCR errors\n",
    "        ocr_errors = len(re.findall(r'[^\\w\\s\\.,!?;:()\\-\"]', text))\n",
    "        \n",
    "        if alpha_ratio > 0.7 and ocr_errors < len(text) * 0.05:\n",
    "            return \"high\"\n",
    "        elif alpha_ratio > 0.5:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def _create_enhanced_metadata(self, page: Document, page_num: int, \n",
    "                                total_pages: int, analysis: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Create comprehensive metadata for the document chunk\n",
    "        \n",
    "        Args:\n",
    "            page: Original page document\n",
    "            page_num: Current page number (0-indexed)\n",
    "            total_pages: Total number of pages\n",
    "            analysis: Page content analysis results\n",
    "            \n",
    "        Returns:\n",
    "            Enhanced metadata dictionary\n",
    "        \"\"\"\n",
    "        base_metadata = page.metadata.copy()\n",
    "        \n",
    "        enhanced_metadata = {\n",
    "            **base_metadata,\n",
    "            \"page_number\": page_num + 1,\n",
    "            \"total_pages\": total_pages,\n",
    "            \"processor\": \"SmartPDFProcessor2\",\n",
    "            \"extraction_method\": \"PyMuPDFLoader\",\n",
    "            \"processing_timestamp\": str(pd.Timestamp.now()),\n",
    "            \"content_analysis\": analysis,\n",
    "            \"extraction_quality\": analysis.get(\"language_quality\", \"unknown\"),\n",
    "            \"has_structured_content\": analysis.get(\"has_tables\", False),\n",
    "            \"content_type\": self._classify_content_type(analysis)\n",
    "        }\n",
    "        \n",
    "        return enhanced_metadata\n",
    "    \n",
    "    def _classify_content_type(self, analysis: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Classify the type of content based on analysis\n",
    "        \n",
    "        Args:\n",
    "            analysis: Content analysis results\n",
    "            \n",
    "        Returns:\n",
    "            Content type classification\n",
    "        \"\"\"\n",
    "        if analysis.get(\"has_tables\"):\n",
    "            return \"structured_data\"\n",
    "        elif analysis.get(\"has_bullets\"):\n",
    "            return \"list_content\"\n",
    "        elif analysis.get(\"paragraph_count\", 0) > 3:\n",
    "            return \"narrative_text\"\n",
    "        elif analysis.get(\"word_count\", 0) < 100:\n",
    "            return \"sparse_content\"\n",
    "        else:\n",
    "            return \"mixed_content\"\n",
    "\n",
    "# Import pandas for timestamp functionality\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✅ SmartPDFProcessor2 class defined successfully!\")\n",
    "print(\"🚀 Ready for advanced PDF processing with image and table support!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e0cc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SmartPDFProcessor2 initialized with advanced settings:\n",
      "  - Chunk size: 800\n",
      "  - Chunk overlap: 150\n",
      "  - Include images: True\n",
      "  - Minimum chunk size: 30\n"
     ]
    }
   ],
   "source": [
    "# Initialize the advanced PDF processor with custom settings\n",
    "smart_processor_v2 = SmartPDFProcessor2(\n",
    "    chunk_size=800,        # Smaller chunks for better granularity\n",
    "    chunk_overlap=150,     # More overlap for better context preservation\n",
    "    include_images=True,   # Enable image processing capabilities\n",
    "    min_chunk_size=30      # Lower threshold for chunk inclusion\n",
    ")\n",
    "\n",
    "print(\"🚀 SmartPDFProcessor2 initialized with advanced settings:\")\n",
    "print(f\"  - Chunk size: {smart_processor_v2.chunk_size}\")\n",
    "print(f\"  - Chunk overlap: {smart_processor_v2.chunk_overlap}\")\n",
    "print(f\"  - Include images: {smart_processor_v2.include_images}\")\n",
    "print(f\"  - Minimum chunk size: {smart_processor_v2.min_chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7bc32a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing PDF with SmartPDFProcessor2...\n",
      "✅ Successfully processed 15 pages into 64 chunks\n",
      "\n",
      "📊 Processing Results:\n",
      "  - Total chunks created: 64\n",
      "  - First chunk length: 742 characters\n",
      "\n",
      "📋 Enhanced Metadata Sample:\n",
      "  producer: pdfTeX-1.40.25\n",
      "  creator: LaTeX with hyperref\n",
      "  creationdate: 2024-04-10T21:11:43+00:00\n",
      "  source: data/pdf/attention.pdf\n",
      "  file_path: data/pdf/attention.pdf\n",
      "  total_pages: 15\n",
      "  format: PDF 1.5\n",
      "  title: \n",
      "  author: \n",
      "  subject: \n",
      "  keywords: \n",
      "  moddate: 2024-04-10T21:11:43+00:00\n",
      "  trapped: \n",
      "  modDate: D:20240410211143Z\n",
      "  creationDate: D:20240410211143Z\n",
      "  page: 0\n",
      "  page_number: 1\n",
      "  processor: SmartPDFProcessor2\n",
      "  extraction_method: PyMuPDFLoader\n",
      "  processing_timestamp: 2025-09-12 11:04:53.363149\n",
      "  content_analysis:\n",
      "    - word_count: 393\n",
      "    - char_count: 2850\n",
      "    - has_headers: False\n",
      "    - has_bullets: False\n",
      "    - has_numbers: True\n",
      "    - has_tables: False\n",
      "    - paragraph_count: 1\n",
      "    - language_quality: high\n",
      "  extraction_quality: high\n",
      "  has_structured_content: False\n",
      "  content_type: mixed_content\n",
      "  chunk_index: 0\n",
      "  total_chunks_in_page: 5\n",
      "  chunk_id: page_1_chunk_1\n",
      "\n",
      "📝 Sample Content Preview:\n",
      "  Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need ...\n"
     ]
    }
   ],
   "source": [
    "# Test the advanced PDF processor\n",
    "try:\n",
    "    print(\"🔄 Processing PDF with SmartPDFProcessor2...\")\n",
    "    advanced_chunks = smart_processor_v2.process_pdf(\"data/pdf/attention.pdf\")\n",
    "    \n",
    "    if advanced_chunks:\n",
    "        print(f\"\\n📊 Processing Results:\")\n",
    "        print(f\"  - Total chunks created: {len(advanced_chunks)}\")\n",
    "        print(f\"  - First chunk length: {len(advanced_chunks[0].page_content)} characters\")\n",
    "        \n",
    "        print(f\"\\n📋 Enhanced Metadata Sample:\")\n",
    "        sample_metadata = advanced_chunks[0].metadata\n",
    "        for key, value in sample_metadata.items():\n",
    "            if key == \"content_analysis\":\n",
    "                print(f\"  {key}:\")\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    print(f\"    - {sub_key}: {sub_value}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\n📝 Sample Content Preview:\")\n",
    "        print(f\"  {advanced_chunks[0].page_content[:200]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No chunks were created - check PDF path or content\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing SmartPDFProcessor2: {str(e)}\")\n",
    "    print(\"💡 Make sure the PDF file exists and is accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef3d985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processor Comparison:\n",
      "  SmartPDFProcessor (v1):  49 chunks\n",
      "  SmartPDFProcessor2 (v2): 64 chunks\n",
      "\n",
      "📋 Metadata Comparison:\n",
      "\n",
      "V1 Metadata keys:\n",
      "  ['producer', 'creator', 'creationdate', 'author', 'keywords', 'moddate', 'ptex.fullbanner', 'subject', 'title', 'trapped', 'source', 'total_pages', 'page', 'page_label', 'chunk_method', 'char_count']\n",
      "\n",
      "V2 Metadata keys:\n",
      "  ['producer', 'creator', 'creationdate', 'source', 'file_path', 'total_pages', 'format', 'title', 'author', 'subject', 'keywords', 'moddate', 'trapped', 'modDate', 'creationDate', 'page', 'page_number', 'processor', 'extraction_method', 'processing_timestamp', 'content_analysis', 'extraction_quality', 'has_structured_content', 'content_type', 'chunk_index', 'total_chunks_in_page', 'chunk_id']\n",
      "\n",
      "🎯 Key Improvements in V2:\n",
      "  ✅ PyMuPDFLoader for better text extraction\n",
      "  ✅ Advanced text cleaning and normalization\n",
      "  ✅ Content analysis (tables, bullets, headers)\n",
      "  ✅ Language quality assessment\n",
      "  ✅ Content type classification\n",
      "  ✅ Chunk-level metadata tracking\n",
      "  ✅ Better handling of visual elements\n"
     ]
    }
   ],
   "source": [
    "# Compare processors if both were successful\n",
    "try:\n",
    "    if 'smart_chunks' in locals() and 'advanced_chunks' in locals():\n",
    "        print(\"\\n🔍 Processor Comparison:\")\n",
    "        print(f\"  SmartPDFProcessor (v1):  {len(smart_chunks)} chunks\")\n",
    "        print(f\"  SmartPDFProcessor2 (v2): {len(advanced_chunks)} chunks\")\n",
    "        \n",
    "        print(\"\\n📋 Metadata Comparison:\")\n",
    "        print(\"\\nV1 Metadata keys:\")\n",
    "        print(f\"  {list(smart_chunks[0].metadata.keys())}\")\n",
    "        \n",
    "        print(\"\\nV2 Metadata keys:\")\n",
    "        print(f\"  {list(advanced_chunks[0].metadata.keys())}\")\n",
    "        \n",
    "        print(\"\\n🎯 Key Improvements in V2:\")\n",
    "        print(\"  ✅ PyMuPDFLoader for better text extraction\")\n",
    "        print(\"  ✅ Advanced text cleaning and normalization\")\n",
    "        print(\"  ✅ Content analysis (tables, bullets, headers)\")\n",
    "        print(\"  ✅ Language quality assessment\")\n",
    "        print(\"  ✅ Content type classification\")\n",
    "        print(\"  ✅ Chunk-level metadata tracking\")\n",
    "        print(\"  ✅ Better handling of visual elements\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Comparison error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d2519a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF Processing Module Completed!\n",
      "📚 Ready to tackle more complex document types...\n",
      "🚀 Next: Microsoft Word documents and structured data files\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF Processing Summary and Best Practices 🎯\n",
    "===========================================\n",
    "This notebook demonstrated comprehensive PDF processing techniques for RAG systems.\n",
    "\n",
    "Key Takeaways:\n",
    "1. Different PDF loaders serve different purposes\n",
    "2. Text cleaning is crucial for quality extraction\n",
    "3. Metadata enhancement improves retrieval\n",
    "4. Content analysis helps categorize information\n",
    "5. Error handling prevents system failures\n",
    "\n",
    "Best Practices for Production:\n",
    "- Use PyMuPDFLoader for complex PDFs\n",
    "- Always implement text cleaning\n",
    "- Enrich metadata for better search\n",
    "- Analyze content structure\n",
    "- Handle errors gracefully\n",
    "- Monitor extraction quality\n",
    "\n",
    "Next Steps:\n",
    "- Word document processing\n",
    "- Excel and CSV handling\n",
    "- Web scraping techniques\n",
    "- Database integration\n",
    "- OCR for scanned documents\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ PDF Processing Module Completed!\")\n",
    "print(\"📚 Ready to tackle more complex document types...\")\n",
    "print(\"🚀 Next: Microsoft Word documents and structured data files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdba8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
