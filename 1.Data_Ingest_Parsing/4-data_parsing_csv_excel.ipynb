{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2e6485",
   "metadata": {},
   "source": [
    "# CSV and Excel Processing with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive approaches to loading and processing structured data files (CSV and Excel) using LangChain's document loaders. We'll explore multiple strategies for converting tabular data into documents suitable for RAG applications.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **CSV Processing** - Different strategies for converting CSV data to documents\n",
    "2. **Excel Processing** - Handling multi-sheet Excel files and complex data structures\n",
    "3. **Custom Processing** - Building intelligent document loaders for structured data\n",
    "4. **Best Practices** - Optimizing structured data for vector search and retrieval\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "uv add install langchain-community pandas openpyxl xlrd\n",
    "```\n",
    "\n",
    "## Use Cases\n",
    "- Product catalogs and inventory systems\n",
    "- Financial data and reports\n",
    "- Customer databases\n",
    "- Survey and research data\n",
    "- Any tabular data for Q&A systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cfe8fd",
   "metadata": {},
   "source": [
    "### CSV And EXcel files- Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453e1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directory structure created successfully\n",
      "üìÅ Ready to process CSV and Excel files\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CSV and Excel Data Processing Setup\n",
    "\n",
    "This module sets up the environment for processing structured data files.\n",
    "We'll create sample data and demonstrate different loading strategies.\n",
    "\n",
    "Author: Data Science Team\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries for data manipulation and file handling\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import os  # For operating system interface and directory operations\n",
    "\n",
    "# Create directory structure for storing our sample files\n",
    "# exist_ok=True prevents error if directory already exists\n",
    "os.makedirs(\"data/structured_files\", exist_ok=True)\n",
    "print(\"‚úÖ Directory structure created successfully\")\n",
    "print(\"üìÅ Ready to process CSV and Excel files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5bc269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created sample CSV file: data/structured_files/products.csv\n",
      "üìä Data shape: 5 rows, 5 columns\n",
      "üìù Sample data preview:\n",
      "  Product     Category   Price  Stock  \\\n",
      "0  Laptop  Electronics  999.99     50   \n",
      "1   Mouse  Accessories   29.99    200   \n",
      "\n",
      "                                         Description  \n",
      "0  High-performance laptop with 16GB RAM and 512G...  \n",
      "1       Wireless optical mouse with ergonomic design  \n"
     ]
    }
   ],
   "source": [
    "# Creating Sample Product Data\n",
    "# ============================\n",
    "\"\"\"\n",
    "This section creates realistic sample data for demonstrating CSV and Excel processing.\n",
    "The data structure represents a typical product inventory system with various data types.\n",
    "\"\"\"\n",
    "\n",
    "# Define sample product data with realistic business information\n",
    "# This structure represents common e-commerce or inventory data\n",
    "data = {\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],  # Product names\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Electronics'],  # Categories\n",
    "    'Price': [999.99, 29.99, 79.99, 299.99, 89.99],  # Prices in USD\n",
    "    'Stock': [50, 200, 150, 75, 100],  # Available quantities\n",
    "    'Description': [  # Detailed product descriptions for text processing\n",
    "        'High-performance laptop with 16GB RAM and 512GB SSD',\n",
    "        'Wireless optical mouse with ergonomic design',\n",
    "        'Mechanical keyboard with RGB backlighting',\n",
    "        '27-inch 4K monitor with HDR support',\n",
    "        '1080p webcam with noise cancellation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dictionary to pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data as CSV file - most common format for structured data\n",
    "csv_file_path = 'data/structured_files/products.csv'\n",
    "df.to_csv(csv_file_path, index=False)  # index=False prevents adding row numbers\n",
    "\n",
    "print(f\"‚úÖ Created sample CSV file: {csv_file_path}\")\n",
    "print(f\"üìä Data shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"üìù Sample data preview:\")\n",
    "print(df.head(2))  # Show first 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec416eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created Excel file: data/structured_files/inventory.xlsx\n",
      "üìä Created with 2 sheets: 'Products' and 'Summary'\n",
      "üí° This simulates real-world multi-sheet business files\n"
     ]
    }
   ],
   "source": [
    "# Creating Excel File with Multiple Sheets\n",
    "# =========================================\n",
    "\"\"\"\n",
    "This section demonstrates creating Excel files with multiple worksheets.\n",
    "Multi-sheet Excel files are common in business environments and require\n",
    "special handling for document processing.\n",
    "\"\"\"\n",
    "\n",
    "# Create Excel file with multiple sheets using ExcelWriter\n",
    "excel_file_path = 'data/structured_files/inventory.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Product details (main data)\n",
    "    df.to_excel(writer, sheet_name='Products', index=False)\n",
    "    \n",
    "    # Sheet 2: Category summary (aggregated data)\n",
    "    # This demonstrates how real business files often contain multiple data views\n",
    "    summary_data = {\n",
    "        'Category': ['Electronics', 'Accessories'],\n",
    "        'Total_Items': [3, 2],  # Count of products per category\n",
    "        'Total_Value': [1389.97, 109.98]  # Sum of prices per category\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "print(f\"‚úÖ Created Excel file: {excel_file_path}\")\n",
    "print(f\"üìä Created with 2 sheets: 'Products' and 'Summary'\")\n",
    "print(f\"üí° This simulates real-world multi-sheet business files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac11a0f",
   "metadata": {},
   "source": [
    "## CSV Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f7787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain Document Loaders for Structured Data\n",
    "# =====================================================\n",
    "\"\"\"\n",
    "This section imports the necessary document loaders for processing CSV files.\n",
    "Different loaders provide different approaches to converting tabular data into documents.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,  # Standard CSV loader - converts each row to a document\n",
    "    UnstructuredCSVLoader  # Advanced CSV loader with more flexible parsing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb8832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CSVLoader - Row-based Documents\n",
      "----------------------------------------\n",
      "‚úÖ Successfully loaded 5 documents (one per row)\n",
      "üìä Total documents: 5\n",
      "\n",
      "üìÑ First document preview:\n",
      "Content: Product: Laptop\n",
      "Category: Electronics\n",
      "Price: 999.99\n",
      "Stock: 50\n",
      "Description: High-performance laptop with 16GB RAM and 512GB SSD\n",
      "Metadata: {'source': 'data/structured_files/products.csv', 'row': 0}\n",
      "\n",
      "üìà Statistics:\n",
      "  ‚Ä¢ Average document length: 116.8 characters\n",
      "  ‚Ä¢ Document source: data/structured_files/products.csv\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using CSVLoader - Row-based Document Creation\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "CSVLoader is the simplest approach to convert CSV data into documents.\n",
    "Each row becomes a separate document with all columns as content.\n",
    "\n",
    "Pros: Simple, fast, preserves all data\n",
    "Cons: May create verbose documents, limited customization\n",
    "\"\"\"\n",
    "\n",
    "print(\"1Ô∏è‚É£ CSVLoader - Row-based Documents\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Initialize CSVLoader with file path and configuration\n",
    "    csv_loader = CSVLoader(\n",
    "        file_path='data/structured_files/products.csv',\n",
    "        encoding='utf-8',  # Ensure proper character encoding\n",
    "        csv_args={\n",
    "            'delimiter': ',',    # Comma-separated values\n",
    "            'quotechar': '\"',   # Quote character for text fields\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Load documents - each row becomes a Document object\n",
    "    csv_docs = csv_loader.load()\n",
    "    \n",
    "    # Display results and analysis\n",
    "    print(f\"‚úÖ Successfully loaded {len(csv_docs)} documents (one per row)\")\n",
    "    print(f\"üìä Total documents: {len(csv_docs)}\")\n",
    "    \n",
    "    # Show first document structure\n",
    "    print(f\"\\nüìÑ First document preview:\")\n",
    "    print(f\"Content: {csv_docs[0].page_content}\")\n",
    "    print(f\"Metadata: {csv_docs[0].metadata}\")\n",
    "    \n",
    "    # Analyze document characteristics\n",
    "    avg_length = sum(len(doc.page_content) for doc in csv_docs) / len(csv_docs)\n",
    "    print(f\"\\nüìà Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Average document length: {avg_length:.1f} characters\")\n",
    "    print(f\"  ‚Ä¢ Document source: {csv_docs[0].metadata.get('source', 'unknown')}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: CSV file not found. Please run the data creation cells first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CSV: {e}\")\n",
    "    print(\"üí° Tip: Ensure 'langchain-community' is installed: pip install langchain-community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4c6a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Custom CSV Processing\n",
      "------------------------------\n",
      "‚úÖ Custom processing completed\n",
      "üìä Created 5 documents\n",
      "\n",
      "üìÑ Custom document example:\n",
      "Content:\n",
      "Product Information:\n",
      "Name: Laptop\n",
      "Category: Electronics\n",
      "Price: $999.99\n",
      "Stock: 50 units\n",
      "Description: High-performance laptop with 16GB RAM and 512GB SSD\n",
      "\n",
      "Metadata keys: ['source', 'row_index', 'product_name', 'category', 'price', 'stock_level', 'data_type', 'content_type']\n",
      "\n",
      "üîç Content Quality Analysis:\n",
      "  ‚Ä¢ Structured format: ‚úÖ Human-readable\n",
      "  ‚Ä¢ Rich metadata: ‚úÖ 8 fields\n",
      "  ‚Ä¢ Search optimization: ‚úÖ Key-value format\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Custom CSV Processing for Enhanced Control\n",
    "# ===================================================\n",
    "\"\"\"\n",
    "This section demonstrates custom CSV processing that provides more control over\n",
    "how tabular data is converted into documents for RAG applications.\n",
    "\n",
    "This approach allows for:\n",
    "- Intelligent content formatting\n",
    "- Rich metadata creation\n",
    "- Custom document structures\n",
    "- Better optimization for vector search\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Custom CSV Processing\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def process_csv_intelligently(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process CSV with intelligent document creation strategy.\n",
    "    \n",
    "    This function creates structured documents from CSV data with enhanced\n",
    "    formatting and metadata for better RAG performance.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file to process\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of Document objects with structured content\n",
    "        \n",
    "    Features:\n",
    "        - Structured content formatting for better readability\n",
    "        - Rich metadata for filtering and search\n",
    "        - Product-specific document creation\n",
    "        - Optimized for Q&A applications\n",
    "    \n",
    "    Example:\n",
    "        docs = process_csv_intelligently(\"products.csv\")\n",
    "        print(f\"Created {len(docs)} documents\")\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(filepath)\n",
    "    documents = []\n",
    "    \n",
    "    # Strategy: Create one document per row with structured content\n",
    "    for idx, row in df.iterrows():\n",
    "        # Create human-readable, structured content\n",
    "        # This format is optimized for LLM understanding and retrieval\n",
    "        content = f\"\"\"Product Information:\n",
    "Name: {row['Product']}\n",
    "Category: {row['Category']}\n",
    "Price: ${row['Price']}\n",
    "Stock: {row['Stock']} units\n",
    "Description: {row['Description']}\"\"\"\n",
    "        \n",
    "        # Create document with comprehensive metadata\n",
    "        # Metadata enables filtering, categorization, and advanced retrieval\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                'source': filepath,\n",
    "                'row_index': idx,\n",
    "                'product_name': row['Product'],\n",
    "                'category': row['Category'],\n",
    "                'price': row['Price'],\n",
    "                'stock_level': row['Stock'],\n",
    "                'data_type': 'product_info',\n",
    "                'content_type': 'structured_data'\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test the custom processing function\n",
    "try:\n",
    "    custom_docs = process_csv_intelligently('data/structured_files/products.csv')\n",
    "    \n",
    "    print(f\"‚úÖ Custom processing completed\")\n",
    "    print(f\"üìä Created {len(custom_docs)} documents\")\n",
    "    \n",
    "    # Show example of improved document structure\n",
    "    print(f\"\\nüìÑ Custom document example:\")\n",
    "    print(f\"Content:\\n{custom_docs[0].page_content}\")\n",
    "    print(f\"\\nMetadata keys: {list(custom_docs[0].metadata.keys())}\")\n",
    "    \n",
    "    # Compare content quality\n",
    "    print(f\"\\nüîç Content Quality Analysis:\")\n",
    "    print(f\"  ‚Ä¢ Structured format: ‚úÖ Human-readable\")\n",
    "    print(f\"  ‚Ä¢ Rich metadata: ‚úÖ {len(custom_docs[0].metadata)} fields\")\n",
    "    print(f\"  ‚Ä¢ Search optimization: ‚úÖ Key-value format\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in custom processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea5d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/structured_files/products.csv', 'row_index': 0, 'product_name': 'Laptop', 'category': 'Electronics', 'price': 999.99, 'stock_level': 50, 'data_type': 'product_info', 'content_type': 'structured_data'}, page_content='Product Information:\\nName: Laptop\\nCategory: Electronics\\nPrice: $999.99\\nStock: 50 units\\nDescription: High-performance laptop with 16GB RAM and 512GB SSD'),\n",
       " Document(metadata={'source': 'data/structured_files/products.csv', 'row_index': 1, 'product_name': 'Mouse', 'category': 'Accessories', 'price': 29.99, 'stock_level': 200, 'data_type': 'product_info', 'content_type': 'structured_data'}, page_content='Product Information:\\nName: Mouse\\nCategory: Accessories\\nPrice: $29.99\\nStock: 200 units\\nDescription: Wireless optical mouse with ergonomic design'),\n",
       " Document(metadata={'source': 'data/structured_files/products.csv', 'row_index': 2, 'product_name': 'Keyboard', 'category': 'Accessories', 'price': 79.99, 'stock_level': 150, 'data_type': 'product_info', 'content_type': 'structured_data'}, page_content='Product Information:\\nName: Keyboard\\nCategory: Accessories\\nPrice: $79.99\\nStock: 150 units\\nDescription: Mechanical keyboard with RGB backlighting'),\n",
       " Document(metadata={'source': 'data/structured_files/products.csv', 'row_index': 3, 'product_name': 'Monitor', 'category': 'Electronics', 'price': 299.99, 'stock_level': 75, 'data_type': 'product_info', 'content_type': 'structured_data'}, page_content='Product Information:\\nName: Monitor\\nCategory: Electronics\\nPrice: $299.99\\nStock: 75 units\\nDescription: 27-inch 4K monitor with HDR support'),\n",
       " Document(metadata={'source': 'data/structured_files/products.csv', 'row_index': 4, 'product_name': 'Webcam', 'category': 'Electronics', 'price': 89.99, 'stock_level': 100, 'data_type': 'product_info', 'content_type': 'structured_data'}, page_content='Product Information:\\nName: Webcam\\nCategory: Electronics\\nPrice: $89.99\\nStock: 100 units\\nDescription: 1080p webcam with noise cancellation')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the custom CSV processing function directly\n",
    "# This will return the list of documents for inspection\n",
    "process_csv_intelligently('data/structured_files/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e215d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CSV Processing Strategies Comparison\n",
      "=============================================\n",
      "\n",
      "1Ô∏è‚É£ Row-based Processing (CSVLoader):\n",
      "  ‚úÖ Simple one-row-one-document mapping\n",
      "  ‚úÖ Good for record lookups and searches\n",
      "  ‚úÖ Fast processing with minimal overhead\n",
      "  ‚ùå Loses table structure and relationships\n",
      "  ‚ùå May create verbose or redundant content\n",
      "\n",
      "2Ô∏è‚É£ Intelligent Custom Processing:\n",
      "  ‚úÖ Preserves data relationships and context\n",
      "  ‚úÖ Creates structured, readable content\n",
      "  ‚úÖ Rich metadata for advanced filtering\n",
      "  ‚úÖ Better performance in Q&A systems\n",
      "  ‚ö†Ô∏è  Requires more development effort\n",
      "\n",
      "üí° Recommendation:\n",
      "  ‚Ä¢ Use CSVLoader for: Simple data extraction and basic search\n",
      "  ‚Ä¢ Use Custom Processing for: Production RAG systems and complex queries\n"
     ]
    }
   ],
   "source": [
    "# CSV Processing Strategies Comparison\n",
    "# ====================================\n",
    "\"\"\"\n",
    "This section compares different CSV processing approaches to help you\n",
    "choose the right strategy for your specific use case.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä CSV Processing Strategies Comparison\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Row-based Processing (CSVLoader):\")\n",
    "print(\"  ‚úÖ Simple one-row-one-document mapping\")\n",
    "print(\"  ‚úÖ Good for record lookups and searches\")\n",
    "print(\"  ‚úÖ Fast processing with minimal overhead\")\n",
    "print(\"  ‚ùå Loses table structure and relationships\")\n",
    "print(\"  ‚ùå May create verbose or redundant content\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Intelligent Custom Processing:\")\n",
    "print(\"  ‚úÖ Preserves data relationships and context\")\n",
    "print(\"  ‚úÖ Creates structured, readable content\")\n",
    "print(\"  ‚úÖ Rich metadata for advanced filtering\")\n",
    "print(\"  ‚úÖ Better performance in Q&A systems\")\n",
    "print(\"  ‚ö†Ô∏è  Requires more development effort\")\n",
    "\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "print(\"  ‚Ä¢ Use CSVLoader for: Simple data extraction and basic search\")\n",
    "print(\"  ‚Ä¢ Use Custom Processing for: Production RAG systems and complex queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af529",
   "metadata": {},
   "source": [
    "## Excel Processing\n",
    "\n",
    "Excel files present unique challenges for document processing:\n",
    "- Multiple worksheets with different data structures\n",
    "- Complex formatting and merged cells\n",
    "- Mixed data types within sheets\n",
    "- Metadata embedded in sheet structure\n",
    "\n",
    "We'll explore different strategies for handling these complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d19045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ UnstructuredExcelLoader - File-level Processing\n",
      "--------------------------------------------------\n",
      "‚ùå Import Error: Missing dependencies\n",
      "üí° Install required packages: pip install unstructured openpyxl\n",
      "‚ùå Import Error: Missing dependencies\n",
      "üí° Install required packages: pip install unstructured openpyxl\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using UnstructuredExcelLoader\n",
    "# =======================================\n",
    "\"\"\"\n",
    "UnstructuredExcelLoader can handle Excel files with multiple sheets,\n",
    "but it treats the entire file as a single document unit.\n",
    "\n",
    "Pros: Simple setup, handles multiple sheets automatically\n",
    "Cons: Limited control over sheet-specific processing\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "\n",
    "print(\"1Ô∏è‚É£ UnstructuredExcelLoader - File-level Processing\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Initialize the Excel loader\n",
    "    excel_loader = UnstructuredExcelLoader('data/structured_files/inventory.xlsx')\n",
    "    \n",
    "    # Load the entire Excel file as documents\n",
    "    excel_docs = excel_loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded Excel file\")\n",
    "    print(f\"üìä Number of documents: {len(excel_docs)}\")\n",
    "    print(f\"üìÑ Content preview (first 300 chars):\")\n",
    "    print(f\"{excel_docs[0].page_content[:300]}...\")\n",
    "    print(f\"üè∑Ô∏è  Metadata: {excel_docs[0].metadata}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå Import Error: Missing dependencies\")\n",
    "    print(\"üí° Install required packages: pip install unstructured openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95e33220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Custom Excel Processing - Sheet-aware\n",
      "---------------------------------------------\n",
      "üìä Processing sheet: 'Products'\n",
      "üìä Processing sheet: 'Summary'\n",
      "\n",
      "‚úÖ Custom Excel processing completed\n",
      "üìä Created 6 documents from Excel sheets\n",
      "\n",
      "üìã Sheet Processing Summary:\n",
      "  ‚Ä¢ Products: 5 documents (product_catalog)\n",
      "  ‚Ä¢ Summary: 1 documents (summary_report)\n",
      "\n",
      "üìÑ Example document from Products sheet:\n",
      "Content:\n",
      "Product Record from Products Sheet:\n",
      "Product: Laptop\n",
      "Category: Electronics\n",
      "Price: $999.99\n",
      "Stock: 50 units\n",
      "Description: High-performance laptop with 16GB RAM and 512GB SSD\n",
      "Metadata keys: ['source', 'sheet_name', 'sheet_type', 'row_index', 'product_name', 'category', 'data_type']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Custom Excel Processing with Sheet-Specific Handling\n",
    "# =============================================================\n",
    "\"\"\"\n",
    "This approach provides granular control over each Excel sheet,\n",
    "allowing for different processing strategies per sheet type.\n",
    "\n",
    "Benefits:\n",
    "- Sheet-aware document creation\n",
    "- Contextual metadata for each sheet\n",
    "- Different processing strategies per sheet type\n",
    "- Better document organization and retrieval\n",
    "\"\"\"\n",
    "\n",
    "def process_excel_by_sheets(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process Excel file with sheet-specific document creation.\n",
    "    \n",
    "    This function reads each sheet separately and creates optimized\n",
    "    documents based on the sheet's data type and structure.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the Excel file\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: Documents with sheet-aware processing\n",
    "        \n",
    "    Features:\n",
    "        - Individual sheet processing\n",
    "        - Sheet-specific content formatting\n",
    "        - Rich metadata including sheet information\n",
    "        - Optimized for multi-sheet business files\n",
    "    \n",
    "    Example:\n",
    "        docs = process_excel_by_sheets(\"inventory.xlsx\")\n",
    "        for doc in docs:\n",
    "            print(f\"Sheet: {doc.metadata['sheet_name']}\")\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Read all sheets from Excel file into a dictionary\n",
    "    # key = sheet name, value = DataFrame\n",
    "    excel_data = pd.read_excel(filepath, sheet_name=None)\n",
    "    \n",
    "    # Process each sheet individually\n",
    "    for sheet_name, df in excel_data.items():\n",
    "        print(f\"üìä Processing sheet: '{sheet_name}'\")\n",
    "        \n",
    "        # Determine processing strategy based on sheet name/content\n",
    "        if sheet_name.lower() == 'products':\n",
    "            # Product sheet: Create one document per product\n",
    "            for idx, row in df.iterrows():\n",
    "                content = f\"\"\"Product Record from {sheet_name} Sheet:\n",
    "Product: {row['Product']}\n",
    "Category: {row['Category']}\n",
    "Price: ${row['Price']}\n",
    "Stock: {row['Stock']} units\n",
    "Description: {row['Description']}\"\"\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        'source': filepath,\n",
    "                        'sheet_name': sheet_name,\n",
    "                        'sheet_type': 'product_catalog',\n",
    "                        'row_index': idx,\n",
    "                        'product_name': row['Product'],\n",
    "                        'category': row['Category'],\n",
    "                        'data_type': 'product_record'\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                \n",
    "        elif sheet_name.lower() == 'summary':\n",
    "            # Summary sheet: Create aggregated documents\n",
    "            content = f\"\"\"Category Summary from {sheet_name} Sheet:\\n\"\"\"\n",
    "            for idx, row in df.iterrows():\n",
    "                content += f\"\"\"\n",
    "Category: {row['Category']}\n",
    "Total Items: {row['Total_Items']}\n",
    "Total Value: ${row['Total_Value']}\"\"\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'source': filepath,\n",
    "                    'sheet_name': sheet_name,\n",
    "                    'sheet_type': 'summary_report',\n",
    "                    'data_type': 'aggregated_summary',\n",
    "                    'categories_count': len(df)\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        else:\n",
    "            # Generic sheet processing for unknown sheet types\n",
    "            content = f\"Data from {sheet_name} Sheet:\\n\"\n",
    "            content += df.to_string(index=False)\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'source': filepath,\n",
    "                    'sheet_name': sheet_name,\n",
    "                    'sheet_type': 'generic_data',\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'data_type': 'tabular_data'\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Custom Excel Processing - Sheet-aware\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "try:\n",
    "    # Process Excel file with custom sheet handling\n",
    "    excel_custom_docs = process_excel_by_sheets('data/structured_files/inventory.xlsx')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Custom Excel processing completed\")\n",
    "    print(f\"üìä Created {len(excel_custom_docs)} documents from Excel sheets\")\n",
    "    \n",
    "    # Analyze documents by sheet\n",
    "    sheet_analysis = {}\n",
    "    for doc in excel_custom_docs:\n",
    "        sheet_name = doc.metadata['sheet_name']\n",
    "        sheet_type = doc.metadata['sheet_type']\n",
    "        \n",
    "        if sheet_name not in sheet_analysis:\n",
    "            sheet_analysis[sheet_name] = {'count': 0, 'type': sheet_type}\n",
    "        sheet_analysis[sheet_name]['count'] += 1\n",
    "    \n",
    "    print(f\"\\nüìã Sheet Processing Summary:\")\n",
    "    for sheet, info in sheet_analysis.items():\n",
    "        print(f\"  ‚Ä¢ {sheet}: {info['count']} documents ({info['type']})\")\n",
    "    \n",
    "    # Show example document\n",
    "    print(f\"\\nüìÑ Example document from Products sheet:\")\n",
    "    products_doc = next(doc for doc in excel_custom_docs if doc.metadata['sheet_name'] == 'Products')\n",
    "    print(f\"Content:\\n{products_doc.page_content}\")\n",
    "    print(f\"Metadata keys: {list(products_doc.metadata.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in custom Excel processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f80e6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Structured Data Processing Performance Analysis\n",
      "=======================================================\n",
      "‚ö†Ô∏è  Run the document loading cells first to see performance metrics\n",
      "\n",
      "üéØ Best Practices for Production:\n",
      "  ‚Ä¢ Use custom processing for better document quality\n",
      "  ‚Ä¢ Include rich metadata for advanced filtering\n",
      "  ‚Ä¢ Consider document chunking for very large datasets\n",
      "  ‚Ä¢ Implement caching for frequently accessed files\n"
     ]
    }
   ],
   "source": [
    "# Performance Analysis and Comparison\n",
    "# ====================================\n",
    "\"\"\"\n",
    "This section compares the performance characteristics of different\n",
    "structured data processing approaches to help you make informed decisions\n",
    "for production RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def analyze_structured_data_performance():\n",
    "    \"\"\"\n",
    "    Analyze memory usage and processing characteristics of loaded documents.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics for different processing methods\n",
    "    \"\"\"\n",
    "    performance_metrics = {}\n",
    "    \n",
    "    # Analyze CSV documents if loaded\n",
    "    if 'csv_docs' in locals() and csv_docs:\n",
    "        performance_metrics['csv_standard'] = {\n",
    "            'method': 'CSVLoader',\n",
    "            'document_count': len(csv_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in csv_docs) / len(csv_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in csv_docs),\n",
    "            'metadata_richness': len(csv_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze custom CSV documents if loaded\n",
    "    if 'custom_docs' in locals() and custom_docs:\n",
    "        performance_metrics['csv_custom'] = {\n",
    "            'method': 'Custom CSV Processing',\n",
    "            'document_count': len(custom_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in custom_docs) / len(custom_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in custom_docs),\n",
    "            'metadata_richness': len(custom_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze Excel documents if loaded\n",
    "    if 'excel_custom_docs' in locals() and excel_custom_docs:\n",
    "        performance_metrics['excel_custom'] = {\n",
    "            'method': 'Custom Excel Processing',\n",
    "            'document_count': len(excel_custom_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in excel_custom_docs) / len(excel_custom_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in excel_custom_docs),\n",
    "            'metadata_richness': len(excel_custom_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "# Run performance analysis\n",
    "print(\"‚ö° Structured Data Processing Performance Analysis\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "try:\n",
    "    metrics = analyze_structured_data_performance()\n",
    "    \n",
    "    if metrics:\n",
    "        for method_key, data in metrics.items():\n",
    "            print(f\"\\nüìä {data['method']}:\")\n",
    "            print(f\"  ‚Ä¢ Documents created: {data['document_count']}\")\n",
    "            print(f\"  ‚Ä¢ Average document length: {data['avg_doc_length']:.1f} characters\")\n",
    "            print(f\"  ‚Ä¢ Total memory usage: {data['total_memory']} bytes\")\n",
    "            print(f\"  ‚Ä¢ Metadata fields: {data['metadata_richness']}\")\n",
    "        \n",
    "        print(f\"\\nüí° Key Insights:\")\n",
    "        print(f\"  ‚Ä¢ Custom processing creates more structured, readable documents\")\n",
    "        print(f\"  ‚Ä¢ Rich metadata enables better filtering and retrieval\")\n",
    "        print(f\"  ‚Ä¢ Sheet-aware processing provides contextual information\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Run the document loading cells first to see performance metrics\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in performance analysis: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Best Practices for Production:\")\n",
    "print(f\"  ‚Ä¢ Use custom processing for better document quality\")\n",
    "print(f\"  ‚Ä¢ Include rich metadata for advanced filtering\")\n",
    "print(f\"  ‚Ä¢ Consider document chunking for very large datasets\")\n",
    "print(f\"  ‚Ä¢ Implement caching for frequently accessed files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cf0b3",
   "metadata": {},
   "source": [
    "## Best Practices for Structured Data Processing\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "1. **CSVLoader (Standard)**\n",
    "   - ‚úÖ Quick prototyping and simple data extraction\n",
    "   - ‚úÖ Small datasets with homogeneous structure\n",
    "   - ‚úÖ When processing speed is priority over content quality\n",
    "   - ‚ùå Complex business logic or data relationships\n",
    "\n",
    "2. **Custom Processing**\n",
    "   - ‚úÖ Production RAG systems requiring high-quality documents\n",
    "   - ‚úÖ Complex data structures with multiple relationships\n",
    "   - ‚úÖ Need for rich metadata and filtering capabilities\n",
    "   - ‚úÖ Integration with business-specific document formats\n",
    "\n",
    "3. **Excel Sheet-Aware Processing**\n",
    "   - ‚úÖ Multi-sheet business files with different data types\n",
    "   - ‚úÖ When sheet structure provides important context\n",
    "   - ‚úÖ Need for sheet-specific processing strategies\n",
    "   - ‚úÖ Complex business reports and dashboards\n",
    "\n",
    "### Common Challenges and Solutions\n",
    "\n",
    "1. **Large File Performance**\n",
    "   - **Problem**: Memory issues with large CSV/Excel files\n",
    "   - **Solution**: Implement chunking and streaming processing\n",
    "   - **Code Pattern**: Process files in batches of 1000 rows\n",
    "\n",
    "2. **Data Quality Issues**\n",
    "   - **Problem**: Missing values, inconsistent formats\n",
    "   - **Solution**: Add data validation and cleaning steps\n",
    "   - **Code Pattern**: Validate data types before document creation\n",
    "\n",
    "3. **Metadata Optimization**\n",
    "   - **Problem**: Too much or too little metadata\n",
    "   - **Solution**: Include only actionable metadata for retrieval\n",
    "   - **Code Pattern**: Focus on filterable and searchable fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1975a521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Utility Functions for Production RAG\n",
      "========================================\n",
      "‚úÖ Data validation completed\n",
      "  ‚Ä¢ Original rows: 5\n",
      "  ‚Ä¢ Cleaned rows: 5\n",
      "  ‚Ä¢ Removed duplicates: 0\n",
      "\n",
      "‚úÖ Created 5 enhanced documents\n",
      "üìÑ Example enhanced document:\n",
      "Content:\n",
      "Product Information:\n",
      "Name: Laptop\n",
      "Category: Electronics\n",
      "Price: $999.99\n",
      "Stock: 50 units available\n",
      "Description: High-performance laptop with 16GB RAM and 512GB SSD\n",
      "\n",
      "This product is in the Electronics ca...\n",
      "Metadata: ['row_index', 'source_type', 'processing_timestamp', 'product', 'category', 'price', 'stock']\n"
     ]
    }
   ],
   "source": [
    "# Practical Utility Functions for Production RAG Systems\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "This section provides reusable utility functions for structured data processing\n",
    "in production RAG applications.\n",
    "\"\"\"\n",
    "\n",
    "def validate_and_clean_data(df: pd.DataFrame, required_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate and clean DataFrame before processing.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame to validate\n",
    "        required_columns (List[str]): List of required column names\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and validated DataFrame\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing\n",
    "    \"\"\"\n",
    "    # Check for required columns\n",
    "    missing_columns = set(required_columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':  # String columns\n",
    "            df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "        else:  # Numeric columns\n",
    "            df_clean[col] = df_clean[col].fillna(0)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    print(f\"‚úÖ Data validation completed\")\n",
    "    print(f\"  ‚Ä¢ Original rows: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Cleaned rows: {len(df_clean)}\")\n",
    "    print(f\"  ‚Ä¢ Removed duplicates: {len(df) - len(df_clean)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def create_enhanced_documents(df: pd.DataFrame, \n",
    "                           content_template: str,\n",
    "                           metadata_fields: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create enhanced documents with customizable content and metadata.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Source data\n",
    "        content_template (str): Template for document content with {column} placeholders\n",
    "        metadata_fields (List[str]): Column names to include as metadata\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: Enhanced document objects\n",
    "        \n",
    "    Example:\n",
    "        template = \"Product: {Product}\\nPrice: ${Price}\\nDescription: {Description}\"\n",
    "        fields = ['Product', 'Category', 'Price']\n",
    "        docs = create_enhanced_documents(df, template, fields)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create content using template\n",
    "        try:\n",
    "            content = content_template.format(**row.to_dict())\n",
    "        except KeyError as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Missing column {e} in template\")\n",
    "            content = str(row.to_dict())\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'row_index': idx,\n",
    "            'source_type': 'structured_data',\n",
    "            'processing_timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add specified metadata fields\n",
    "        for field in metadata_fields:\n",
    "            if field in row.index:\n",
    "                metadata[field.lower().replace(' ', '_')] = row[field]\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage of utility functions\n",
    "print(\"üîß Utility Functions for Production RAG\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test data validation\n",
    "try:\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv('data/structured_files/products.csv')\n",
    "    \n",
    "    # Validate data\n",
    "    required_cols = ['Product', 'Category', 'Price', 'Description']\n",
    "    clean_df = validate_and_clean_data(test_df, required_cols)\n",
    "    \n",
    "    # Create enhanced documents\n",
    "    template = \"\"\"Product Information:\n",
    "Name: {Product}\n",
    "Category: {Category}\n",
    "Price: ${Price}\n",
    "Stock: {Stock} units available\n",
    "Description: {Description}\n",
    "\n",
    "This product is in the {Category} category and costs ${Price}.\"\"\"\n",
    "    \n",
    "    metadata_fields = ['Product', 'Category', 'Price', 'Stock']\n",
    "    enhanced_docs = create_enhanced_documents(clean_df, template, metadata_fields)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created {len(enhanced_docs)} enhanced documents\")\n",
    "    print(f\"üìÑ Example enhanced document:\")\n",
    "    print(f\"Content:\\n{enhanced_docs[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {list(enhanced_docs[0].metadata.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in utility functions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f097942",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this notebook, we explored comprehensive strategies for processing structured data:\n",
    "\n",
    "1. **CSV Processing**: From simple CSVLoader to intelligent custom processing\n",
    "2. **Excel Processing**: Sheet-aware processing for complex business files\n",
    "3. **Performance Analysis**: Understanding trade-offs between different approaches\n",
    "4. **Production Utilities**: Reusable functions for data validation and enhancement\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Choose the right approach**: Simple loaders for quick prototyping, custom processing for production\n",
    "- **Metadata is crucial**: Rich metadata enables advanced filtering and retrieval\n",
    "- **Structure matters**: Preserve data relationships and context in document format\n",
    "- **Validation is essential**: Always validate and clean data before processing\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- [ ] Implement data validation and cleaning\n",
    "- [ ] Create structured, human-readable content\n",
    "- [ ] Include rich metadata for filtering\n",
    "- [ ] Add error handling and logging\n",
    "- [ ] Consider performance and memory usage\n",
    "- [ ] Implement caching for large files\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try with your own data**: Apply these techniques to your structured datasets\n",
    "2. **Build a data pipeline**: Create automated processing workflows\n",
    "3. **Optimize for retrieval**: Test different document structures with your queries\n",
    "4. **Scale for production**: Implement batch processing and monitoring\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "- [Structured Data Best Practices](https://docs.llamaindex.ai/en/stable/examples/data_connectors/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81890307",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6284c42c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
