{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b20e295",
   "metadata": {},
   "source": [
    "# JSON Data Processing with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive approaches to loading and processing JSON data using LangChain's document loaders. JSON is one of the most common data formats in modern applications, and understanding how to effectively process it for RAG systems is crucial for building robust AI applications.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **JSON Structure Analysis** - Understanding complex nested JSON data\n",
    "2. **JSONLoader Usage** - Using jq schemas for selective data extraction\n",
    "3. **Custom Processing** - Building intelligent JSON processors for complex structures\n",
    "4. **JSONL Handling** - Processing JSON Lines format for streaming data\n",
    "5. **Production Strategies** - Best practices for JSON processing in RAG pipelines\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "uv add install langchain-community jq\n",
    "```\n",
    "\n",
    "## Common JSON Use Cases in RAG\n",
    "- API responses and web service data\n",
    "- Configuration files and settings\n",
    "- Log files and event streams\n",
    "- Product catalogs and inventories\n",
    "- User profiles and social media data\n",
    "- IoT sensor data and telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edaa0e",
   "metadata": {},
   "source": [
    "### Json Parsing And Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07d61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directory structure created successfully\n",
      "üìÅ Ready to process JSON and JSONL files\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "JSON Data Processing Setup\n",
    "\n",
    "This module sets up the environment for processing JSON data files.\n",
    "We'll create sample JSON data that represents real-world complex structures\n",
    "commonly found in business applications and APIs.\n",
    "\n",
    "Author: Data Science Team\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries for JSON manipulation and file handling\n",
    "import json  # For JSON parsing and generation\n",
    "import os    # For operating system interface and directory operations\n",
    "\n",
    "# Create directory structure for storing our sample JSON files\n",
    "# exist_ok=True prevents error if directory already exists\n",
    "os.makedirs(\"data/json_files\", exist_ok=True)\n",
    "print(\"‚úÖ Directory structure created successfully\")\n",
    "print(\"üìÅ Ready to process JSON and JSONL files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adac1568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complex nested JSON data structure created\n",
      "üìä Structure includes:\n",
      "  ‚Ä¢ Company metadata\n",
      "  ‚Ä¢ Employee profiles with skills and projects\n",
      "  ‚Ä¢ Department information with budgets\n",
      "  ‚Ä¢ Nested arrays and objects at multiple levels\n"
     ]
    }
   ],
   "source": [
    "# Creating Complex Nested JSON Sample Data\n",
    "# ==========================================\n",
    "\"\"\"\n",
    "This section creates realistic sample JSON data that demonstrates common patterns\n",
    "found in business applications:\n",
    "- Nested objects and arrays\n",
    "- Mixed data types (strings, numbers, arrays, objects)\n",
    "- Multiple levels of hierarchy\n",
    "- Real-world business entities (employees, projects, departments)\n",
    "\n",
    "This structure represents a typical company API response or configuration file.\n",
    "\"\"\"\n",
    "\n",
    "# Define complex nested JSON data structure\n",
    "# This represents a realistic business scenario with multiple data relationships\n",
    "json_data = {\n",
    "    \"company\": \"TechCorp\",  # Company name (string)\n",
    "    \n",
    "    # Array of employee objects - demonstrates nested arrays with complex objects\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"id\": 1,  # Unique identifier (number)\n",
    "            \"name\": \"John Doe\",  # Employee name (string)\n",
    "            \"role\": \"Software Engineer\",  # Job role (string)\n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"React\"],  # Array of skills (strings)\n",
    "            \n",
    "            # Nested array of project objects\n",
    "            \"projects\": [\n",
    "                {\"name\": \"RAG System\", \"status\": \"In Progress\"},\n",
    "                {\"name\": \"Data Pipeline\", \"status\": \"Completed\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"role\": \"Data Scientist\",\n",
    "            \"skills\": [\"Python\", \"Machine Learning\", \"SQL\"],\n",
    "            \n",
    "            # Different project structure for variety\n",
    "            \"projects\": [\n",
    "                {\"name\": \"ML Model\", \"status\": \"In Progress\"},\n",
    "                {\"name\": \"Analytics Dashboard\", \"status\": \"Planning\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Nested object structure - demonstrates hierarchical data organization\n",
    "    \"departments\": {\n",
    "        \"engineering\": {  # Department-specific nested object\n",
    "            \"head\": \"Mike Johnson\",  # Department head (string)\n",
    "            \"budget\": 1000000,  # Budget amount (number)\n",
    "            \"team_size\": 25  # Team size (number)\n",
    "        },\n",
    "        \"data_science\": {  # Another department with same structure\n",
    "            \"head\": \"Sarah Williams\",\n",
    "            \"budget\": 750000,\n",
    "            \"team_size\": 15\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Complex nested JSON data structure created\")\n",
    "print(\"üìä Structure includes:\")\n",
    "print(\"  ‚Ä¢ Company metadata\")\n",
    "print(\"  ‚Ä¢ Employee profiles with skills and projects\")\n",
    "print(\"  ‚Ä¢ Department information with budgets\")\n",
    "print(\"  ‚Ä¢ Nested arrays and objects at multiple levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e2cbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 'TechCorp',\n",
       " 'employees': [{'id': 1,\n",
       "   'name': 'John Doe',\n",
       "   'role': 'Software Engineer',\n",
       "   'skills': ['Python', 'JavaScript', 'React'],\n",
       "   'projects': [{'name': 'RAG System', 'status': 'In Progress'},\n",
       "    {'name': 'Data Pipeline', 'status': 'Completed'}]},\n",
       "  {'id': 2,\n",
       "   'name': 'Jane Smith',\n",
       "   'role': 'Data Scientist',\n",
       "   'skills': ['Python', 'Machine Learning', 'SQL'],\n",
       "   'projects': [{'name': 'ML Model', 'status': 'In Progress'},\n",
       "    {'name': 'Analytics Dashboard', 'status': 'Planning'}]}],\n",
       " 'departments': {'engineering': {'head': 'Mike Johnson',\n",
       "   'budget': 1000000,\n",
       "   'team_size': 25},\n",
       "  'data_science': {'head': 'Sarah Williams',\n",
       "   'budget': 750000,\n",
       "   'team_size': 15}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the created JSON data structure\n",
    "# This will display the complete nested structure for understanding\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complex JSON Data to File\n",
    "# ===============================\n",
    "\"\"\"\n",
    "This section saves our structured JSON data to a file for processing.\n",
    "The 'indent=2' parameter creates human-readable formatting with proper indentation.\n",
    "\n",
    "This demonstrates how real-world JSON files are typically structured and stored.\n",
    "\"\"\"\n",
    "\n",
    "# Save the nested JSON data to a file with proper formatting\n",
    "json_file_path = 'data/json_files/company_data.json'\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Complex JSON data saved to: {json_file_path}\")\n",
    "print(\"üìÅ File contains nested employee and department data\")\n",
    "print(\"üîç Ready for JSONLoader processing with jq queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON Lines (JSONL) Format Sample Data\n",
    "# ==============================================\n",
    "\"\"\"\n",
    "JSON Lines is a format where each line is a valid JSON object.\n",
    "This format is commonly used for:\n",
    "- Streaming data processing\n",
    "- Log files and event streams\n",
    "- Large datasets that can't fit in memory\n",
    "- API responses with multiple records\n",
    "\n",
    "Each line represents a separate event or record that can be processed independently.\n",
    "\"\"\"\n",
    "\n",
    "# Define sample event data for JSONL format\n",
    "# This represents a typical event tracking or logging scenario\n",
    "jsonl_data = [\n",
    "    # User login event - contains timestamp, event type, and user ID\n",
    "    {\"timestamp\": \"2024-01-01T08:00:00Z\", \"event\": \"user_login\", \"user_id\": 123, \"ip_address\": \"192.168.1.1\"},\n",
    "    \n",
    "    # Page view event - includes additional page information\n",
    "    {\"timestamp\": \"2024-01-01T08:01:00Z\", \"event\": \"page_view\", \"user_id\": 123, \"page\": \"/home\", \"duration\": 30},\n",
    "    \n",
    "    # Purchase event - contains transaction details\n",
    "    {\"timestamp\": \"2024-01-01T08:05:00Z\", \"event\": \"purchase\", \"user_id\": 123, \"amount\": 99.99, \"product\": \"Premium Plan\"},\n",
    "    \n",
    "    # Search event - demonstrates different event structure\n",
    "    {\"timestamp\": \"2024-01-01T08:03:00Z\", \"event\": \"search\", \"user_id\": 123, \"query\": \"machine learning\", \"results\": 42}\n",
    "]\n",
    "\n",
    "# Save as JSONL file - each JSON object on a separate line\n",
    "jsonl_file_path = 'data/json_files/events.jsonl'\n",
    "\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as f:\n",
    "    for item in jsonl_data:\n",
    "        # Write each JSON object as a single line\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ JSONL data saved to: {jsonl_file_path}\")\n",
    "print(f\"üìä Created {len(jsonl_data)} event records\")\n",
    "print(\"üìù Each line contains a separate JSON object\")\n",
    "print(\"üîç Ready for streaming and batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183939b7",
   "metadata": {},
   "source": [
    "## Json Processing Stratergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ JSONLoader - Extract specific fields\n",
      "Loaded 2 employee documents\n",
      "First employee: {\"id\": 1, \"name\": \"John Doe\", \"role\": \"Software Engineer\", \"skills\": [\"Python\", \"JavaScript\", \"React\"], \"projects\": [{\"name\": \"RAG System\", \"status\": \"In Progress\"}, {\"name\": \"Data Pipeline\", \"status\"...\n",
      "[Document(metadata={'source': '/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/Data_Ingest_Parsing/data/json_files/company_data.json', 'seq_num': 1}, page_content='{\"id\": 1, \"name\": \"John Doe\", \"role\": \"Software Engineer\", \"skills\": [\"Python\", \"JavaScript\", \"React\"], \"projects\": [{\"name\": \"RAG System\", \"status\": \"In Progress\"}, {\"name\": \"Data Pipeline\", \"status\": \"Completed\"}]}'), Document(metadata={'source': '/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/Data_Ingest_Parsing/data/json_files/company_data.json', 'seq_num': 2}, page_content='{\"id\": 2, \"name\": \"Jane Smith\", \"role\": \"Data Scientist\", \"skills\": [\"Python\", \"Machine Learning\", \"SQL\"], \"projects\": [{\"name\": \"ML Model\", \"status\": \"In Progress\"}, {\"name\": \"Analytics Dashboard\", \"status\": \"Planning\"}]}')]\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using JSONLoader with jq Schema for Selective Data Extraction\n",
    "# ========================================================================\n",
    "\"\"\"\n",
    "JSONLoader with jq schema allows precise extraction of specific parts of JSON data.\n",
    "jq is a powerful query language for JSON that enables complex filtering and transformation.\n",
    "\n",
    "Common jq patterns:\n",
    "- '.employees[]' - Extract each employee from the employees array\n",
    "- '.departments.engineering' - Extract specific nested object\n",
    "- '.employees[] | select(.role == \"Engineer\")' - Filter with conditions\n",
    "- '.employees[].skills[]' - Flatten nested arrays\n",
    "\n",
    "Pros: Precise data extraction, supports complex queries\n",
    "Cons: Requires jq knowledge, can be complex for beginners\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "print(\"1Ô∏è‚É£ JSONLoader - Extract Specific Fields with jq Schema\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "try:\n",
    "    # Strategy 1: Extract employee information using jq schema\n",
    "    # The jq query '.employees[]' extracts each employee as a separate document\n",
    "    employee_loader = JSONLoader(\n",
    "        file_path='data/json_files/company_data.json',\n",
    "        jq_schema='.employees[]',  # jq query to extract each employee\n",
    "        text_content=False  # Get full JSON objects instead of text representation\n",
    "    )\n",
    "\n",
    "    # Load documents - each employee becomes a separate Document object\n",
    "    employee_docs = employee_loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {len(employee_docs)} employee documents\")\n",
    "    print(f\"üìä Each document represents one employee record\")\n",
    "    print(f\"üìÑ First employee preview: {employee_docs[0].page_content[:200]}...\")\n",
    "    \n",
    "    # Display metadata information\n",
    "    print(f\"üè∑Ô∏è  Metadata keys: {list(employee_docs[0].metadata.keys())}\")\n",
    "    \n",
    "    # Show all employee documents for analysis\n",
    "    print(f\"\\nüìã All Employee Documents:\")\n",
    "    for i, doc in enumerate(employee_docs):\n",
    "        print(f\"  Employee {i+1}: {len(doc.page_content)} characters\")\n",
    "    \n",
    "    employee_docs  # Display the actual documents\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: JSON file not found. Please run the data creation cells first.\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Import Error: Missing jq dependency\")\n",
    "    print(\"üí° Install jq: pip install jq\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading JSON: {e}\")\n",
    "    print(\"üí° Tip: Ensure 'langchain-community' is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Custom JSON Processing\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Custom JSON Processing for Complex Structures\n",
    "# =======================================================\n",
    "\"\"\"\n",
    "This section demonstrates custom JSON processing that provides more control over\n",
    "how complex nested JSON data is converted into documents for RAG applications.\n",
    "\n",
    "Benefits of custom processing:\n",
    "- Intelligent content formatting optimized for LLM understanding\n",
    "- Rich metadata creation for filtering and search\n",
    "- Context preservation across nested relationships\n",
    "- Flexible document structures based on data types\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Custom JSON Processing - Intelligent Structure Handling\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def process_json_intelligently(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process JSON with intelligent flattening and context preservation.\n",
    "    \n",
    "    This function creates structured documents from complex JSON data with\n",
    "    enhanced formatting and metadata for better RAG performance.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the JSON file to process\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of Document objects with structured content\n",
    "        \n",
    "    Features:\n",
    "        - Context-aware document creation\n",
    "        - Structured content formatting for readability\n",
    "        - Rich metadata for filtering and search\n",
    "        - Relationship preservation between nested entities\n",
    "    \n",
    "    Example:\n",
    "        docs = process_json_intelligently(\"company_data.json\")\n",
    "        print(f\"Created {len(docs)} documents\")\n",
    "    \"\"\"\n",
    "    # Load and parse the JSON file\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Strategy 1: Create comprehensive employee profile documents\n",
    "    # This preserves relationships between employees, their skills, and projects\n",
    "    for emp in data.get('employees', []):\n",
    "        # Create structured, human-readable content\n",
    "        # This format is optimized for LLM understanding and Q&A\n",
    "        content = f\"\"\"Employee Profile:\n",
    "Name: {emp['name']}\n",
    "Role: {emp['role']}\n",
    "Employee ID: {emp['id']}\n",
    "Skills: {', '.join(emp['skills'])}\n",
    "\n",
    "Current Projects:\"\"\"\n",
    "        \n",
    "        # Add project information with context\n",
    "        for proj in emp.get('projects', []):\n",
    "            content += f\"\\n- {proj['name']} (Status: {proj['status']})\"\n",
    "        \n",
    "        # Add company context\n",
    "        content += f\"\\n\\nCompany: {data.get('company', 'Unknown')}\"\n",
    "        \n",
    "        # Create document with comprehensive metadata\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                'source': filepath,\n",
    "                'data_type': 'employee_profile',\n",
    "                'employee_id': emp['id'],\n",
    "                'employee_name': emp['name'],\n",
    "                'role': emp['role'],\n",
    "                'skills': emp['skills'],\n",
    "                'project_count': len(emp.get('projects', [])),\n",
    "                'company': data.get('company', 'Unknown'),\n",
    "                'content_type': 'structured_profile'\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Strategy 2: Create department summary documents\n",
    "    # This provides organizational context and hierarchy\n",
    "    departments = data.get('departments', {})\n",
    "    if departments:\n",
    "        for dept_name, dept_info in departments.items():\n",
    "            content = f\"\"\"Department Information:\n",
    "Department: {dept_name.title()}\n",
    "Department Head: {dept_info.get('head', 'Not specified')}\n",
    "Budget: ${dept_info.get('budget', 0):,}\n",
    "Team Size: {dept_info.get('team_size', 0)} employees\n",
    "Company: {data.get('company', 'Unknown')}\n",
    "\n",
    "This department is part of {data.get('company', 'the organization')} with a budget allocation of ${dept_info.get('budget', 0):,}.\"\"\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'source': filepath,\n",
    "                    'data_type': 'department_info',\n",
    "                    'department_name': dept_name,\n",
    "                    'department_head': dept_info.get('head', 'Not specified'),\n",
    "                    'budget': dept_info.get('budget', 0),\n",
    "                    'team_size': dept_info.get('team_size', 0),\n",
    "                    'company': data.get('company', 'Unknown'),\n",
    "                    'content_type': 'organizational_data'\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test the custom processing function\n",
    "try:\n",
    "    custom_docs = process_json_intelligently('data/json_files/company_data.json')\n",
    "    \n",
    "    print(f\"‚úÖ Custom JSON processing completed\")\n",
    "    print(f\"üìä Created {len(custom_docs)} documents\")\n",
    "    \n",
    "    # Analyze document types\n",
    "    doc_types = {}\n",
    "    for doc in custom_docs:\n",
    "        doc_type = doc.metadata.get('data_type', 'unknown')\n",
    "        if doc_type not in doc_types:\n",
    "            doc_types[doc_type] = 0\n",
    "        doc_types[doc_type] += 1\n",
    "    \n",
    "    print(f\"\\nüìã Document Type Analysis:\")\n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  ‚Ä¢ {doc_type}: {count} documents\")\n",
    "    \n",
    "    # Show example of custom-processed document\n",
    "    print(f\"\\nüìÑ Example custom document:\")\n",
    "    print(f\"Content:\\n{custom_docs[0].page_content[:300]}...\")\n",
    "    print(f\"Metadata keys: {list(custom_docs[0].metadata.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in custom JSON processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10149690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/json_files/company_data.json', 'data_type': 'employee_profile', 'employee_id': 1, 'employee_name': 'John Doe', 'role': 'Software Engineer'}, page_content='Employee Profile:\\n        Name: John Doe\\n        Role: Software Engineer\\n        Skills: Python, JavaScript, React\\n\\n        Projects:\\n- RAG System (Status: In Progress)\\n- Data Pipeline (Status: Completed)'),\n",
       " Document(metadata={'source': 'data/json_files/company_data.json', 'data_type': 'employee_profile', 'employee_id': 2, 'employee_name': 'Jane Smith', 'role': 'Data Scientist'}, page_content='Employee Profile:\\n        Name: Jane Smith\\n        Role: Data Scientist\\n        Skills: Python, Machine Learning, SQL\\n\\n        Projects:\\n- ML Model (Status: In Progress)\\n- Analytics Dashboard (Status: Planning)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the custom JSON processing function directly\n",
    "# This will return the list of documents for inspection\n",
    "process_json_intelligently(\"data/json_files/company_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870beb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3Ô∏è‚É£ JSONL Processing - Event Stream Data\n",
      "----------------------------------------\n",
      "‚úÖ JSONL processing completed\n",
      "üìä Created 3 event documents\n",
      "\n",
      "üìã Event Type Distribution:\n",
      "  ‚Ä¢ user_login: 1 events\n",
      "  ‚Ä¢ page_view: 1 events\n",
      "  ‚Ä¢ purchase: 1 events\n",
      "\n",
      "üìÑ Example event document:\n",
      "Content:\n",
      "User Login Event:\n",
      "Timestamp: 2024-01-01\n",
      "User ID: 123\n",
      "IP Address: Unknown\n",
      "Event Type: Login\n",
      "\n",
      "A user successfully logged into the system from IP unknown address.\n",
      "Metadata keys: ['source', 'line_number', 'event_type', 'timestamp', 'user_id', 'data_type', 'content_type']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Processing JSON Lines (JSONL) Format\n",
    "# ==============================================\n",
    "\"\"\"\n",
    "JSON Lines format is commonly used for streaming data, logs, and large datasets.\n",
    "Each line is a separate JSON object, allowing for efficient processing of\n",
    "large files without loading everything into memory.\n",
    "\n",
    "Benefits of JSONL:\n",
    "- Memory efficient for large datasets\n",
    "- Supports streaming processing\n",
    "- Easy to append new records\n",
    "- Common in data pipelines and APIs\n",
    "\"\"\"\n",
    "\n",
    "print(\"3Ô∏è‚É£ JSONL Processing - Event Stream Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def process_jsonl_events(filepath: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process JSONL file with event-specific document creation.\n",
    "    \n",
    "    This function handles streaming JSON data where each line represents\n",
    "    a separate event or record that should be processed independently.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the JSONL file to process\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: Documents created from JSONL events\n",
    "        \n",
    "    Features:\n",
    "        - Line-by-line processing for memory efficiency\n",
    "        - Event-specific content formatting\n",
    "        - Temporal metadata for chronological queries\n",
    "        - Event type categorization\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Process JSONL file line by line (memory efficient)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    # Parse each line as a separate JSON object\n",
    "                    event_data = json.loads(line.strip())\n",
    "                    \n",
    "                    # Create event-specific content based on event type\n",
    "                    event_type = event_data.get('event', 'unknown')\n",
    "                    \n",
    "                    if event_type == 'user_login':\n",
    "                        content = f\"\"\"User Login Event:\n",
    "Timestamp: {event_data.get('timestamp', 'Unknown')}\n",
    "User ID: {event_data.get('user_id', 'Unknown')}\n",
    "IP Address: {event_data.get('ip_address', 'Unknown')}\n",
    "Event Type: Login\n",
    "\n",
    "A user successfully logged into the system from IP {event_data.get('ip_address', 'unknown address')}.\"\"\"\n",
    "                    \n",
    "                    elif event_type == 'page_view':\n",
    "                        content = f\"\"\"Page View Event:\n",
    "Timestamp: {event_data.get('timestamp', 'Unknown')}\n",
    "User ID: {event_data.get('user_id', 'Unknown')}\n",
    "Page: {event_data.get('page', 'Unknown')}\n",
    "Duration: {event_data.get('duration', 0)} seconds\n",
    "Event Type: Page View\n",
    "\n",
    "User viewed {event_data.get('page', 'a page')} for {event_data.get('duration', 0)} seconds.\"\"\"\n",
    "                    \n",
    "                    elif event_type == 'purchase':\n",
    "                        content = f\"\"\"Purchase Event:\n",
    "Timestamp: {event_data.get('timestamp', 'Unknown')}\n",
    "User ID: {event_data.get('user_id', 'Unknown')}\n",
    "Product: {event_data.get('product', 'Unknown')}\n",
    "Amount: ${event_data.get('amount', 0)}\n",
    "Event Type: Purchase\n",
    "\n",
    "User completed a purchase of {event_data.get('product', 'unknown product')} for ${event_data.get('amount', 0)}.\"\"\"\n",
    "                    \n",
    "                    elif event_type == 'search':\n",
    "                        content = f\"\"\"Search Event:\n",
    "Timestamp: {event_data.get('timestamp', 'Unknown')}\n",
    "User ID: {event_data.get('user_id', 'Unknown')}\n",
    "Query: \"{event_data.get('query', 'Unknown')}\"\n",
    "Results: {event_data.get('results', 0)} found\n",
    "Event Type: Search\n",
    "\n",
    "User searched for \"{event_data.get('query', 'unknown query')}\" and found {event_data.get('results', 0)} results.\"\"\"\n",
    "                    \n",
    "                    else:\n",
    "                        # Generic event processing\n",
    "                        content = f\"Event Data:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in event_data.items()])\n",
    "                    \n",
    "                    # Create document with event metadata\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': filepath,\n",
    "                            'line_number': line_num,\n",
    "                            'event_type': event_type,\n",
    "                            'timestamp': event_data.get('timestamp', ''),\n",
    "                            'user_id': event_data.get('user_id', ''),\n",
    "                            'data_type': 'event_stream',\n",
    "                            'content_type': 'temporal_event',\n",
    "                            **{k: v for k, v in event_data.items() if k not in ['timestamp', 'event', 'user_id']}\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  Warning: Invalid JSON on line {line_num}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test JSONL processing\n",
    "try:\n",
    "    jsonl_docs = process_jsonl_events('data/json_files/events.jsonl')\n",
    "    \n",
    "    print(f\"‚úÖ JSONL processing completed\")\n",
    "    print(f\"üìä Created {len(jsonl_docs)} event documents\")\n",
    "    \n",
    "    # Analyze event types\n",
    "    event_types = {}\n",
    "    for doc in jsonl_docs:\n",
    "        event_type = doc.metadata.get('event_type', 'unknown')\n",
    "        if event_type not in event_types:\n",
    "            event_types[event_type] = 0\n",
    "        event_types[event_type] += 1\n",
    "    \n",
    "    print(f\"\\nüìã Event Type Distribution:\")\n",
    "    for event_type, count in event_types.items():\n",
    "        print(f\"  ‚Ä¢ {event_type}: {count} events\")\n",
    "    \n",
    "    # Show example event document\n",
    "    print(f\"\\nüìÑ Example event document:\")\n",
    "    print(f\"Content:\\n{jsonl_docs[0].page_content}\")\n",
    "    print(f\"Metadata keys: {list(jsonl_docs[0].metadata.keys())}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: JSONL file not found. Please run the JSONL creation cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in JSONL processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5325de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä JSON Processing Methods Comparison\n",
      "==========================================\n",
      "‚ö†Ô∏è  Run all processing methods first to see comparison\n",
      "\n",
      "üí° Method Selection Guidelines:\n",
      "  üìã JSONLoader + jq:\n",
      "     - Quick extraction of specific fields\n",
      "     - Known JSON structure with consistent schema\n",
      "     - Minimal processing overhead required\n",
      "  üß† Custom Processing:\n",
      "     - Production RAG systems requiring high-quality documents\n",
      "     - Complex nested data with business relationships\n",
      "     - Need for human-readable, context-rich content\n",
      "  üì° JSONL Processing:\n",
      "     - Event streams and temporal data\n",
      "     - Large datasets processed in streaming fashion\n",
      "     - Log files and API response sequences\n"
     ]
    }
   ],
   "source": [
    "# JSON Processing Methods Comparison and Analysis\n",
    "# ===============================================\n",
    "\"\"\"\n",
    "This section compares different JSON processing approaches to help you choose\n",
    "the best strategy for your specific use case and data structure.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìä JSON Processing Methods Comparison\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Compare results if all methods have been executed\n",
    "try:\n",
    "    if 'employee_docs' in locals() and 'custom_docs' in locals() and 'jsonl_docs' in locals():\n",
    "        print(\"\\n1Ô∏è‚É£ JSONLoader with jq Schema:\")\n",
    "        print(f\"  ‚Ä¢ Documents created: {len(employee_docs)}\")\n",
    "        print(f\"  ‚Ä¢ Content approach: Direct JSON extraction\")\n",
    "        print(f\"  ‚Ä¢ Best for: Specific field extraction from known structures\")\n",
    "        print(f\"  ‚Ä¢ Metadata richness: {len(employee_docs[0].metadata) if employee_docs else 0} fields\")\n",
    "        \n",
    "        print(\"\\n2Ô∏è‚É£ Custom JSON Processing:\")\n",
    "        print(f\"  ‚Ä¢ Documents created: {len(custom_docs)}\")\n",
    "        print(f\"  ‚Ä¢ Content approach: Structured, human-readable formatting\")\n",
    "        print(f\"  ‚Ä¢ Best for: Complex data with relationships and context\")\n",
    "        print(f\"  ‚Ä¢ Metadata richness: {len(custom_docs[0].metadata) if custom_docs else 0} fields\")\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ JSONL Event Processing:\")\n",
    "        print(f\"  ‚Ä¢ Documents created: {len(jsonl_docs)}\")\n",
    "        print(f\"  ‚Ä¢ Content approach: Event-specific formatted content\")\n",
    "        print(f\"  ‚Ä¢ Best for: Streaming data and temporal events\")\n",
    "        print(f\"  ‚Ä¢ Metadata richness: {len(jsonl_docs[0].metadata) if jsonl_docs else 0} fields\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        print(f\"\\nüìà Performance Analysis:\")\n",
    "        json_memory = sum(len(doc.page_content) for doc in employee_docs) if employee_docs else 0\n",
    "        custom_memory = sum(len(doc.page_content) for doc in custom_docs) if custom_docs else 0\n",
    "        jsonl_memory = sum(len(doc.page_content) for doc in jsonl_docs) if jsonl_docs else 0\n",
    "        \n",
    "        print(f\"  ‚Ä¢ JSONLoader memory footprint: {json_memory} characters\")\n",
    "        print(f\"  ‚Ä¢ Custom processing footprint: {custom_memory} characters\")\n",
    "        print(f\"  ‚Ä¢ JSONL processing footprint: {jsonl_memory} characters\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Run all processing methods first to see comparison\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in comparison analysis: {e}\")\n",
    "\n",
    "print(f\"\\nüí° Method Selection Guidelines:\")\n",
    "print(f\"  üìã JSONLoader + jq:\")\n",
    "print(f\"     - Quick extraction of specific fields\")\n",
    "print(f\"     - Known JSON structure with consistent schema\")\n",
    "print(f\"     - Minimal processing overhead required\")\n",
    "\n",
    "print(f\"  üß† Custom Processing:\")\n",
    "print(f\"     - Production RAG systems requiring high-quality documents\")\n",
    "print(f\"     - Complex nested data with business relationships\")\n",
    "print(f\"     - Need for human-readable, context-rich content\")\n",
    "\n",
    "print(f\"  üì° JSONL Processing:\")\n",
    "print(f\"     - Event streams and temporal data\")\n",
    "print(f\"     - Large datasets processed in streaming fashion\")\n",
    "print(f\"     - Log files and API response sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d60053a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Advanced JSON Processing Techniques\n",
      "======================================\n",
      "\n",
      "üìä Strategy: ENTITY\n",
      "  ‚Ä¢ Documents created: 2\n",
      "  ‚Ä¢ Avg content length: 122.5\n",
      "  ‚Ä¢ Example content preview:\n",
      "    Employee: John Doe (ID: 1)\n",
      "Role: Software Engineer\n",
      "Skills: Python, JavaScript, React\n",
      "Active Projects...\n",
      "\n",
      "üìä Strategy: FLAT\n",
      "  ‚Ä¢ Documents created: 2\n",
      "  ‚Ä¢ Avg content length: 486.0\n",
      "  ‚Ä¢ Example content preview:\n",
      "    Employee Information:\n",
      "employees.0.id: 1\n",
      "employees.0.name: John Doe\n",
      "employees.0.role: Software Engine...\n",
      "\n",
      "üìä Strategy: CONTEXTUAL\n",
      "  ‚Ä¢ Documents created: 2\n",
      "  ‚Ä¢ Avg content length: 336.5\n",
      "  ‚Ä¢ Example content preview:\n",
      "    Employee Profile with Context:\n",
      "Name: John Doe\n",
      "Role: Software Engineer\n",
      "Skills: Python, JavaScript, Re...\n",
      "\n",
      "üí° Strategy Recommendations:\n",
      "  üéØ Entity Strategy: Best for focused queries about specific entities\n",
      "  üìä Flat Strategy: Good for comprehensive searches across all data\n",
      "  üß† Contextual Strategy: Optimal for relationship-aware questions\n",
      "\n",
      "üîç JSON Flattening Example:\n",
      "  Original: {'user': {'profile': {'name': 'John', 'details': {'age': 30}}}}\n",
      "  Flattened: {'user.profile.name': 'John', 'user.profile.details.age': 30}\n"
     ]
    }
   ],
   "source": [
    "# Advanced JSON Processing Techniques\n",
    "# ====================================\n",
    "\"\"\"\n",
    "This section demonstrates advanced techniques for handling complex JSON scenarios\n",
    "that are commonly encountered in production RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "def process_nested_json_with_flattening(data: dict, parent_key: str = '', separator: str = '.') -> dict:\n",
    "    \"\"\"\n",
    "    Flatten nested JSON structures for easier processing.\n",
    "    \n",
    "    This utility function converts deeply nested JSON into a flat structure\n",
    "    while preserving the hierarchical relationships in the key names.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Nested JSON data to flatten\n",
    "        parent_key (str): Parent key for recursive processing\n",
    "        separator (str): Separator to use between nested keys\n",
    "        \n",
    "    Returns:\n",
    "        dict: Flattened dictionary with concatenated keys\n",
    "        \n",
    "    Example:\n",
    "        nested = {\"user\": {\"profile\": {\"name\": \"John\"}}}\n",
    "        flattened = flatten_json(nested)\n",
    "        # Result: {\"user.profile.name\": \"John\"}\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            # Recursively flatten nested dictionaries\n",
    "            items.extend(process_nested_json_with_flattening(value, new_key, separator).items())\n",
    "        elif isinstance(value, list):\n",
    "            # Handle arrays by enumerating items\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    items.extend(process_nested_json_with_flattening(item, f\"{new_key}{separator}{i}\", separator).items())\n",
    "                else:\n",
    "                    items.append((f\"{new_key}{separator}{i}\", item))\n",
    "        else:\n",
    "            # Add primitive values directly\n",
    "            items.append((new_key, value))\n",
    "    \n",
    "    return dict(items)\n",
    "\n",
    "def create_contextual_json_documents(filepath: str, chunk_strategy: str = 'entity') -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create JSON documents with advanced contextual processing strategies.\n",
    "    \n",
    "    This function implements multiple strategies for creating documents from\n",
    "    JSON data based on different business logic approaches.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to JSON file\n",
    "        chunk_strategy (str): Strategy for document creation\n",
    "            - 'entity': One document per business entity\n",
    "            - 'flat': Flatten and create comprehensive documents\n",
    "            - 'contextual': Preserve relationships and context\n",
    "            \n",
    "    Returns:\n",
    "        List[Document]: Optimized documents for RAG systems\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    if chunk_strategy == 'entity':\n",
    "        # Strategy: Create focused documents per business entity\n",
    "        for emp in data.get('employees', []):\n",
    "            content = f\"\"\"Employee: {emp['name']} (ID: {emp['id']})\n",
    "Role: {emp['role']}\n",
    "Skills: {', '.join(emp['skills'])}\n",
    "Active Projects: {len(emp.get('projects', []))}\n",
    "Company: {data.get('company', 'Unknown')}\"\"\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'entity_type': 'employee',\n",
    "                    'entity_id': emp['id'],\n",
    "                    'strategy': 'entity_focused',\n",
    "                    **{k: v for k, v in emp.items() if k not in ['projects']}\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "    elif chunk_strategy == 'flat':\n",
    "        # Strategy: Flatten entire structure and create comprehensive documents\n",
    "        flat_data = process_nested_json_with_flattening(data)\n",
    "        \n",
    "        # Group related flat keys into documents\n",
    "        employee_data = {k: v for k, v in flat_data.items() if 'employees' in k}\n",
    "        dept_data = {k: v for k, v in flat_data.items() if 'departments' in k}\n",
    "        \n",
    "        if employee_data:\n",
    "            content = \"Employee Information:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in employee_data.items()])\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={'data_type': 'flattened_employees', 'strategy': 'flattened'}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        if dept_data:\n",
    "            content = \"Department Information:\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in dept_data.items()])\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={'data_type': 'flattened_departments', 'strategy': 'flattened'}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "    elif chunk_strategy == 'contextual':\n",
    "        # Strategy: Preserve context and create relationship-aware documents\n",
    "        for emp in data.get('employees', []):\n",
    "            # Find department context\n",
    "            dept_context = \"\"\n",
    "            for dept_name, dept_info in data.get('departments', {}).items():\n",
    "                if emp.get('role', '').lower() in ['software engineer', 'data scientist']:\n",
    "                    if dept_name == 'engineering' and 'engineer' in emp.get('role', '').lower():\n",
    "                        dept_context = f\"\\nDepartment Context: {dept_name.title()} (Head: {dept_info.get('head')}, Budget: ${dept_info.get('budget', 0):,})\"\n",
    "                    elif dept_name == 'data_science' and 'data scientist' in emp.get('role', '').lower():\n",
    "                        dept_context = f\"\\nDepartment Context: {dept_name.title()} (Head: {dept_info.get('head')}, Budget: ${dept_info.get('budget', 0):,})\"\n",
    "            \n",
    "            content = f\"\"\"Employee Profile with Context:\n",
    "Name: {emp['name']}\n",
    "Role: {emp['role']}\n",
    "Skills: {', '.join(emp['skills'])}\n",
    "Projects: {[p['name'] for p in emp.get('projects', [])]}\n",
    "Company: {data.get('company', 'Unknown')}{dept_context}\n",
    "\n",
    "This employee works in {emp['role']} capacity with expertise in {', '.join(emp['skills'])}.\"\"\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'strategy': 'contextual',\n",
    "                    'employee_id': emp['id'],\n",
    "                    'has_dept_context': bool(dept_context),\n",
    "                    'context_richness': 'high'\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "print(\"üî¨ Advanced JSON Processing Techniques\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "# Test different strategies\n",
    "strategies = ['entity', 'flat', 'contextual']\n",
    "results = {}\n",
    "\n",
    "try:\n",
    "    for strategy in strategies:\n",
    "        docs = create_contextual_json_documents('data/json_files/company_data.json', strategy)\n",
    "        results[strategy] = docs\n",
    "        \n",
    "        print(f\"\\nüìä Strategy: {strategy.upper()}\")\n",
    "        print(f\"  ‚Ä¢ Documents created: {len(docs)}\")\n",
    "        print(f\"  ‚Ä¢ Avg content length: {sum(len(d.page_content) for d in docs) / len(docs) if docs else 0:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Example content preview:\")\n",
    "        if docs:\n",
    "            print(f\"    {docs[0].page_content[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nüí° Strategy Recommendations:\")\n",
    "    print(f\"  üéØ Entity Strategy: Best for focused queries about specific entities\")\n",
    "    print(f\"  üìä Flat Strategy: Good for comprehensive searches across all data\")\n",
    "    print(f\"  üß† Contextual Strategy: Optimal for relationship-aware questions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in advanced processing: {e}\")\n",
    "\n",
    "# Display flattening example\n",
    "print(f\"\\nüîç JSON Flattening Example:\")\n",
    "sample_nested = {\"user\": {\"profile\": {\"name\": \"John\", \"details\": {\"age\": 30}}}}\n",
    "flattened = process_nested_json_with_flattening(sample_nested)\n",
    "print(f\"  Original: {sample_nested}\")\n",
    "print(f\"  Flattened: {flattened}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecdb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Performance Analysis and Production Utilities\n",
      "================================================\n",
      "‚ö†Ô∏è  Run the JSON processing methods first to see performance metrics\n",
      "\n",
      "üè≠ Production JSON Processor Example:\n",
      "‚úÖ Production processing completed:\n",
      "  ‚Ä¢ Success: True\n",
      "  ‚Ä¢ Documents: 2\n",
      "  ‚Ä¢ Time: 0.001s\n",
      "  ‚Ä¢ Strategy: intelligent\n",
      "\n",
      "üéØ Production Best Practices:\n",
      "  ‚Ä¢ Always validate JSON structure before processing\n",
      "  ‚Ä¢ Include error handling and recovery mechanisms\n",
      "  ‚Ä¢ Monitor processing time and memory usage\n",
      "  ‚Ä¢ Use appropriate chunking strategies for large files\n",
      "  ‚Ä¢ Implement caching for frequently accessed data\n"
     ]
    }
   ],
   "source": [
    "# Performance Analysis and Production Utilities\n",
    "# ==============================================\n",
    "\"\"\"\n",
    "This section provides performance analysis and utility functions for production\n",
    "JSON processing in RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_json_processing_performance():\n",
    "    \"\"\"\n",
    "    Analyze memory usage and processing characteristics of different JSON approaches.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics for different processing methods\n",
    "    \"\"\"\n",
    "    performance_metrics = {}\n",
    "    \n",
    "    # Analyze JSONLoader documents if loaded\n",
    "    if 'employee_docs' in locals() and employee_docs:\n",
    "        performance_metrics['json_loader'] = {\n",
    "            'method': 'JSONLoader with jq',\n",
    "            'document_count': len(employee_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in employee_docs) / len(employee_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in employee_docs),\n",
    "            'metadata_richness': len(employee_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze custom processing documents if loaded\n",
    "    if 'custom_docs' in locals() and custom_docs:\n",
    "        performance_metrics['custom_processing'] = {\n",
    "            'method': 'Custom JSON Processing',\n",
    "            'document_count': len(custom_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in custom_docs) / len(custom_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in custom_docs),\n",
    "            'metadata_richness': len(custom_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze JSONL processing documents if loaded\n",
    "    if 'jsonl_docs' in locals() and jsonl_docs:\n",
    "        performance_metrics['jsonl_processing'] = {\n",
    "            'method': 'JSONL Event Processing',\n",
    "            'document_count': len(jsonl_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in jsonl_docs) / len(jsonl_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in jsonl_docs),\n",
    "            'metadata_richness': len(jsonl_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "def validate_json_structure(data: dict, required_fields: List[str]) -> tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate JSON structure against required fields.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): JSON data to validate\n",
    "        required_fields (List[str]): List of required field paths (dot notation)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_valid, missing_fields)\n",
    "        \n",
    "    Example:\n",
    "        valid, missing = validate_json_structure(data, ['company', 'employees.0.name'])\n",
    "    \"\"\"\n",
    "    missing_fields = []\n",
    "    \n",
    "    for field_path in required_fields:\n",
    "        parts = field_path.split('.')\n",
    "        current_data = data\n",
    "        \n",
    "        try:\n",
    "            for part in parts:\n",
    "                if part.isdigit():\n",
    "                    # Array index\n",
    "                    current_data = current_data[int(part)]\n",
    "                else:\n",
    "                    # Object key\n",
    "                    current_data = current_data[part]\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            missing_fields.append(field_path)\n",
    "    \n",
    "    return len(missing_fields) == 0, missing_fields\n",
    "\n",
    "def create_production_json_processor(validation_rules: dict = None):\n",
    "    \"\"\"\n",
    "    Factory function to create production-ready JSON processors.\n",
    "    \n",
    "    Args:\n",
    "        validation_rules (dict): Optional validation rules for JSON structure\n",
    "        \n",
    "    Returns:\n",
    "        function: Configured JSON processor function\n",
    "    \"\"\"\n",
    "    def process_json_production(filepath: str, strategy: str = 'intelligent') -> tuple[List[Document], dict]:\n",
    "        \"\"\"\n",
    "        Production JSON processor with error handling and metrics.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to JSON file\n",
    "            strategy (str): Processing strategy to use\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (documents, processing_metrics)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        processing_metrics = {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'file_path': filepath,\n",
    "            'strategy': strategy,\n",
    "            'success': False,\n",
    "            'error_message': None,\n",
    "            'documents_created': 0,\n",
    "            'processing_time_seconds': 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Load and validate JSON\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Optional validation\n",
    "            if validation_rules:\n",
    "                required_fields = validation_rules.get('required_fields', [])\n",
    "                is_valid, missing = validate_json_structure(data, required_fields)\n",
    "                if not is_valid:\n",
    "                    processing_metrics['error_message'] = f\"Missing required fields: {missing}\"\n",
    "                    return [], processing_metrics\n",
    "            \n",
    "            # Process based on strategy\n",
    "            if strategy == 'intelligent':\n",
    "                documents = process_json_intelligently(filepath)\n",
    "            elif strategy == 'flat':\n",
    "                documents = create_contextual_json_documents(filepath, 'flat')\n",
    "            elif strategy == 'contextual':\n",
    "                documents = create_contextual_json_documents(filepath, 'contextual')\n",
    "            else:\n",
    "                documents = process_json_intelligently(filepath)  # Default fallback\n",
    "            \n",
    "            # Update metrics\n",
    "            processing_metrics['success'] = True\n",
    "            processing_metrics['documents_created'] = len(documents)\n",
    "            processing_metrics['processing_time_seconds'] = time.time() - start_time\n",
    "            \n",
    "            return documents, processing_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_metrics['error_message'] = str(e)\n",
    "            processing_metrics['processing_time_seconds'] = time.time() - start_time\n",
    "            return [], processing_metrics\n",
    "    \n",
    "    return process_json_production\n",
    "\n",
    "print(\"‚ö° Performance Analysis and Production Utilities\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Run performance analysis\n",
    "try:\n",
    "    metrics = analyze_json_processing_performance()\n",
    "    \n",
    "    if metrics:\n",
    "        print(\"\\nüìä Processing Method Performance:\")\n",
    "        for method_key, data in metrics.items():\n",
    "            print(f\"\\n{data['method']}:\")\n",
    "            print(f\"  ‚Ä¢ Documents: {data['document_count']}\")\n",
    "            print(f\"  ‚Ä¢ Avg length: {data['avg_doc_length']:.1f} chars\")\n",
    "            print(f\"  ‚Ä¢ Memory: {data['total_memory']} bytes\")\n",
    "            print(f\"  ‚Ä¢ Metadata fields: {data['metadata_richness']}\")\n",
    "        \n",
    "        print(f\"\\nüí° Performance Insights:\")\n",
    "        print(f\"  ‚Ä¢ Custom processing creates more readable documents\")\n",
    "        print(f\"  ‚Ä¢ JSONL processing is memory efficient for large datasets\")\n",
    "        print(f\"  ‚Ä¢ Rich metadata enables better filtering and search\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Run the JSON processing methods first to see performance metrics\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in performance analysis: {e}\")\n",
    "\n",
    "# Test production processor\n",
    "print(f\"\\nüè≠ Production JSON Processor Example:\")\n",
    "try:\n",
    "    # Create production processor with validation\n",
    "    validation_rules = {\n",
    "        'required_fields': ['company', 'employees']\n",
    "    }\n",
    "    \n",
    "    production_processor = create_production_json_processor(validation_rules)\n",
    "    \n",
    "    # Process JSON with metrics\n",
    "    prod_docs, prod_metrics = production_processor('data/json_files/company_data.json', 'intelligent')\n",
    "    \n",
    "    print(f\"‚úÖ Production processing completed:\")\n",
    "    print(f\"  ‚Ä¢ Success: {prod_metrics['success']}\")\n",
    "    print(f\"  ‚Ä¢ Documents: {prod_metrics['documents_created']}\")\n",
    "    print(f\"  ‚Ä¢ Time: {prod_metrics['processing_time_seconds']:.3f}s\")\n",
    "    print(f\"  ‚Ä¢ Strategy: {prod_metrics['strategy']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in production processor: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Production Best Practices:\")\n",
    "print(f\"  ‚Ä¢ Always validate JSON structure before processing\")\n",
    "print(f\"  ‚Ä¢ Include error handling and recovery mechanisms\")\n",
    "print(f\"  ‚Ä¢ Monitor processing time and memory usage\")\n",
    "print(f\"  ‚Ä¢ Use appropriate chunking strategies for large files\")\n",
    "print(f\"  ‚Ä¢ Implement caching for frequently accessed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc68c04",
   "metadata": {},
   "source": [
    "## Best Practices for JSON Processing\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "1. **JSONLoader with jq Schema**\n",
    "   - ‚úÖ Quick extraction of specific fields from known structures\n",
    "   - ‚úÖ Consistent JSON schemas with predictable structure\n",
    "   - ‚úÖ Minimal processing overhead for simple extractions\n",
    "   - ‚ùå Complex business logic or relationship preservation\n",
    "\n",
    "2. **Custom JSON Processing**\n",
    "   - ‚úÖ Production RAG systems requiring high-quality documents\n",
    "   - ‚úÖ Complex nested structures with business relationships\n",
    "   - ‚úÖ Need for human-readable, context-rich content\n",
    "   - ‚úÖ Rich metadata requirements for advanced filtering\n",
    "\n",
    "3. **JSONL Processing**\n",
    "   - ‚úÖ Event streams and temporal data processing\n",
    "   - ‚úÖ Large datasets requiring memory-efficient processing\n",
    "   - ‚úÖ Log files and API response sequences\n",
    "   - ‚úÖ Streaming data pipelines\n",
    "\n",
    "### Common Challenges and Solutions\n",
    "\n",
    "1. **Large JSON File Performance**\n",
    "   - **Problem**: Memory issues with large nested JSON files\n",
    "   - **Solution**: Implement streaming JSON parsing or chunk processing\n",
    "   - **Code Pattern**: Process top-level arrays in batches\n",
    "\n",
    "2. **Complex Nested Structures**\n",
    "   - **Problem**: Deep nesting makes extraction difficult\n",
    "   - **Solution**: Use flattening utilities or recursive processing\n",
    "   - **Code Pattern**: Implement path-based field extraction\n",
    "\n",
    "3. **Inconsistent Schema**\n",
    "   - **Problem**: JSON structures vary across files\n",
    "   - **Solution**: Implement schema validation and flexible processing\n",
    "   - **Code Pattern**: Use try-catch blocks with fallback strategies\n",
    "\n",
    "4. **jq Query Complexity**\n",
    "   - **Problem**: Complex jq queries are hard to maintain\n",
    "   - **Solution**: Use custom processing for complex logic\n",
    "   - **Code Pattern**: Reserve jq for simple field extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735abd22",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this notebook, we explored comprehensive strategies for processing JSON data:\n",
    "\n",
    "1. **JSONLoader with jq**: Precise field extraction using query language\n",
    "2. **Custom Processing**: Intelligent document creation with rich context\n",
    "3. **JSONL Processing**: Efficient streaming data handling\n",
    "4. **Advanced Techniques**: Flattening, contextual processing, and validation\n",
    "5. **Production Utilities**: Error handling, performance monitoring, and best practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Choose the right approach**: Simple JSONLoader for basic extraction, custom processing for production\n",
    "- **Context matters**: Preserve relationships and business logic in document structure\n",
    "- **Metadata is crucial**: Rich metadata enables advanced filtering and retrieval\n",
    "- **Validation is essential**: Always validate JSON structure before processing\n",
    "- **Performance considerations**: Memory usage and processing time vary significantly between methods\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- [ ] Implement JSON schema validation\n",
    "- [ ] Add comprehensive error handling\n",
    "- [ ] Create context-rich, human-readable documents\n",
    "- [ ] Include actionable metadata for filtering\n",
    "- [ ] Monitor processing performance and memory usage\n",
    "- [ ] Implement caching for frequently accessed files\n",
    "- [ ] Use streaming processing for large JSON files\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try with your own JSON data**: Apply these techniques to your real datasets\n",
    "2. **Build a JSON processing pipeline**: Create automated workflows for different JSON types\n",
    "3. **Optimize for your queries**: Test different document structures with your specific questions\n",
    "4. **Scale for production**: Implement batch processing, monitoring, and error recovery\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [jq Manual](https://stedolan.github.io/jq/manual/) - Complete jq query language reference\n",
    "- [LangChain JSONLoader Documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/json)\n",
    "- [JSON Schema Specification](https://json-schema.org/) - For validation and structure definition\n",
    "- [Streaming JSON Processing](https://github.com/dcmoura/spyql) - Tools for large JSON file processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3338a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
