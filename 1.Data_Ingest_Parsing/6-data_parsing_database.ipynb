{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2c0790",
   "metadata": {},
   "source": [
    "# Database Processing with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive approaches to extracting and processing data from SQL databases for RAG applications. Databases are one of the most common sources of structured business data, and understanding how to effectively convert database content into documents is crucial for building enterprise-ready AI systems.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Database Setup** - Creating sample SQLite databases with realistic business data\n",
    "2. **SQLDatabaseLoader** - Using LangChain's built-in database utilities\n",
    "3. **Custom SQL Processing** - Building intelligent database-to-document converters\n",
    "4. **Relationship Extraction** - Preserving database relationships in document format\n",
    "5. **Query-Based Processing** - Converting SQL query results into contextual documents\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "uv add langchain-community sqlite3\n",
    "```\n",
    "\n",
    "## Common Database Use Cases in RAG\n",
    "- Customer relationship management (CRM) data\n",
    "- Product catalogs and inventory systems\n",
    "- Financial records and transaction histories\n",
    "- Employee and organizational data\n",
    "- Project management and task tracking\n",
    "- Order processing and e-commerce data\n",
    "- Configuration and settings management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9359102",
   "metadata": {},
   "source": [
    "### SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c083b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Directory structure created successfully\n",
      "ðŸ—„ï¸ Ready to create and process SQL databases\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Database Processing Setup for RAG Applications\n",
    "\n",
    "This module sets up the environment for processing SQL database data.\n",
    "We'll create a realistic SQLite database that represents common business scenarios\n",
    "with related tables and relationships.\n",
    "\n",
    "SQLite is perfect for demonstrations because:\n",
    "- No server setup required\n",
    "- File-based storage\n",
    "- Supports full SQL functionality\n",
    "- Easy to distribute and reproduce\n",
    "\n",
    "Author: Data Science Team\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries for database operations and file handling\n",
    "import sqlite3  # SQLite database interface for Python\n",
    "import os       # Operating system interface for directory operations\n",
    "\n",
    "# Create directory structure for storing our database files\n",
    "# exist_ok=True prevents error if directory already exists\n",
    "os.makedirs(\"data/databases\", exist_ok=True)\n",
    "print(\"âœ… Directory structure created successfully\")\n",
    "print(\"ðŸ—„ï¸ Ready to create and process SQL databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0c7ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Database connection established: data/databases/company.db\n",
      "ðŸ”§ Ready to create tables and populate with sample data\n"
     ]
    }
   ],
   "source": [
    "# Creating Sample SQLite Database\n",
    "# ===============================\n",
    "\"\"\"\n",
    "This section creates a comprehensive sample database that represents\n",
    "a typical business scenario with multiple related tables.\n",
    "\n",
    "The database structure includes:\n",
    "- Employees table: Staff information with roles and departments\n",
    "- Projects table: Project data with relationships to employees\n",
    "- Realistic data relationships that demonstrate foreign key concepts\n",
    "\"\"\"\n",
    "\n",
    "# Establish connection to SQLite database\n",
    "# If the file doesn't exist, SQLite will create it automatically\n",
    "database_path = 'data/databases/company.db'\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"âœ… Database connection established: {database_path}\")\n",
    "print(\"ðŸ”§ Ready to create tables and populate with sample data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e5a35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Employees table created successfully\n",
      "ðŸ“Š Table structure: id, name, role, department, salary\n"
     ]
    }
   ],
   "source": [
    "# Create Employees Table\n",
    "# ======================\n",
    "\"\"\"\n",
    "The employees table stores staff information and represents a common\n",
    "business entity found in most organizational databases.\n",
    "\n",
    "Table Structure:\n",
    "- id: Primary key (INTEGER) - Unique identifier for each employee\n",
    "- name: Employee full name (TEXT)\n",
    "- role: Job title/position (TEXT)\n",
    "- department: Organizational department (TEXT)\n",
    "- salary: Annual salary (REAL) - Numeric data for calculations\n",
    "\n",
    "Using IF NOT EXISTS prevents errors if the table already exists.\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS employees\n",
    "                 (id INTEGER PRIMARY KEY, \n",
    "                  name TEXT NOT NULL,\n",
    "                  role TEXT NOT NULL,\n",
    "                  department TEXT NOT NULL,\n",
    "                  salary REAL)''')\n",
    "\n",
    "print(\"âœ… Employees table created successfully\")\n",
    "print(\"ðŸ“Š Table structure: id, name, role, department, salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d217e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Projects table created successfully\n",
      "ðŸ“Š Table structure: id, name, status, budget, lead_id\n",
      "ðŸ”— Foreign key relationship: projects.lead_id â†’ employees.id\n"
     ]
    }
   ],
   "source": [
    "# Create Projects Table\n",
    "# =====================\n",
    "\"\"\"\n",
    "The projects table stores project information and demonstrates foreign key relationships.\n",
    "\n",
    "Table Structure:\n",
    "- id: Primary key (INTEGER) - Unique identifier for each project\n",
    "- name: Project name (TEXT)\n",
    "- status: Current project status (TEXT) - e.g., 'Active', 'Completed', 'Planning'\n",
    "- budget: Project budget allocation (REAL) - Numeric data for financial calculations\n",
    "- lead_id: Foreign key (INTEGER) - References employees.id for project leader\n",
    "\n",
    "This table demonstrates a one-to-many relationship: one employee can lead multiple projects.\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS projects\n",
    "                 (id INTEGER PRIMARY KEY, \n",
    "                  name TEXT NOT NULL,\n",
    "                  status TEXT NOT NULL,\n",
    "                  budget REAL,\n",
    "                  lead_id INTEGER,\n",
    "                  FOREIGN KEY (lead_id) REFERENCES employees (id))''')\n",
    "\n",
    "print(\"âœ… Projects table created successfully\")\n",
    "print(\"ðŸ“Š Table structure: id, name, status, budget, lead_id\")\n",
    "print(\"ðŸ”— Foreign key relationship: projects.lead_id â†’ employees.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a4c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample data defined successfully\n",
      "ðŸ“Š Employee records: 4 employees across 3 departments\n",
      "ðŸ“Š Project records: 4 projects with various statuses and budgets\n",
      "ðŸ”— Relationships: Each project has an assigned lead from the employees table\n"
     ]
    }
   ],
   "source": [
    "# Define Realistic Sample Data\n",
    "# ============================\n",
    "\"\"\"\n",
    "This section defines sample data that represents a realistic business scenario.\n",
    "The data demonstrates various aspects of organizational structure and project management.\n",
    "\n",
    "Employee Data Features:\n",
    "- Diverse roles across different departments\n",
    "- Realistic salary ranges for different positions\n",
    "- Mix of technical and non-technical roles\n",
    "\n",
    "Project Data Features:\n",
    "- Various project statuses (Active, Completed, Planning)\n",
    "- Different budget allocations\n",
    "- Clear leadership relationships through foreign keys\n",
    "\"\"\"\n",
    "\n",
    "# Sample employee data with realistic business information\n",
    "# Format: (id, name, role, department, salary)\n",
    "employees = [\n",
    "    (1, 'John Doe', 'Senior Developer', 'Engineering', 95000),      # Senior technical role\n",
    "    (2, 'Jane Smith', 'Data Scientist', 'Analytics', 105000),       # Specialized analytics role\n",
    "    (3, 'Mike Johnson', 'Product Manager', 'Product', 110000),      # Management role\n",
    "    (4, 'Sarah Williams', 'DevOps Engineer', 'Engineering', 98000)  # Infrastructure role\n",
    "]\n",
    "\n",
    "# Sample project data with relationships to employees\n",
    "# Format: (id, name, status, budget, lead_id)\n",
    "projects = [\n",
    "    (1, 'RAG Implementation', 'Active', 150000, 1),      # John leads RAG project\n",
    "    (2, 'Data Pipeline', 'Completed', 80000, 2),         # Jane completed data pipeline\n",
    "    (3, 'Customer Portal', 'Planning', 200000, 3),       # Mike planning customer portal\n",
    "    (4, 'ML Platform', 'Active', 250000, 2)              # Jane leads ML platform (shows one-to-many)\n",
    "]\n",
    "\n",
    "print(\"âœ… Sample data defined successfully\")\n",
    "print(\"ðŸ“Š Employee records: 4 employees across 3 departments\")\n",
    "print(\"ðŸ“Š Project records: 4 projects with various statuses and budgets\")\n",
    "print(\"ðŸ”— Relationships: Each project has an assigned lead from the employees table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbc344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inserted 4 employee records\n",
      "âœ… Inserted 4 project records\n",
      "ðŸ”— Foreign key relationships established between employees and projects\n"
     ]
    }
   ],
   "source": [
    "# Insert Sample Data into Tables\n",
    "# ===============================\n",
    "\"\"\"\n",
    "This section populates the tables with our sample data.\n",
    "\n",
    "Using executemany() for efficient batch insertion.\n",
    "INSERT OR REPLACE ensures idempotent operations - safe to run multiple times.\n",
    "\"\"\"\n",
    "\n",
    "# Insert employee data - batch operation for efficiency\n",
    "cursor.executemany('INSERT OR REPLACE INTO employees VALUES (?,?,?,?,?)', employees)\n",
    "print(f\"âœ… Inserted {len(employees)} employee records\")\n",
    "\n",
    "# Insert project data - demonstrates foreign key relationships\n",
    "cursor.executemany('INSERT OR REPLACE INTO projects VALUES (?,?,?,?,?)', projects)\n",
    "print(f\"âœ… Inserted {len(projects)} project records\")\n",
    "print(\"ðŸ”— Foreign key relationships established between employees and projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4c7567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Employee table contents:\n",
      "  (1, 'John Doe', 'Senior Developer', 'Engineering', 95000.0)\n",
      "  (2, 'Jane Smith', 'Data Scientist', 'Analytics', 105000.0)\n",
      "  (3, 'Mike Johnson', 'Product Manager', 'Product', 110000.0)\n",
      "  (4, 'Sarah Williams', 'DevOps Engineer', 'Engineering', 98000.0)\n"
     ]
    }
   ],
   "source": [
    "# Verify database content by querying employees table\n",
    "# This allows us to see the data that was inserted\n",
    "cursor.execute(\"SELECT * FROM employees\")\n",
    "print(\"ðŸ“Š Employee table contents:\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c218fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Database setup completed successfully\n",
      "ðŸ’¾ All changes committed and connection closed\n",
      "ðŸ—„ï¸ Database ready for processing with LangChain loaders\n"
     ]
    }
   ],
   "source": [
    "# Commit changes and close connection\n",
    "# ====================================\n",
    "# commit() saves all changes made during this session\n",
    "conn.commit()\n",
    "\n",
    "# close() properly closes the database connection to free up resources\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Database setup completed successfully\")\n",
    "print(\"ðŸ’¾ All changes committed and connection closed\")\n",
    "print(\"ðŸ—„ï¸ Database ready for processing with LangChain loaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255af47d",
   "metadata": {},
   "source": [
    "## Database Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8580bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain Database Utilities\n",
    "# ====================================\n",
    "\"\"\"\n",
    "This section imports the necessary LangChain utilities for database processing.\n",
    "\n",
    "SQLDatabase: Provides a high-level interface to SQL databases\n",
    "- Connection management and query execution\n",
    "- Schema inspection and table information\n",
    "- Safety features for production use\n",
    "\n",
    "SQLDatabaseLoader: Converts database content to Document objects\n",
    "- Table-based document creation\n",
    "- Query-based document extraction\n",
    "- Metadata preservation for filtering and search\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.utilities import SQLDatabase  # Database utility for connection and queries\n",
    "from langchain_community.document_loaders import SQLDatabaseLoader  # Document loader for SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a46fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ï¸âƒ£ SQLDatabase Utility - Database Schema Analysis\n",
      "--------------------------------------------------\n",
      "ðŸ“Š Available tables: ['employees', 'projects']\n",
      "\n",
      "ðŸ” Database Schema Information:\n",
      "\n",
      "CREATE TABLE employees (\n",
      "\tid INTEGER, \n",
      "\tname TEXT, \n",
      "\trole TEXT, \n",
      "\tdepartment TEXT, \n",
      "\tsalary REAL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from employees table:\n",
      "id\tname\trole\tdepartment\tsalary\n",
      "1\tJohn Doe\tSenior Developer\tEngineering\t95000.0\n",
      "2\tJane Smith\tData Scientist\tAnalytics\t105000.0\n",
      "3\tMike Johnson\tProduct Manager\tProduct\t110000.0\n",
      "*/\n",
      "\n",
      "\n",
      "CREATE TABLE projects (\n",
      "\tid INTEGER, \n",
      "\tname TEXT, \n",
      "\tstatus TEXT, \n",
      "\tbudget REAL, \n",
      "\tlead_id INTEGER, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from projects table:\n",
      "id\tname\tstatus\tbudget\tlead_id\n",
      "1\tRAG Implementation\tActive\t150000.0\t1\n",
      "2\tData Pipeline\tCompleted\t80000.0\t2\n",
      "3\tCustomer Portal\tPlanning\t200000.0\t3\n",
      "*/\n",
      "\n",
      "ðŸ“‹ Sample Query Results:\n",
      "Query: SELECT name, role, department FROM employees LIMIT 3\n",
      "Results: [('John Doe', 'Senior Developer', 'Engineering'), ('Jane Smith', 'Data Scientist', 'Analytics'), ('Mike Johnson', 'Product Manager', 'Product')]\n",
      "\n",
      "âœ… Database connection and inspection successful\n",
      "ðŸ’¡ Ready for document creation and processing\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using SQLDatabase Utility for Database Inspection\n",
    "# ===========================================================\n",
    "\"\"\"\n",
    "SQLDatabase provides a high-level interface for database operations.\n",
    "It's useful for:\n",
    "- Schema inspection and understanding database structure\n",
    "- Safe query execution with built-in protections\n",
    "- Integration with LangChain's SQL tools and agents\n",
    "\n",
    "Pros: Built-in safety features, schema inspection, LangChain integration\n",
    "Cons: Less control over document creation, standardized output format\n",
    "\"\"\"\n",
    "\n",
    "print(\"1ï¸âƒ£ SQLDatabase Utility - Database Schema Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Connect to the SQLite database using LangChain's SQLDatabase wrapper\n",
    "    # The from_uri method creates a connection from a database URI\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///data/databases/company.db\")\n",
    "    \n",
    "    # Get list of available tables in the database\n",
    "    # This is useful for understanding the database structure\n",
    "    tables = db.get_usable_table_names()\n",
    "    print(f\"ðŸ“Š Available tables: {tables}\")\n",
    "    \n",
    "    # Get detailed table information including schema and relationships\n",
    "    # This provides DDL (Data Definition Language) statements for tables\n",
    "    print(f\"\\nðŸ” Database Schema Information:\")\n",
    "    print(f\"{db.get_table_info()}\")\n",
    "    \n",
    "    # Test a sample query to verify database connectivity and data\n",
    "    print(f\"\\nðŸ“‹ Sample Query Results:\")\n",
    "    sample_query = \"SELECT name, role, department FROM employees LIMIT 3\"\n",
    "    result = db.run(sample_query)\n",
    "    print(f\"Query: {sample_query}\")\n",
    "    print(f\"Results: {result}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Database connection and inspection successful\")\n",
    "    print(f\"ðŸ’¡ Ready for document creation and processing\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to database: {e}\")\n",
    "    print(\"ðŸ’¡ Ensure the database file exists and was created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efad2c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2ï¸âƒ£ Custom SQL Processing - Intelligent Database-to-Document Conversion\n",
      "----------------------------------------------------------------------\n",
      "ðŸ”— Extracting database relationships...\n",
      "âœ… Custom SQL processing completed\n",
      "ðŸ“Š Created 3 documents from database\n",
      "\n",
      "ðŸ“‹ Document Analysis:\n",
      "  â€¢ sql_table_overview: 2 documents\n",
      "  â€¢ sql_relationships: 1 documents\n",
      "\n",
      "ðŸ“„ Example document preview:\n",
      "Content (first 300 chars):\n",
      "Database Table: employees\n",
      "\n",
      "Schema Information:\n",
      "â€¢ id INTEGER (PRIMARY KEY)\n",
      "â€¢ name TEXT\n",
      "â€¢ role TEXT\n",
      "â€¢ department TEXT\n",
      "â€¢ salary REAL\n",
      "\n",
      "Total Records: 4\n",
      "Columns: 5\n",
      "\n",
      "Sample Data (first 5 records):\n",
      "\n",
      "Record 1:\n",
      "  â€¢ id: 1\n",
      "  â€¢ name: John Doe\n",
      "  â€¢ role: Senior Developer\n",
      "  â€¢ department: Engineering\n",
      "  â€¢ salary: 95...\n",
      "Metadata keys: ['source', 'table_name', 'num_records', 'num_columns', 'column_names', 'data_type', 'content_type']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Custom SQL Processing for Enhanced Document Creation\n",
    "# =============================================================\n",
    "\"\"\"\n",
    "This section demonstrates custom SQL processing that provides more control over\n",
    "how database content is converted into documents for RAG applications.\n",
    "\n",
    "Benefits of custom processing:\n",
    "- Table-aware document creation with schema information\n",
    "- Relationship extraction and preservation\n",
    "- Rich metadata creation for filtering and search\n",
    "- Flexible document structures based on data types\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Custom SQL Processing - Intelligent Database-to-Document Conversion\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def sql_to_documents(db_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Convert SQL Database to documents with preserved context and relationships.\n",
    "    \n",
    "    This function creates structured documents from database content with\n",
    "    enhanced formatting and metadata for better RAG performance.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of Document objects with structured content\n",
    "        \n",
    "    Features:\n",
    "        - Table-aware document creation\n",
    "        - Schema information preservation\n",
    "        - Relationship extraction between tables\n",
    "        - Rich metadata for filtering and search\n",
    "        - Sample data inclusion for context\n",
    "    \n",
    "    Example:\n",
    "        docs = sql_to_documents(\"company.db\")\n",
    "        print(f\"Created {len(docs)} documents from database\")\n",
    "    \"\"\"\n",
    "    # Establish connection to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    documents = []\n",
    "    \n",
    "    # Strategy 1: Create comprehensive table overview documents\n",
    "    # Get all table names from the database\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        \n",
    "        # Get detailed table schema information\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        column_info = []\n",
    "        for col in columns:\n",
    "            col_name, col_type, not_null, default_val, primary_key = col[1], col[2], col[3], col[4], col[5]\n",
    "            pk_indicator = \" (PRIMARY KEY)\" if primary_key else \"\"\n",
    "            nn_indicator = \" NOT NULL\" if not_null else \"\"\n",
    "            column_info.append(f\"{col_name} {col_type}{pk_indicator}{nn_indicator}\")\n",
    "        \n",
    "        # Get all table data for content and analysis\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        rows = cursor.fetchall()\n",
    "        column_names = [col[1] for col in columns]\n",
    "        \n",
    "        # Create comprehensive table documentation\n",
    "        table_content = f\"\"\"Database Table: {table_name}\n",
    "        \n",
    "Schema Information:\n",
    "{chr(10).join([f\"â€¢ {info}\" for info in column_info])}\n",
    "\n",
    "Total Records: {len(rows)}\n",
    "Columns: {len(column_names)}\n",
    "\n",
    "Sample Data (first 5 records):\"\"\"\n",
    "        \n",
    "        # Add formatted sample records for better readability\n",
    "        for i, row in enumerate(rows[:5], 1):\n",
    "            table_content += f\"\\n\\nRecord {i}:\"\n",
    "            record = dict(zip(column_names, row))\n",
    "            for key, value in record.items():\n",
    "                table_content += f\"\\n  â€¢ {key}: {value}\"\n",
    "        \n",
    "        # Create document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=table_content,\n",
    "            metadata={\n",
    "                'source': db_path,\n",
    "                'table_name': table_name,\n",
    "                'num_records': len(rows),\n",
    "                'num_columns': len(column_names),\n",
    "                'column_names': column_names,\n",
    "                'data_type': 'sql_table_overview',\n",
    "                'content_type': 'structured_database'\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Strategy 2: Create relationship documents using SQL joins\n",
    "    # This demonstrates how to extract and preserve database relationships\n",
    "    print(f\"ðŸ”— Extracting database relationships...\")\n",
    "    \n",
    "    try:\n",
    "        # Join employees and projects to show relationships\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                e.name as employee_name, \n",
    "                e.role as employee_role, \n",
    "                e.department as employee_dept,\n",
    "                e.salary as employee_salary,\n",
    "                p.name as project_name, \n",
    "                p.status as project_status,\n",
    "                p.budget as project_budget\n",
    "            FROM employees e\n",
    "            JOIN projects p ON e.id = p.lead_id\n",
    "        \"\"\")\n",
    "        \n",
    "        relationships = cursor.fetchall()\n",
    "        \n",
    "        if relationships:\n",
    "            # Create a comprehensive relationship document\n",
    "            rel_content = \"\"\"Employee-Project Relationships\n",
    "\n",
    "This document contains information about employees and the projects they lead,\n",
    "demonstrating organizational structure and project assignments.\n",
    "\n",
    "Project Leadership Details:\n",
    "\"\"\"\n",
    "            \n",
    "            for rel in relationships:\n",
    "                employee_name, employee_role, employee_dept, employee_salary, project_name, project_status, project_budget = rel\n",
    "                rel_content += f\"\"\"\n",
    "â€¢ {employee_name} ({employee_role})\n",
    "  Department: {employee_dept}\n",
    "  Salary: ${employee_salary:,}\n",
    "  Leads Project: {project_name}\n",
    "  Project Status: {project_status}\n",
    "  Project Budget: ${project_budget:,}\n",
    "\"\"\"\n",
    "            \n",
    "            # Add summary statistics\n",
    "            total_budget = sum([rel[6] for rel in relationships])  # project_budget is index 6\n",
    "            active_projects = len([rel for rel in relationships if rel[5] == 'Active'])  # project_status is index 5\n",
    "            \n",
    "            rel_content += f\"\"\"\n",
    "\n",
    "Summary Statistics:\n",
    "â€¢ Total projects with assigned leads: {len(relationships)}\n",
    "â€¢ Active projects: {active_projects}\n",
    "â€¢ Total combined project budget: ${total_budget:,}\n",
    "â€¢ Departments involved: {len(set([rel[2] for rel in relationships]))}\n",
    "\"\"\"\n",
    "            \n",
    "            rel_doc = Document(\n",
    "                page_content=rel_content,\n",
    "                metadata={\n",
    "                    'source': db_path,\n",
    "                    'data_type': 'sql_relationships',\n",
    "                    'query_type': 'employee_project_join',\n",
    "                    'relationship_count': len(relationships),\n",
    "                    'total_budget': total_budget,\n",
    "                    'active_projects': active_projects,\n",
    "                    'content_type': 'organizational_relationships'\n",
    "                }\n",
    "            )\n",
    "            documents.append(rel_doc)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Warning: Could not extract relationships - {e}\")\n",
    "    \n",
    "    # Clean up database connection\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"âœ… Custom SQL processing completed\")\n",
    "    print(f\"ðŸ“Š Created {len(documents)} documents from database\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test the custom SQL processing function\n",
    "try:\n",
    "    custom_sql_docs = sql_to_documents(\"data/databases/company.db\")\n",
    "    \n",
    "    # Analyze the created documents\n",
    "    print(f\"\\nðŸ“‹ Document Analysis:\")\n",
    "    doc_types = {}\n",
    "    for doc in custom_sql_docs:\n",
    "        doc_type = doc.metadata.get('data_type', 'unknown')\n",
    "        if doc_type not in doc_types:\n",
    "            doc_types[doc_type] = 0\n",
    "        doc_types[doc_type] += 1\n",
    "    \n",
    "    for doc_type, count in doc_types.items():\n",
    "        print(f\"  â€¢ {doc_type}: {count} documents\")\n",
    "    \n",
    "    # Show example document\n",
    "    if custom_sql_docs:\n",
    "        print(f\"\\nðŸ“„ Example document preview:\")\n",
    "        print(f\"Content (first 300 chars):\\n{custom_sql_docs[0].page_content[:300]}...\")\n",
    "        print(f\"Metadata keys: {list(custom_sql_docs[0].metadata.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in custom SQL processing: {e}\")\n",
    "    print(\"ðŸ’¡ Ensure the database was created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af5ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Extracting database relationships...\n",
      "âœ… Custom SQL processing completed\n",
      "ðŸ“Š Created 3 documents from database\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/databases/company.db', 'table_name': 'employees', 'num_records': 4, 'num_columns': 5, 'column_names': ['id', 'name', 'role', 'department', 'salary'], 'data_type': 'sql_table_overview', 'content_type': 'structured_database'}, page_content='Database Table: employees\\n\\nSchema Information:\\nâ€¢ id INTEGER (PRIMARY KEY)\\nâ€¢ name TEXT\\nâ€¢ role TEXT\\nâ€¢ department TEXT\\nâ€¢ salary REAL\\n\\nTotal Records: 4\\nColumns: 5\\n\\nSample Data (first 5 records):\\n\\nRecord 1:\\n  â€¢ id: 1\\n  â€¢ name: John Doe\\n  â€¢ role: Senior Developer\\n  â€¢ department: Engineering\\n  â€¢ salary: 95000.0\\n\\nRecord 2:\\n  â€¢ id: 2\\n  â€¢ name: Jane Smith\\n  â€¢ role: Data Scientist\\n  â€¢ department: Analytics\\n  â€¢ salary: 105000.0\\n\\nRecord 3:\\n  â€¢ id: 3\\n  â€¢ name: Mike Johnson\\n  â€¢ role: Product Manager\\n  â€¢ department: Product\\n  â€¢ salary: 110000.0\\n\\nRecord 4:\\n  â€¢ id: 4\\n  â€¢ name: Sarah Williams\\n  â€¢ role: DevOps Engineer\\n  â€¢ department: Engineering\\n  â€¢ salary: 98000.0'),\n",
       " Document(metadata={'source': 'data/databases/company.db', 'table_name': 'projects', 'num_records': 4, 'num_columns': 5, 'column_names': ['id', 'name', 'status', 'budget', 'lead_id'], 'data_type': 'sql_table_overview', 'content_type': 'structured_database'}, page_content='Database Table: projects\\n\\nSchema Information:\\nâ€¢ id INTEGER (PRIMARY KEY)\\nâ€¢ name TEXT\\nâ€¢ status TEXT\\nâ€¢ budget REAL\\nâ€¢ lead_id INTEGER\\n\\nTotal Records: 4\\nColumns: 5\\n\\nSample Data (first 5 records):\\n\\nRecord 1:\\n  â€¢ id: 1\\n  â€¢ name: RAG Implementation\\n  â€¢ status: Active\\n  â€¢ budget: 150000.0\\n  â€¢ lead_id: 1\\n\\nRecord 2:\\n  â€¢ id: 2\\n  â€¢ name: Data Pipeline\\n  â€¢ status: Completed\\n  â€¢ budget: 80000.0\\n  â€¢ lead_id: 2\\n\\nRecord 3:\\n  â€¢ id: 3\\n  â€¢ name: Customer Portal\\n  â€¢ status: Planning\\n  â€¢ budget: 200000.0\\n  â€¢ lead_id: 3\\n\\nRecord 4:\\n  â€¢ id: 4\\n  â€¢ name: ML Platform\\n  â€¢ status: Active\\n  â€¢ budget: 250000.0\\n  â€¢ lead_id: 2'),\n",
       " Document(metadata={'source': 'data/databases/company.db', 'data_type': 'sql_relationships', 'query_type': 'employee_project_join', 'relationship_count': 4, 'total_budget': 680000.0, 'active_projects': 2, 'content_type': 'organizational_relationships'}, page_content='Employee-Project Relationships\\n\\nThis document contains information about employees and the projects they lead,\\ndemonstrating organizational structure and project assignments.\\n\\nProject Leadership Details:\\n\\nâ€¢ John Doe (Senior Developer)\\n  Department: Engineering\\n  Salary: $95,000.0\\n  Leads Project: RAG Implementation\\n  Project Status: Active\\n  Project Budget: $150,000.0\\n\\nâ€¢ Jane Smith (Data Scientist)\\n  Department: Analytics\\n  Salary: $105,000.0\\n  Leads Project: Data Pipeline\\n  Project Status: Completed\\n  Project Budget: $80,000.0\\n\\nâ€¢ Mike Johnson (Product Manager)\\n  Department: Product\\n  Salary: $110,000.0\\n  Leads Project: Customer Portal\\n  Project Status: Planning\\n  Project Budget: $200,000.0\\n\\nâ€¢ Jane Smith (Data Scientist)\\n  Department: Analytics\\n  Salary: $105,000.0\\n  Leads Project: ML Platform\\n  Project Status: Active\\n  Project Budget: $250,000.0\\n\\n\\nSummary Statistics:\\nâ€¢ Total projects with assigned leads: 4\\nâ€¢ Active projects: 2\\nâ€¢ Total combined project budget: $680,000.0\\nâ€¢ Departments involved: 3\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the custom SQL processing function directly\n",
    "# This will return the list of documents for inspection\n",
    "sql_to_documents(\"data/databases/company.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260b70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3ï¸âƒ£ SQLDatabaseLoader - Direct Database-to-Document Conversion\n",
      "------------------------------------------------------------\n",
      "âœ… SQLDatabaseLoader query processing completed\n",
      "ðŸ“Š Created 4 documents from query\n",
      "\n",
      "ðŸ“„ Query-based document example:\n",
      "Content (first 200 chars):\n",
      "name: John Doe\n",
      "role: Senior Developer\n",
      "department: Engineering\n",
      "salary: 95000.0...\n",
      "Metadata keys: []\n",
      "\n",
      "ðŸ”— Relationship query processing completed\n",
      "ðŸ“Š Created 4 relationship documents\n",
      "\n",
      "ðŸ“„ Relationship document example:\n",
      "Content:\n",
      "employee_info: Jane Smith (Data Scientist)\n",
      "project_name: Data Pipeline\n",
      "project_status: Completed\n",
      "project_budget: 80000.0\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Using SQLDatabaseLoader for Direct Document Creation\n",
    "# =============================================================\n",
    "\"\"\"\n",
    "SQLDatabaseLoader provides a direct way to convert database tables into documents.\n",
    "It's designed to work seamlessly with LangChain's document processing pipeline.\n",
    "\n",
    "Benefits:\n",
    "- Automatic table-to-document conversion\n",
    "- Built-in metadata handling\n",
    "- Integration with LangChain document chains\n",
    "- Simple API for quick implementation\n",
    "\"\"\"\n",
    "\n",
    "print(\"3ï¸âƒ£ SQLDatabaseLoader - Direct Database-to-Document Conversion\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Method 3a: Create documents from specific SQL query\n",
    "    loader = SQLDatabaseLoader(\n",
    "        query=\"SELECT name, role, department, salary FROM employees\",\n",
    "        db=db  # Use the SQLDatabase instance created earlier\n",
    "    )\n",
    "    \n",
    "    query_docs = loader.load()\n",
    "    \n",
    "    print(f\"âœ… SQLDatabaseLoader query processing completed\")\n",
    "    print(f\"ðŸ“Š Created {len(query_docs)} documents from query\")\n",
    "    \n",
    "    if query_docs:\n",
    "        print(f\"\\nðŸ“„ Query-based document example:\")\n",
    "        print(f\"Content (first 200 chars):\\n{query_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata keys: {list(query_docs[0].metadata.keys())}\")\n",
    "    \n",
    "    # Method 3b: Process join query for relationship extraction\n",
    "    relationship_loader = SQLDatabaseLoader(\n",
    "        query=\"\"\"\n",
    "        SELECT \n",
    "            e.name || ' (' || e.role || ')' as employee_info,\n",
    "            p.name as project_name,\n",
    "            p.status as project_status,\n",
    "            p.budget as project_budget\n",
    "        FROM employees e\n",
    "        JOIN projects p ON e.id = p.lead_id\n",
    "        ORDER BY e.name\n",
    "        \"\"\",\n",
    "        db=db\n",
    "    )\n",
    "    \n",
    "    relationship_docs = relationship_loader.load()\n",
    "    \n",
    "    print(f\"\\nðŸ”— Relationship query processing completed\")\n",
    "    print(f\"ðŸ“Š Created {len(relationship_docs)} relationship documents\")\n",
    "    \n",
    "    if relationship_docs:\n",
    "        print(f\"\\nðŸ“„ Relationship document example:\")\n",
    "        print(f\"Content:\\n{relationship_docs[0].page_content}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error with SQLDatabaseLoader: {e}\")\n",
    "    print(\"ðŸ’¡ Ensure the database connection is established successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf3ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Database Processing Methods Comparison\n",
      "=============================================\n",
      "\n",
      "1ï¸âƒ£ Custom SQL Processing:\n",
      "  â€¢ Documents created: 3\n",
      "  â€¢ Content approach: Table-aware with schema information\n",
      "  â€¢ Best for: Complex databases with multiple relationships\n",
      "  â€¢ Metadata richness: 7 fields\n",
      "\n",
      "2ï¸âƒ£ SQLDatabaseLoader Query-based:\n",
      "  â€¢ Documents created: 4\n",
      "  â€¢ Content approach: Direct query result conversion\n",
      "  â€¢ Best for: Specific data extraction with known requirements\n",
      "  â€¢ Metadata richness: 0 fields\n",
      "\n",
      "3ï¸âƒ£ SQLDatabaseLoader Relationship Extraction:\n",
      "  â€¢ Documents created: 4\n",
      "  â€¢ Content approach: JOIN-based relationship preservation\n",
      "  â€¢ Best for: Maintaining data relationships and context\n",
      "  â€¢ Metadata richness: 0 fields\n",
      "\n",
      "ðŸ“ˆ Performance Analysis:\n",
      "  â€¢ Custom processing content size: 2266 characters\n",
      "  â€¢ Query-based content size: 312 characters\n",
      "  â€¢ Relationship extraction size: 484 characters\n",
      "\n",
      "ðŸ’¡ Method Selection Guidelines:\n",
      "  ðŸ—„ï¸  Custom SQL Processing:\n",
      "     - Complex databases with multiple tables and relationships\n",
      "     - Need for comprehensive schema information\n",
      "     - Production RAG systems requiring rich context\n",
      "  ðŸ” SQLDatabaseLoader Query-based:\n",
      "     - Specific data extraction requirements\n",
      "     - Known database schema and query patterns\n",
      "     - Simple integration with LangChain pipelines\n",
      "  ðŸ”— Relationship-aware Processing:\n",
      "     - Preserving foreign key relationships\n",
      "     - Cross-table context requirements\n",
      "     - Business intelligence and reporting scenarios\n"
     ]
    }
   ],
   "source": [
    "# Database Processing Methods Comparison and Analysis\n",
    "# ====================================================\n",
    "\"\"\"\n",
    "This section compares different database processing approaches to help you choose\n",
    "the best strategy for your specific database structure and use case.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š Database Processing Methods Comparison\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Compare results if all methods have been executed\n",
    "try:\n",
    "    if 'custom_sql_docs' in locals() and 'query_docs' in locals() and 'relationship_docs' in locals():\n",
    "        print(\"\\n1ï¸âƒ£ Custom SQL Processing:\")\n",
    "        print(f\"  â€¢ Documents created: {len(custom_sql_docs)}\")\n",
    "        print(f\"  â€¢ Content approach: Table-aware with schema information\")\n",
    "        print(f\"  â€¢ Best for: Complex databases with multiple relationships\")\n",
    "        print(f\"  â€¢ Metadata richness: {len(custom_sql_docs[0].metadata) if custom_sql_docs else 0} fields\")\n",
    "        \n",
    "        print(\"\\n2ï¸âƒ£ SQLDatabaseLoader Query-based:\")\n",
    "        print(f\"  â€¢ Documents created: {len(query_docs)}\")\n",
    "        print(f\"  â€¢ Content approach: Direct query result conversion\")\n",
    "        print(f\"  â€¢ Best for: Specific data extraction with known requirements\")\n",
    "        print(f\"  â€¢ Metadata richness: {len(query_docs[0].metadata) if query_docs else 0} fields\")\n",
    "        \n",
    "        print(\"\\n3ï¸âƒ£ SQLDatabaseLoader Relationship Extraction:\")\n",
    "        print(f\"  â€¢ Documents created: {len(relationship_docs)}\")\n",
    "        print(f\"  â€¢ Content approach: JOIN-based relationship preservation\")\n",
    "        print(f\"  â€¢ Best for: Maintaining data relationships and context\")\n",
    "        print(f\"  â€¢ Metadata richness: {len(relationship_docs[0].metadata) if relationship_docs else 0} fields\")\n",
    "        \n",
    "        # Performance analysis\n",
    "        print(f\"\\nðŸ“ˆ Performance Analysis:\")\n",
    "        custom_memory = sum(len(doc.page_content) for doc in custom_sql_docs) if custom_sql_docs else 0\n",
    "        query_memory = sum(len(doc.page_content) for doc in query_docs) if query_docs else 0\n",
    "        rel_memory = sum(len(doc.page_content) for doc in relationship_docs) if relationship_docs else 0\n",
    "        \n",
    "        print(f\"  â€¢ Custom processing content size: {custom_memory} characters\")\n",
    "        print(f\"  â€¢ Query-based content size: {query_memory} characters\")\n",
    "        print(f\"  â€¢ Relationship extraction size: {rel_memory} characters\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  Run all processing methods first to see comparison\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in comparison analysis: {e}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Method Selection Guidelines:\")\n",
    "print(f\"  ðŸ—„ï¸  Custom SQL Processing:\")\n",
    "print(f\"     - Complex databases with multiple tables and relationships\")\n",
    "print(f\"     - Need for comprehensive schema information\")\n",
    "print(f\"     - Production RAG systems requiring rich context\")\n",
    "\n",
    "print(f\"  ðŸ” SQLDatabaseLoader Query-based:\")\n",
    "print(f\"     - Specific data extraction requirements\")\n",
    "print(f\"     - Known database schema and query patterns\")\n",
    "print(f\"     - Simple integration with LangChain pipelines\")\n",
    "\n",
    "print(f\"  ðŸ”— Relationship-aware Processing:\")\n",
    "print(f\"     - Preserving foreign key relationships\")\n",
    "print(f\"     - Cross-table context requirements\")\n",
    "print(f\"     - Business intelligence and reporting scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9199290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Advanced Database Processing Techniques\n",
      "==========================================\n",
      "\n",
      "ðŸ“Š Strategy: COMPREHENSIVE\n",
      "  â€¢ Success: True\n",
      "  â€¢ Documents: 2\n",
      "  â€¢ Processing time: 0.001s\n",
      "  â€¢ Tables processed: 2\n",
      "  â€¢ Avg document length: 683 chars\n",
      "\n",
      "ðŸ“Š Strategy: RELATIONSHIPS\n",
      "  â€¢ Success: True\n",
      "  â€¢ Documents: 4\n",
      "  â€¢ Processing time: 0.000s\n",
      "  â€¢ Tables processed: 2\n",
      "  â€¢ Avg document length: 406 chars\n",
      "\n",
      "ðŸ’¡ Advanced Strategy Benefits:\n",
      "  ðŸ” Comprehensive: Rich context with business insights\n",
      "  ðŸ”— Relationships: Cross-table connections and leadership analysis\n",
      "\n",
      "ðŸ” Database Structure Validation:\n",
      "  âœ… Valid database: True\n",
      "  ðŸ“Š Tables found: 2\n",
      "  ðŸ”— Relationships found: 0\n",
      "  ðŸ’¡ Recommendations:\n",
      "    â€¢ Consider adding foreign keys for better data relationships\n"
     ]
    }
   ],
   "source": [
    "# Advanced Database Processing Techniques\n",
    "# ========================================\n",
    "\"\"\"\n",
    "This section demonstrates advanced techniques for handling complex database scenarios\n",
    "commonly encountered in production RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def create_contextual_sql_processor(db_path: str, table_context_rules: dict = None):\n",
    "    \"\"\"\n",
    "    Factory function to create production-ready SQL processors with configurable rules.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database file\n",
    "        table_context_rules (dict): Rules for table-specific processing\n",
    "        \n",
    "    Returns:\n",
    "        function: Configured SQL processor function\n",
    "    \"\"\"\n",
    "    def process_database_with_context(strategy: str = 'comprehensive') -> tuple[List[Document], dict]:\n",
    "        \"\"\"\n",
    "        Process database with advanced context preservation strategies.\n",
    "        \n",
    "        Args:\n",
    "            strategy (str): Processing strategy - 'comprehensive', 'focused', 'relationships'\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (documents, processing_metrics)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        processing_metrics = {\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'database_path': db_path,\n",
    "            'strategy': strategy,\n",
    "            'success': False,\n",
    "            'error_message': None,\n",
    "            'documents_created': 0,\n",
    "            'tables_processed': 0,\n",
    "            'processing_time_seconds': 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "            documents = []\n",
    "            \n",
    "            # Get table information for processing\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = [row[0] for row in cursor.fetchall()]\n",
    "            processing_metrics['tables_processed'] = len(tables)\n",
    "            \n",
    "            if strategy == 'comprehensive':\n",
    "                # Create comprehensive documents with full context\n",
    "                for table_name in tables:\n",
    "                    # Get column information\n",
    "                    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "                    columns = cursor.fetchall()\n",
    "                    column_names = [col[1] for col in columns]\n",
    "                    \n",
    "                    # Get all data\n",
    "                    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "                    rows = cursor.fetchall()\n",
    "                    \n",
    "                    # Create rich content with business context\n",
    "                    content = f\"\"\"Database Table Analysis: {table_name.upper()}\n",
    "\n",
    "Table Overview:\n",
    "â€¢ Table Name: {table_name}\n",
    "â€¢ Total Records: {len(rows)}\n",
    "â€¢ Columns: {', '.join(column_names)}\n",
    "\n",
    "Business Context:\n",
    "This table contains {len(rows)} records representing \"\"\"\n",
    "                    \n",
    "                    # Add table-specific business context\n",
    "                    if table_name == 'employees':\n",
    "                        content += \"employee information including personal details, roles, and compensation data.\"\n",
    "                        content += \"\\n\\nKey Insights:\"\n",
    "                        if rows:\n",
    "                            departments = set([row[3] for row in rows])  # Assuming department is 4th column\n",
    "                            avg_salary = sum([row[4] for row in rows]) / len(rows)  # Assuming salary is 5th column\n",
    "                            content += f\"\\nâ€¢ Departments: {', '.join(departments)}\"\n",
    "                            content += f\"\\nâ€¢ Average Salary: ${avg_salary:,.2f}\"\n",
    "                    \n",
    "                    elif table_name == 'projects':\n",
    "                        content += \"project information including status, budgets, and leadership assignments.\"\n",
    "                        content += \"\\n\\nKey Insights:\"\n",
    "                        if rows:\n",
    "                            statuses = set([row[2] for row in rows])  # Assuming status is 3rd column\n",
    "                            total_budget = sum([row[3] for row in rows])  # Assuming budget is 4th column\n",
    "                            content += f\"\\nâ€¢ Project Statuses: {', '.join(statuses)}\"\n",
    "                            content += f\"\\nâ€¢ Total Budget: ${total_budget:,}\"\n",
    "                    \n",
    "                    # Add sample records for context\n",
    "                    content += f\"\\n\\nSample Records:\"\n",
    "                    for i, row in enumerate(rows[:3], 1):\n",
    "                        record_dict = dict(zip(column_names, row))\n",
    "                        content += f\"\\n\\nRecord {i}:\"\n",
    "                        for key, value in record_dict.items():\n",
    "                            content += f\"\\n  {key}: {value}\"\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': db_path,\n",
    "                            'table_name': table_name,\n",
    "                            'strategy': 'comprehensive',\n",
    "                            'record_count': len(rows),\n",
    "                            'column_count': len(column_names),\n",
    "                            'data_type': 'sql_comprehensive'\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "            \n",
    "            elif strategy == 'relationships':\n",
    "                # Focus on cross-table relationships and business logic\n",
    "                try:\n",
    "                    # Create relationship-focused documents\n",
    "                    cursor.execute(\"\"\"\n",
    "                        SELECT \n",
    "                            e.name as employee_name,\n",
    "                            e.role as employee_role,\n",
    "                            e.department,\n",
    "                            e.salary,\n",
    "                            COUNT(p.id) as project_count,\n",
    "                            GROUP_CONCAT(p.name || ' (' || p.status || ')') as projects,\n",
    "                            SUM(p.budget) as total_budget_managed\n",
    "                        FROM employees e\n",
    "                        LEFT JOIN projects p ON e.id = p.lead_id\n",
    "                        GROUP BY e.id, e.name, e.role, e.department, e.salary\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    employee_relationships = cursor.fetchall()\n",
    "                    \n",
    "                    for emp_rel in employee_relationships:\n",
    "                        name, role, dept, salary, proj_count, projects, total_budget = emp_rel\n",
    "                        projects_managed = projects if projects else \"No projects assigned\"\n",
    "                        \n",
    "                        content = f\"\"\"Employee Leadership Profile: {name}\n",
    "\n",
    "Professional Details:\n",
    "â€¢ Name: {name}\n",
    "â€¢ Role: {role}\n",
    "â€¢ Department: {dept}\n",
    "â€¢ Annual Salary: ${salary:,}\n",
    "\n",
    "Project Management:\n",
    "â€¢ Projects Led: {proj_count}\n",
    "â€¢ Total Budget Managed: ${total_budget or 0:,}\n",
    "â€¢ Current Projects: {projects_managed}\n",
    "\n",
    "Leadership Assessment:\n",
    "This employee {\"demonstrates significant project leadership\" if proj_count > 0 else \"is not currently leading any projects\"}.\n",
    "{\"Budget management responsibility indicates senior role.\" if total_budget and total_budget > 100000 else \"\"}\n",
    "\"\"\"\n",
    "                        \n",
    "                        doc = Document(\n",
    "                            page_content=content,\n",
    "                            metadata={\n",
    "                                'source': db_path,\n",
    "                                'employee_name': name,\n",
    "                                'employee_role': role,\n",
    "                                'project_count': proj_count,\n",
    "                                'total_budget': total_budget or 0,\n",
    "                                'strategy': 'relationships',\n",
    "                                'data_type': 'sql_employee_profile'\n",
    "                            }\n",
    "                        )\n",
    "                        documents.append(doc)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Warning: Relationship processing failed - {e}\")\n",
    "            \n",
    "            # Update metrics\n",
    "            processing_metrics['success'] = True\n",
    "            processing_metrics['documents_created'] = len(documents)\n",
    "            processing_metrics['processing_time_seconds'] = time.time() - start_time\n",
    "            \n",
    "            conn.close()\n",
    "            return documents, processing_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_metrics['error_message'] = str(e)\n",
    "            processing_metrics['processing_time_seconds'] = time.time() - start_time\n",
    "            return [], processing_metrics\n",
    "    \n",
    "    return process_database_with_context\n",
    "\n",
    "print(\"ðŸ”¬ Advanced Database Processing Techniques\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Test advanced processing strategies\n",
    "strategies = ['comprehensive', 'relationships']\n",
    "advanced_results = {}\n",
    "\n",
    "try:\n",
    "    # Create advanced processor\n",
    "    advanced_processor = create_contextual_sql_processor(\"data/databases/company.db\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        docs, metrics = advanced_processor(strategy)\n",
    "        advanced_results[strategy] = {'docs': docs, 'metrics': metrics}\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Strategy: {strategy.upper()}\")\n",
    "        print(f\"  â€¢ Success: {metrics['success']}\")\n",
    "        print(f\"  â€¢ Documents: {metrics['documents_created']}\")\n",
    "        print(f\"  â€¢ Processing time: {metrics['processing_time_seconds']:.3f}s\")\n",
    "        print(f\"  â€¢ Tables processed: {metrics['tables_processed']}\")\n",
    "        \n",
    "        if docs:\n",
    "            avg_length = sum(len(doc.page_content) for doc in docs) / len(docs)\n",
    "            print(f\"  â€¢ Avg document length: {avg_length:.0f} chars\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Advanced Strategy Benefits:\")\n",
    "    print(f\"  ðŸ” Comprehensive: Rich context with business insights\")\n",
    "    print(f\"  ðŸ”— Relationships: Cross-table connections and leadership analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in advanced processing: {e}\")\n",
    "\n",
    "# Database validation utilities\n",
    "def validate_database_structure(db_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Validate database structure and provide recommendations.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Validation results and recommendations\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'valid': False,\n",
    "        'tables': [],\n",
    "        'relationships': [],\n",
    "        'recommendations': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check table existence\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        validation_results['tables'] = tables\n",
    "        \n",
    "        if len(tables) == 0:\n",
    "            validation_results['warnings'].append(\"No tables found in database\")\n",
    "            return validation_results\n",
    "        \n",
    "        # Check for relationships\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA foreign_key_list({table});\")\n",
    "            fkeys = cursor.fetchall()\n",
    "            if fkeys:\n",
    "                for fkey in fkeys:\n",
    "                    validation_results['relationships'].append({\n",
    "                        'from_table': table,\n",
    "                        'from_column': fkey[3],\n",
    "                        'to_table': fkey[2],\n",
    "                        'to_column': fkey[4]\n",
    "                    })\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if len(validation_results['relationships']) > 0:\n",
    "            validation_results['recommendations'].append(\"Use relationship-aware processing for better context\")\n",
    "        else:\n",
    "            validation_results['recommendations'].append(\"Consider adding foreign keys for better data relationships\")\n",
    "        \n",
    "        if len(tables) > 5:\n",
    "            validation_results['recommendations'].append(\"Use chunking strategies for large databases\")\n",
    "        \n",
    "        validation_results['valid'] = True\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_results['warnings'].append(f\"Database validation error: {e}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "print(f\"\\nðŸ” Database Structure Validation:\")\n",
    "try:\n",
    "    validation = validate_database_structure(\"data/databases/company.db\")\n",
    "    \n",
    "    print(f\"  âœ… Valid database: {validation['valid']}\")\n",
    "    print(f\"  ðŸ“Š Tables found: {len(validation['tables'])}\")\n",
    "    print(f\"  ðŸ”— Relationships found: {len(validation['relationships'])}\")\n",
    "    \n",
    "    if validation['recommendations']:\n",
    "        print(f\"  ðŸ’¡ Recommendations:\")\n",
    "        for rec in validation['recommendations']:\n",
    "            print(f\"    â€¢ {rec}\")\n",
    "    \n",
    "    if validation['warnings']:\n",
    "        print(f\"  âš ï¸  Warnings:\")\n",
    "        for warn in validation['warnings']:\n",
    "            print(f\"    â€¢ {warn}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in database validation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd65ccd",
   "metadata": {},
   "source": [
    "## Best Practices for Database Processing\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "1. **SQLDatabase Utility**\n",
    "   - âœ… Quick database inspection and schema analysis\n",
    "   - âœ… Integration with LangChain SQL agents and tools\n",
    "   - âœ… Built-in safety features for query execution\n",
    "   - âŒ Limited control over document structure\n",
    "\n",
    "2. **Custom SQL Processing**\n",
    "   - âœ… Production RAG systems requiring rich context\n",
    "   - âœ… Complex databases with multiple relationships\n",
    "   - âœ… Need for business-specific document formatting\n",
    "   - âœ… Advanced metadata requirements for filtering\n",
    "\n",
    "3. **SQLDatabaseLoader**\n",
    "   - âœ… Simple query-to-document conversion\n",
    "   - âœ… Integration with LangChain document pipelines\n",
    "   - âœ… Specific data extraction requirements\n",
    "   - âŒ Less flexibility in document structure\n",
    "\n",
    "### Common Challenges and Solutions\n",
    "\n",
    "1. **Large Database Performance**\n",
    "   - **Problem**: Memory issues with large tables and result sets\n",
    "   - **Solution**: Implement pagination and batch processing\n",
    "   - **Code Pattern**: Use LIMIT and OFFSET in queries for chunking\n",
    "\n",
    "2. **Complex Relationships**\n",
    "   - **Problem**: Maintaining foreign key relationships in documents\n",
    "   - **Solution**: Use JOIN queries and relationship-aware processing\n",
    "   - **Code Pattern**: Create documents that preserve business context\n",
    "\n",
    "3. **Schema Changes**\n",
    "   - **Problem**: Database schema evolution breaks processing\n",
    "   - **Solution**: Implement dynamic schema inspection and validation\n",
    "   - **Code Pattern**: Use PRAGMA queries to inspect table structure\n",
    "\n",
    "4. **SQL Injection Security**\n",
    "   - **Problem**: Dynamic SQL construction poses security risks\n",
    "   - **Solution**: Use parameterized queries and input validation\n",
    "   - **Code Pattern**: Always use cursor.execute() with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b055b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ­ Production Database Processing Utilities\n",
      "===========================================\n",
      "âš ï¸  Run database processing methods first to see performance metrics\n",
      "\n",
      "ðŸ“¦ Batch Processing Example:\n",
      "âœ… Batch processing completed\n",
      "  â€¢ Batches created: 2\n",
      "  â€¢ Records per batch: 2 (demo size)\n",
      "  â€¢ Example batch document length: 362 chars\n",
      "\n",
      "ðŸ”’ Secure Query Processing Example:\n",
      "âœ… Secure query processing completed\n",
      "  â€¢ Documents created: 2\n",
      "  â€¢ Query used parameterized placeholders\n",
      "  â€¢ SQL injection protection enabled\n",
      "\n",
      "ðŸŽ¯ Production Best Practices:\n",
      "  â€¢ Use batch processing for large tables (>10,000 records)\n",
      "  â€¢ Always use parameterized queries for security\n",
      "  â€¢ Implement connection pooling for high-volume applications\n",
      "  â€¢ Monitor memory usage and processing time\n",
      "  â€¢ Cache frequently accessed query results\n",
      "  â€¢ Validate database schema before processing\n"
     ]
    }
   ],
   "source": [
    "# Production Database Processing Utilities\n",
    "# ========================================\n",
    "\"\"\"\n",
    "This section provides production-ready utilities and performance analysis\n",
    "for database processing in RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_database_processing_performance() -> dict:\n",
    "    \"\"\"\n",
    "    Analyze memory usage and processing characteristics of different database approaches.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics for different processing methods\n",
    "    \"\"\"\n",
    "    performance_metrics = {}\n",
    "    \n",
    "    # Analyze custom processing documents if loaded\n",
    "    if 'custom_sql_docs' in locals() and custom_sql_docs:\n",
    "        performance_metrics['custom_sql'] = {\n",
    "            'method': 'Custom SQL Processing',\n",
    "            'document_count': len(custom_sql_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in custom_sql_docs) / len(custom_sql_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in custom_sql_docs),\n",
    "            'metadata_richness': len(custom_sql_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze query-based documents if loaded\n",
    "    if 'query_docs' in locals() and query_docs:\n",
    "        performance_metrics['query_based'] = {\n",
    "            'method': 'SQLDatabaseLoader Query',\n",
    "            'document_count': len(query_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in query_docs) / len(query_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in query_docs),\n",
    "            'metadata_richness': len(query_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    # Analyze relationship documents if loaded\n",
    "    if 'relationship_docs' in locals() and relationship_docs:\n",
    "        performance_metrics['relationship'] = {\n",
    "            'method': 'Relationship Extraction',\n",
    "            'document_count': len(relationship_docs),\n",
    "            'avg_doc_length': sum(len(doc.page_content) for doc in relationship_docs) / len(relationship_docs),\n",
    "            'total_memory': sum(sys.getsizeof(doc.page_content) for doc in relationship_docs),\n",
    "            'metadata_richness': len(relationship_docs[0].metadata)\n",
    "        }\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "def create_batch_sql_processor(batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Create a batch processor for large databases to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Number of records to process at once\n",
    "        \n",
    "    Returns:\n",
    "        function: Batch processing function\n",
    "    \"\"\"\n",
    "    def process_table_in_batches(db_path: str, table_name: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process a database table in batches for memory efficiency.\n",
    "        \n",
    "        Args:\n",
    "            db_path (str): Path to database file\n",
    "            table_name (str): Name of table to process\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: Documents created from batched processing\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        documents = []\n",
    "        \n",
    "        # Get total record count\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        total_records = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get column information\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        column_names = [col[1] for col in columns]\n",
    "        \n",
    "        # Process in batches\n",
    "        for offset in range(0, total_records, batch_size):\n",
    "            cursor.execute(f\"SELECT * FROM {table_name} LIMIT {batch_size} OFFSET {offset}\")\n",
    "            batch_rows = cursor.fetchall()\n",
    "            \n",
    "            # Create batch document\n",
    "            batch_content = f\"\"\"Database Table Batch: {table_name} (Records {offset+1}-{min(offset+batch_size, total_records)})\n",
    "\n",
    "Batch Information:\n",
    "â€¢ Table: {table_name}\n",
    "â€¢ Records in batch: {len(batch_rows)}\n",
    "â€¢ Batch offset: {offset}\n",
    "â€¢ Total records: {total_records}\n",
    "\n",
    "Records:\"\"\"\n",
    "            \n",
    "            for i, row in enumerate(batch_rows):\n",
    "                record_dict = dict(zip(column_names, row))\n",
    "                batch_content += f\"\\n\\nRecord {offset + i + 1}:\"\n",
    "                for key, value in record_dict.items():\n",
    "                    batch_content += f\"\\n  {key}: {value}\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=batch_content,\n",
    "                metadata={\n",
    "                    'source': db_path,\n",
    "                    'table_name': table_name,\n",
    "                    'batch_number': offset // batch_size + 1,\n",
    "                    'batch_size': len(batch_rows),\n",
    "                    'batch_offset': offset,\n",
    "                    'total_records': total_records,\n",
    "                    'data_type': 'sql_batch'\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        conn.close()\n",
    "        return documents\n",
    "    \n",
    "    return process_table_in_batches\n",
    "\n",
    "def create_secure_sql_processor():\n",
    "    \"\"\"\n",
    "    Create a secure SQL processor with parameterized queries and validation.\n",
    "    \n",
    "    Returns:\n",
    "        function: Secure processing function\n",
    "    \"\"\"\n",
    "    def process_with_security(db_path: str, query_template: str, parameters: tuple = ()) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Execute parameterized queries safely with validation.\n",
    "        \n",
    "        Args:\n",
    "            db_path (str): Path to database file\n",
    "            query_template (str): SQL query with placeholders (?)\n",
    "            parameters (tuple): Parameters for query placeholders\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: Documents from secure query execution\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            # Execute parameterized query to prevent SQL injection\n",
    "            cursor.execute(query_template, parameters)\n",
    "            results = cursor.fetchall()\n",
    "            \n",
    "            # Get column names\n",
    "            column_names = [description[0] for description in cursor.description]\n",
    "            \n",
    "            # Create documents from results\n",
    "            for i, row in enumerate(results):\n",
    "                record_dict = dict(zip(column_names, row))\n",
    "                \n",
    "                content = f\"Secure Query Result {i+1}:\\n\"\n",
    "                for key, value in record_dict.items():\n",
    "                    content += f\"{key}: {value}\\n\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        'source': db_path,\n",
    "                        'query_hash': hash(query_template),\n",
    "                        'result_index': i,\n",
    "                        'data_type': 'secure_query_result',\n",
    "                        'column_names': column_names\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Secure query execution failed: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    return process_with_security\n",
    "\n",
    "print(\"ðŸ­ Production Database Processing Utilities\")\n",
    "print(\"=\" * 43)\n",
    "\n",
    "# Run performance analysis\n",
    "try:\n",
    "    metrics = analyze_database_processing_performance()\n",
    "    \n",
    "    if metrics:\n",
    "        print(\"\\nðŸ“Š Database Processing Performance Analysis:\")\n",
    "        for method_key, data in metrics.items():\n",
    "            print(f\"\\n{data['method']}:\")\n",
    "            print(f\"  â€¢ Documents: {data['document_count']}\")\n",
    "            print(f\"  â€¢ Avg length: {data['avg_doc_length']:.1f} chars\")\n",
    "            print(f\"  â€¢ Memory: {data['total_memory']} bytes\")\n",
    "            print(f\"  â€¢ Metadata fields: {data['metadata_richness']}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¡ Performance Insights:\")\n",
    "        print(f\"  â€¢ Custom processing creates richer, more contextual documents\")\n",
    "        print(f\"  â€¢ Relationship extraction preserves business logic\")\n",
    "        print(f\"  â€¢ Query-based approach offers precise data extraction\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  Run database processing methods first to see performance metrics\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in performance analysis: {e}\")\n",
    "\n",
    "# Test batch processing\n",
    "print(f\"\\nðŸ“¦ Batch Processing Example:\")\n",
    "try:\n",
    "    batch_processor = create_batch_sql_processor(batch_size=2)  # Small batch for demo\n",
    "    batch_docs = batch_processor(\"data/databases/company.db\", \"employees\")\n",
    "    \n",
    "    print(f\"âœ… Batch processing completed\")\n",
    "    print(f\"  â€¢ Batches created: {len(batch_docs)}\")\n",
    "    print(f\"  â€¢ Records per batch: 2 (demo size)\")\n",
    "    \n",
    "    if batch_docs:\n",
    "        print(f\"  â€¢ Example batch document length: {len(batch_docs[0].page_content)} chars\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in batch processing: {e}\")\n",
    "\n",
    "# Test secure processing\n",
    "print(f\"\\nðŸ”’ Secure Query Processing Example:\")\n",
    "try:\n",
    "    secure_processor = create_secure_sql_processor()\n",
    "    secure_docs = secure_processor(\n",
    "        \"data/databases/company.db\",\n",
    "        \"SELECT name, role FROM employees WHERE department = ?\",\n",
    "        (\"Engineering\",)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Secure query processing completed\")\n",
    "    print(f\"  â€¢ Documents created: {len(secure_docs)}\")\n",
    "    print(f\"  â€¢ Query used parameterized placeholders\")\n",
    "    print(f\"  â€¢ SQL injection protection enabled\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in secure processing: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Production Best Practices:\")\n",
    "print(f\"  â€¢ Use batch processing for large tables (>10,000 records)\")\n",
    "print(f\"  â€¢ Always use parameterized queries for security\")\n",
    "print(f\"  â€¢ Implement connection pooling for high-volume applications\")\n",
    "print(f\"  â€¢ Monitor memory usage and processing time\")\n",
    "print(f\"  â€¢ Cache frequently accessed query results\")\n",
    "print(f\"  â€¢ Validate database schema before processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366fb14",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this notebook, we explored comprehensive strategies for processing database data:\n",
    "\n",
    "1. **Database Setup**: Created realistic SQLite databases with business relationships\n",
    "2. **SQLDatabase Utility**: Schema inspection and safe query execution\n",
    "3. **Custom SQL Processing**: Intelligent database-to-document conversion with rich context\n",
    "4. **SQLDatabaseLoader**: Direct query-to-document conversion with LangChain integration\n",
    "5. **Advanced Techniques**: Relationship extraction, batch processing, and security measures\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Choose the right approach**: Simple loaders for basic extraction, custom processing for production\n",
    "- **Relationships matter**: Preserve foreign key relationships and business context\n",
    "- **Security is crucial**: Always use parameterized queries to prevent SQL injection\n",
    "- **Performance considerations**: Implement batch processing for large databases\n",
    "- **Metadata is essential**: Rich metadata enables advanced filtering and retrieval\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "- [ ] Implement database schema validation and inspection\n",
    "- [ ] Use parameterized queries for all dynamic SQL\n",
    "- [ ] Create relationship-aware documents that preserve business context\n",
    "- [ ] Include comprehensive metadata for filtering and search\n",
    "- [ ] Implement batch processing for large tables (>10,000 records)\n",
    "- [ ] Add connection pooling for high-volume applications\n",
    "- [ ] Monitor memory usage and query performance\n",
    "- [ ] Cache frequently accessed query results\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try with your own databases**: Apply these techniques to your real database schemas\n",
    "2. **Build a database processing pipeline**: Create automated workflows for different table types\n",
    "3. **Optimize for your queries**: Test different document structures with your specific questions\n",
    "4. **Scale for production**: Implement connection pooling, monitoring, and error recovery\n",
    "5. **Add incremental updates**: Process only changed data for efficiency\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/) - Advanced Python SQL toolkit\n",
    "- [LangChain SQL Utilities](https://python.langchain.com/docs/modules/chains/popular/sqlite) - Complete SQL integration guide\n",
    "- [Database Design Best Practices](https://www.sqlstyle.guide/) - SQL style and design guidelines\n",
    "- [SQL Injection Prevention](https://owasp.org/www-community/attacks/SQL_Injection) - Security best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09612ffe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
