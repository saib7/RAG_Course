{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c41cfe0",
   "metadata": {},
   "source": [
    "### üéØ Module Overview\n",
    "This module covers everything you need to know about parsing and ingesting data for RAG systems, from basic text files to complex PDFs and databases. We'll use LangChain v0.3 and explore each technique with practical examples.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Introduction to Data Ingestion\n",
    "- Text Files (.txt)\n",
    "- PDF Documents\n",
    "- Microsoft Word Documents\n",
    "- CSV and Excel Files\n",
    "- JSON and Structured Data\n",
    "- Web Scraping\n",
    "- Databases (SQL)\n",
    "- Audio and Video Transcripts\n",
    "- Advanced Techniques\n",
    "- Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53141050",
   "metadata": {},
   "source": [
    "### Introduction To Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up Completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Ingestion and Parsing Setup\n",
    "=================================\n",
    "This module demonstrates various techniques for ingesting and parsing different types\n",
    "of documents for RAG (Retrieval-Augmented Generation) systems using LangChain v0.3.\n",
    "\n",
    "Key components:\n",
    "- Document loaders for various file formats\n",
    "- Text splitting strategies for chunking\n",
    "- Metadata handling for better retrieval\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain imports for document handling\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,  # Best general-purpose splitter\n",
    "    CharacterTextSplitter,           # Simple character-based splitting\n",
    "    TokenTextSplitter                # Token-aware splitting for LLMs\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup Completed! Ready for data ingestion demonstrations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c74298",
   "metadata": {},
   "source": [
    "### Understanding Document Structure In Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7850a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content :This is the main text content that will be embedded and searched.\n",
      "Metadata :{'source': 'example.txt', 'page': 1, 'author': 'Emil', 'date_created': '2024-01-01', 'cutom_field': 'any_value'}\n",
      "\n",
      "üìù Metadata is crucial for:\n",
      "- Filtering search results\n",
      "- Tracking document sources\n",
      "- Providing context in responses\n",
      "- Debugging and auditing\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Understanding LangChain Document Structure\n",
    "=========================================\n",
    "A Document in LangChain consists of two main components:\n",
    "1. page_content: The actual text content\n",
    "2. metadata: Additional information about the document\n",
    "\n",
    "This structure allows for rich context and filtering capabilities in RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "# Create a simple document with sample content and metadata\n",
    "doc = Document(\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",        # File source for traceability\n",
    "        \"page\": 1,                      # Page number for reference\n",
    "        \"author\": \"Emil\",               # Author information\n",
    "        \"date_created\": \"2024-01-01\",   # Creation timestamp\n",
    "        \"custom_field\": \"any_value\"     # Custom fields for specific use cases\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Document Structure\")\n",
    "print(f\"Content: {doc.page_content}\")\n",
    "print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Why metadata matters:\n",
    "print(\"\\nüìù Metadata is crucial for:\")\n",
    "print(\"- Filtering search results\")\n",
    "print(\"- Tracking document sources\")\n",
    "print(\"- Providing context in responses\")\n",
    "print(\"- Debugging and auditing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1d7802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the type of the document object to understand its class structure\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39741edb",
   "metadata": {},
   "source": [
    "### Text Files (.txt) - The Simplest Case {#2-text-files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Directory Setup for Text Files\n",
    "=============================\n",
    "Create a data directory structure for organizing sample text files.\n",
    "The exist_ok=True parameter prevents errors if the directory already exists.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825243da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample Text Files Creation\n",
    "==========================\n",
    "Create sample text files for demonstrating different loading techniques.\n",
    "These files represent common content types found in RAG systems.\n",
    "\"\"\"\n",
    "\n",
    "# Dictionary mapping file paths to their content\n",
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "# Write each sample text to its respective file\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath,\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"‚úÖ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10902f",
   "metadata": {},
   "source": [
    "### TextLoader- Read Single File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66421f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 1 document\n",
      "Content preview: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TextLoader - Single File Loading\n",
    "===============================\n",
    "TextLoader is the most basic document loader in LangChain for handling plain text files.\n",
    "It creates a Document object with the file content and basic metadata.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Loading a single text file\n",
    "# encoding=\"utf-8\" ensures proper handling of special characters\n",
    "loader=TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# Load the document - returns a list of Document objects\n",
    "documents=loader.load()\n",
    "print(f\"üìÑ Loaded {len(documents)} document\")\n",
    "print(f\"Content preview: {documents[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b796d422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/text_files/python_intro.txt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the metadata structure to understand what information is automatically added\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93f697",
   "metadata": {},
   "source": [
    "### DirectoryLoader- Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a86eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 879.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "  Source: data/text_files/machine_learning.txt\n",
      "  Length: 575 characters\n",
      "\n",
      "Document 2:\n",
      "  Source: data/text_files/python_intro.txt\n",
      "  Length: 489 characters\n",
      "\n",
      "üìä DirectoryLoader Characteristics:\n",
      "‚úÖ Advantages:\n",
      "  - Loads multiple files at once\n",
      "  - Supports glob patterns\n",
      "  - Progress tracking\n",
      "  - Recursive directory scanning\n",
      "\n",
      "‚ùå Disadvantages:\n",
      "  - All files must be same type\n",
      "  - Limited error handling per file\n",
      "  - Can be memory intensive for large directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DirectoryLoader - Batch File Loading\n",
    "===================================\n",
    "DirectoryLoader allows loading multiple files from a directory at once.\n",
    "It's efficient for processing entire directories of similar file types.\n",
    "\n",
    "Parameters:\n",
    "- path: Directory path to load from\n",
    "- glob: Pattern to match files (e.g., \"*.txt\", \"*.pdf\")\n",
    "- loader_cls: The loader class to use for each file\n",
    "- loader_kwargs: Arguments to pass to the loader\n",
    "- show_progress: Display progress bar during loading\n",
    "\"\"\"\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# Configure directory loader for text files\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"data/text_files\",                    # Directory to scan\n",
    "    glob=\"*.txt\",                         # Only load .txt files\n",
    "    loader_cls=TextLoader,                # Use TextLoader for each file\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},   # Pass encoding to TextLoader\n",
    "    show_progress=True                    # Show loading progress\n",
    ")   \n",
    "\n",
    "# Load all documents from the directory\n",
    "documents = directory_loader.load()\n",
    "\n",
    "# Display information about loaded documents\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Length: {len(doc.page_content)} characters\")\n",
    "\n",
    "# üìä Analysis of DirectoryLoader characteristics\n",
    "print(\"\\nüìä DirectoryLoader Characteristics:\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"  - Loads multiple files at once\")\n",
    "print(\"  - Supports glob patterns\")\n",
    "print(\"  - Progress tracking\")\n",
    "print(\"  - Recursive directory scanning\")\n",
    "\n",
    "print(\"\\n‚ùå Disadvantages:\")\n",
    "print(\"  - All files must be same type\")\n",
    "print(\"  - Limited error handling per file\")\n",
    "print(\"  - Can be memory intensive for large directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc00d16",
   "metadata": {},
   "source": [
    "### Text Splitting Statergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text Splitting Imports\n",
    "=====================\n",
    "Import various text splitters to demonstrate different chunking strategies:\n",
    "- TextSplitter: Base class for all text splitters\n",
    "- RecursiveCharacterTextSplitter: Intelligent splitting with fallback separators\n",
    "- TokenTextSplitter: Token-aware splitting for LLM limits\n",
    "\"\"\"\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    TextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f28e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
      " Document(metadata={'source': 'data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the documents to see their structure\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "988bbbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n",
      "\n",
      "Applications include image recognition, speech processing, and recommendation systems\n",
      "\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character Text Splitter\n",
    "# Extract the text content from the first document for splitting experiments\n",
    "text = documents[0].page_content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c185b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Character-based Text Splitting\n",
    "==============================\n",
    "This approach splits text based on a specific character (like space).\n",
    "It's simple but may break sentences in awkward places.\n",
    "\n",
    "Parameters:\n",
    "- separator: Character to split on\n",
    "- chunk_size: Maximum characters per chunk\n",
    "- chunk_overlap: Characters to overlap between chunks\n",
    "- length_function: How to measure chunk length\n",
    "\"\"\"\n",
    "\n",
    "# Method 1: Character-based splitting\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",      # Split on spaces\n",
    "    chunk_size=200,     # Max 200 characters per chunk\n",
    "    chunk_overlap=20,   # 20-character overlap for context continuity\n",
    "    length_function=len # Use character count to measure length\n",
    ") \n",
    "\n",
    "char_chunks = char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10866cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "------------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n"
     ]
    }
   ],
   "source": [
    "# Display first two chunks to see how splitting works\n",
    "print(char_chunks[0])\n",
    "print(\"------------------\")\n",
    "print(char_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e580c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character-based splitting with newline separator\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Split on newlines instead of spaces\n",
    "    chunk_size=200,  # Max chunk size in characters\n",
    "    chunk_overlap=20,  # Overlap between chunks for context preservation\n",
    "    length_function=len  # How to measure chunk size\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c355cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "-------------\n",
      "from experience without being explicitly programmed. It focuses on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "Types of Machine Learning:\n",
      "-------------\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n"
     ]
    }
   ],
   "source": [
    "# Display the first few chunks to understand how newline splitting works\n",
    "print(char_chunks[0])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[1])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a833b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Recursive character splitting (RECOMMENDED)\n",
    "print(\"\\n2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\")\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Try these separators in order\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8d0cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "-----------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n",
      "------------------\n",
      "Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n",
      "\n",
      "Applications include image recognition, speech processing, and recommendation\n"
     ]
    }
   ],
   "source": [
    "# Compare first three chunks from recursive splitter\n",
    "print(recursive_chunks[0])\n",
    "print(\"-----------------\")\n",
    "print(recursive_chunks[1])\n",
    "print(\"------------------\")\n",
    "print(recursive_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5579a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple text example - 4 chunks:\n",
      "\n",
      "Chunk 1: 'This is sentence one and it is quite long. This is sentence two and it is also'\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "Chunk 4: 'is sentence five. This is sentence six.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demonstrating Chunk Overlap\n",
    "===========================\n",
    "This example shows how chunk overlap works by creating consecutive chunks\n",
    "and displaying how they share common text for context preservation.\n",
    "\"\"\"\n",
    "\n",
    "# Create text without natural break points\n",
    "simple_text = \"This is sentence one and it is quite long. This is sentence two and it is also quite long. This is sentence three which is even longer than the others. This is sentence four. This is sentence five. This is sentence six.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Only split on spaces\n",
    "    chunk_size=80,     # Smaller chunks to see overlap clearly\n",
    "    chunk_overlap=20,  # 20 character overlap between chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(simple_text)\n",
    "\n",
    "print(f\"\\nSimple text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "# Display consecutive chunks to show overlap\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"Chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"Chunk {i+2}: '{chunks[i+1]}'\")\n",
    "    print()  # Empty line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f05fac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ TOKEN TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Token-based splitting\n",
    "print(\"\\n3Ô∏è‚É£ TOKEN TEXT SPLITTER\")\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # Size in tokens (not characters)\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"Created {len(token_chunks)} chunks\")\n",
    "print(f\"First chunk: {token_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ca98914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Text Splitting Methods Comparison:\n",
      "\n",
      "CharacterTextSplitter:\n",
      "  ‚úÖ Simple and predictable\n",
      "  ‚úÖ Good for structured text\n",
      "  ‚ùå May break mid-sentence\n",
      "  Use when: Text has clear delimiters\n",
      "\n",
      "RecursiveCharacterTextSplitter:\n",
      "  ‚úÖ Respects text structure\n",
      "  ‚úÖ Tries multiple separators\n",
      "  ‚úÖ Best general-purpose splitter\n",
      "  ‚ùå Slightly more complex\n",
      "  Use when: Default choice for most texts\n",
      "\n",
      "TokenTextSplitter:\n",
      "  ‚úÖ Respects model token limits\n",
      "  ‚úÖ More accurate for embeddings\n",
      "  ‚ùå Slower than character-based\n",
      "  Use when: Working with token-limited models\n"
     ]
    }
   ],
   "source": [
    "# üìä Comparison\n",
    "print(\"\\nüìä Text Splitting Methods Comparison:\")\n",
    "print(\"\\nCharacterTextSplitter:\")\n",
    "print(\"  ‚úÖ Simple and predictable\")\n",
    "print(\"  ‚úÖ Good for structured text\")\n",
    "print(\"  ‚ùå May break mid-sentence\")\n",
    "print(\"  Use when: Text has clear delimiters\")\n",
    "\n",
    "print(\"\\nRecursiveCharacterTextSplitter:\")\n",
    "print(\"  ‚úÖ Respects text structure\")\n",
    "print(\"  ‚úÖ Tries multiple separators\")\n",
    "print(\"  ‚úÖ Best general-purpose splitter\")\n",
    "print(\"  ‚ùå Slightly more complex\")\n",
    "print(\"  Use when: Default choice for most texts\")\n",
    "\n",
    "print(\"\\nTokenTextSplitter:\")\n",
    "print(\"  ‚úÖ Respects model token limits\")\n",
    "print(\"  ‚úÖ More accurate for embeddings\")\n",
    "print(\"  ‚ùå Slower than character-based\")\n",
    "print(\"  Use when: Working with token-limited models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text Processing Completed! üéâ\n",
    "=============================\n",
    "In this section, we've covered:\n",
    "- Basic LangChain Document structure\n",
    "- Single file loading with TextLoader\n",
    "- Batch loading with DirectoryLoader\n",
    "- Different text splitting strategies\n",
    "\n",
    "Next Steps:\n",
    "- PDF document processing\n",
    "- Word document handling\n",
    "- CSV and Excel files\n",
    "- Web scraping techniques\n",
    "- Database integration\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Text file processing demonstration completed!\")\n",
    "print(\"üìö Ready to move on to more complex document types...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
