{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb17e49b",
   "metadata": {},
   "source": [
    "### Multimodal RAG (PDF With Images)\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9d492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini model initialized for reranking\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google embeddings model for vector representation\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Google's Gemini model for response generation\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",    # Latest fast Gemini model for quick reranking\n",
    "    temperature=0,               # Deterministic output for consistency in ranking\n",
    "    max_tokens=None,            # Use model default token limit\n",
    "    timeout=None,               # No timeout limit for ranking requests\n",
    "    max_retries=2,              # Retry failed requests twice\n",
    ")\n",
    "\n",
    "print(\"✓ Gemini model initialized for reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea34e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model\n",
    "clip_model, clip_processor = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d56830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probabilities: [[0.00112484 0.99612325 0.00275185]]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess an image\n",
    "image_tensor = clip_processor(Image.open(\"dog.jpg\"))\n",
    "image = image_tensor.unsqueeze(0).to(device)  # type: ignore\n",
    "# Prepare text descriptions\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = clip_model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee34bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1ef9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding functions\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):  # If path\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:  # If PIL Image\n",
    "        image = image_data\n",
    "    \n",
    "    # Use the original CLIP preprocessing\n",
    "    image_input_tensor = clip_processor(image)\n",
    "    image_input = image_input_tensor.unsqueeze(0).to(device) # type: ignore\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.encode_image(image_input)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().cpu().numpy()\n",
    "    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    # Use CLIP's tokenizer instead of processor\n",
    "    text_input = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.encode_text(text_input)\n",
    "        # Normalize embeddings\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4fb1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"multimodal_sample.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38915ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d39c551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.'),\n",
       " Document(metadata={'page': 0, 'type': 'text'}, page_content='Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bf63da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00765172,  0.01199926, -0.03564397, ..., -0.10601135,\n",
       "         0.00811986,  0.01698673],\n",
       "       [ 0.01279858,  0.02477532,  0.00411514, ..., -0.08656219,\n",
       "         0.00216391,  0.03036126],\n",
       "       [ 0.01732335, -0.01327692, -0.02427032, ...,  0.08994051,\n",
       "        -0.00272154,  0.0325304 ]], shape=(3, 512), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create unified FAISS vector store with CLIP embeddings\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "012e81f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.'),\n",
       "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')],\n",
       " array([[-0.00765172,  0.01199926, -0.03564397, ..., -0.10601135,\n",
       "          0.00811986,  0.01698673],\n",
       "        [ 0.01279858,  0.02477532,  0.00411514, ..., -0.08656219,\n",
       "          0.00216391,  0.03036126],\n",
       "        [ 0.01732335, -0.01327692, -0.02427032, ...,  0.08994051,\n",
       "         -0.00272154,  0.0325304 ]], shape=(3, 512), dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_docs,embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ed2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x774d2c328e30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings # pyright: ignore[reportArgumentType]\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53cc97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query, k=5):\n",
    "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
    "    # Embed query using CLIP\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Search in unified vector store\n",
    "    results = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "492fc548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
    "    content = []\n",
    "    \n",
    "    # Add the query\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
    "    })\n",
    "    \n",
    "    # Separate text and image documents\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "    \n",
    "    # Add text context\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "        })\n",
    "    \n",
    "    # Add images\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add instruction\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c975ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8330c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What does the chart on page 1 show about revenue trends?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 3 documents:\n",
      "  - Text from page 0: Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\n",
      "Q1 du...\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The chart on page 0 shows that revenue grew steadily with the highest growth recorded in Q3.\n",
      "======================================================================\n",
      "\n",
      "Query: Summarize the main findings from the document\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 3 documents:\n",
      "  - Text from page 0: Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\n",
      "Q1 du...\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The document summarizes revenue trends across Q1, Q2, and Q3. Revenue grew steadily, with the highest growth recorded in Q3. Q1 showed a moderate increase due to new product lines, Q2 outperformed Q1 due to marketing campaigns, and Q3 had exponential growth due to global expansion.\n",
      "======================================================================\n",
      "\n",
      "Query: What visual elements are present in the document?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 3 documents:\n",
      "  - Text from page 0: Q1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\n",
      "Q1 du...\n",
      "  - Text from page 0: Annual Revenue Overview\n",
      "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The document contains a bar chart with three bars. The first bar is blue, the second is green, and the third is red. The bars represent revenue trends across Q1, Q2, and Q3, with the height of each bar corresponding to the revenue for that quarter.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"What does the chart on page 1 show about revenue trends?\",\n",
    "        \"Summarize the main findings from the document\",\n",
    "        \"What visual elements are present in the document?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f15b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
