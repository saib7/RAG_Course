{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb76040",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "- SemanticChunker is a document splitter that uses embedding similarity between sentences to decide chunk boundaries.\n",
    "\n",
    "- It ensures that each chunk is semantically coherent and not cut off mid-thought like traditional character/token splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for semantic chunking\n",
    "from sentence_transformers import SentenceTransformer  # For generating sentence embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # For calculating similarity between embeddings\n",
    "import numpy as np  # For numerical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93135aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Semantic Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone. You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk 2:\n",
      "The Eiffel Tower is located in Paris. France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Google Gemini embeddings model\n",
    "# The gemini-embedding-001 model produces 3072-dimensional vectors\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Sample text for demonstration - contains related and unrelated sentences\n",
    "text=\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Split text into individual sentences\n",
    "sentences=[s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "# Step 2: Generate embeddings for each sentence\n",
    "vectors =embeddings.embed_documents(sentences)\n",
    "\n",
    "# Step 3: Initialize chunking parameters\n",
    "threshold = 0.7  # Similarity threshold - higher means stricter grouping\n",
    "chunks = []  # List to store final chunks\n",
    "current_chunk=[sentences[0]]  # Start with first sentence\n",
    "\n",
    "# Step 4: Group sentences based on semantic similarity\n",
    "for i in range(1, len(sentences)):\n",
    "    # Calculate cosine similarity between consecutive sentences\n",
    "    sim = cosine_similarity(\n",
    "        [vectors[i - 1]],\n",
    "        [vectors[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    # If similarity is above threshold, add to current chunk\n",
    "    if sim>=threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        # Otherwise, finalize current chunk and start new one\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "\n",
    "# Don't forget the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“Œ Semantic Chunks:\")\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {idx+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Semantic Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "Chunk 2:\n",
      "You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk 3:\n",
      "The Eiffel Tower is located in Paris.\n",
      "\n",
      "Chunk 4:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# Alternative implementation using SentenceTransformer directly\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight, fast model\n",
    "\n",
    "# Same sample text for comparison\n",
    "text=\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Split into sentences\n",
    "sentences=[s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "# Step 2: Generate embeddings using SentenceTransformer\n",
    "embeddings=model.encode(sentences)\n",
    "\n",
    "# Step 3: Initialize chunking parameters\n",
    "threshold = 0.7  # Same threshold for comparison\n",
    "chunks = []\n",
    "current_chunk=[sentences[0]]\n",
    "\n",
    "# Step 4: Semantic grouping logic (same as above)\n",
    "for i in range(1, len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [embeddings[i - 1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if sim>=threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "\n",
    "# Append the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Output comparison results\n",
    "print(\"\\nðŸ“Œ Semantic Chunks:\")\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {idx+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327918a",
   "metadata": {},
   "source": [
    "### RAG Pipeline Modular Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API keys loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for RAG pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Environment setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key with error handling\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"âœ“ API keys loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdSemanticChunker:\n",
    "    \"\"\"\n",
    "    Custom semantic chunker that splits text based on embedding similarity threshold.\n",
    "    \n",
    "    This chunker uses sentence embeddings to determine semantic boundaries,\n",
    "    ensuring chunks contain semantically related content.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the SentenceTransformer model to use\n",
    "        threshold (float): Cosine similarity threshold (0-1) for grouping sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the semantic chunker.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): SentenceTransformer model name\n",
    "            threshold (float): Similarity threshold for chunking (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold \n",
    "\n",
    "    def split(self, text: str):\n",
    "        \"\"\"\n",
    "        Split text into semantic chunks based on similarity threshold.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to be chunked\n",
    "            \n",
    "        Returns:\n",
    "            list: List of semantically coherent text chunks\n",
    "        \"\"\"\n",
    "        # Split text into sentences using period as delimiter\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        \n",
    "        # Generate embeddings for all sentences at once (more efficient)\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        \n",
    "        # Initialize chunking variables\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]  # Start with first sentence\n",
    "\n",
    "        # Iterate through sentences and group by similarity\n",
    "        for i in range(1, len(sentences)):\n",
    "            # Calculate similarity between consecutive sentences\n",
    "            sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "            \n",
    "            if sim >= self.threshold:\n",
    "                # High similarity - add to current chunk\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                # Low similarity - finalize current chunk and start new one\n",
    "                chunks.append(\". \".join(current_chunk) + \".\")\n",
    "                current_chunk = [sentences[i]]\n",
    "\n",
    "        # Add the final chunk\n",
    "        chunks.append(\". \".join(current_chunk) + \".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, docs):\n",
    "        \"\"\"\n",
    "        Split multiple Document objects into semantic chunks.\n",
    "        \n",
    "        Args:\n",
    "            docs (list): List of langchain Document objects\n",
    "            \n",
    "        Returns:\n",
    "            list: List of Document objects with semantically chunked content\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        # Process each document individually\n",
    "        for doc in docs:\n",
    "            # Split document content and preserve metadata\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d031d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain is a framework for building applications with LLMs.\\nLangchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample document for testing\n",
    "sample_text = \"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# Wrap text in Document object (LangChain's standard format)\n",
    "doc = Document(page_content=sample_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris.'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the custom semantic chunker\n",
    "chunker = ThresholdSemanticChunker(threshold=0.7)  # Use 0.7 similarity threshold\n",
    "chunks = chunker.split_documents([doc])  # Split the document\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store for RAG pipeline\n",
    "import os\n",
    "\n",
    "# Initialize Google embeddings for vector storage\n",
    "embedding = GoogleGenerativeAIEmbeddings(api_key=GOOGLE_API_KEY, model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Create FAISS vector store from chunked documents\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# Create retriever interface for querying\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1799186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {question}\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define prompt template for RAG responses\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt template object\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for building applications with LLMs. It provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model for generating responses\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",    # Latest fast Gemini model\n",
    "    temperature=0,               # Deterministic output for consistency\n",
    "    max_tokens=None,            # Use model default\n",
    "    timeout=None,               # No timeout limit\n",
    "    max_retries=2,              # Retry failed requests\n",
    ")\n",
    "\n",
    "# Create RAG chain using LangChain Expression Language (LCEL)\n",
    "rag_chain = (\n",
    "    # Step 1: Create parallel map with context retrieval and question passing\n",
    "    RunnableMap(\n",
    "        {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),  # Retrieve relevant chunks\n",
    "        \"question\": lambda x: x[\"question\"],                   # Pass through question\n",
    "        }\n",
    "    )\n",
    "    | prompt        # Step 2: Format prompt with context and question\n",
    "    | llm          # Step 3: Generate response using LLM\n",
    "    | StrOutputParser()  # Step 4: Parse response to string\n",
    ")\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "query = {\"question\": \"What is LangChain used for?\"}\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684b919",
   "metadata": {},
   "source": [
    "### Semantic chunker With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain's built-in semantic chunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " chunk 1:\n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone. You can create chains, agents, memory, and retrievers. The Eiffel Tower is located in Paris.\n",
      "\n",
      " chunk 2:\n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# Load documents from text file\n",
    "loader = TextLoader(\"langchain-intro.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Initialize embedding model for LangChain's semantic chunker\n",
    "embeddings = GoogleGenerativeAIEmbeddings(api_key=GOOGLE_API_KEY, model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Create LangChain's built-in semantic chunker\n",
    "chunker = SemanticChunker(embeddings)\n",
    "\n",
    "# Split documents using the built-in chunker\n",
    "chunks = chunker.split_documents(docs)\n",
    "\n",
    "# Display results from built-in chunker\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is empty - can be used for additional experiments or notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
