{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3496acf9",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance\n",
    "MMR (Maximal Marginal Relevance) is a powerful diversity-aware retrieval technique used in information retrieval and RAG pipelines to balance relevance and novelty when selecting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9c3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a89152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API keys loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Environment Setup\n",
    "Load API credentials securely from environment variables.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Google API key with error handling\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"✓ API keys loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb5a915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='The framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.\\nLangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='BM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.\\nFAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"The 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\\nThe 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\"),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='MMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.\\nTool usage in LangChain allows agents to execute predefined Python functions with contextual input from the user.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load and chunk the document\n",
    "loader = TextLoader(\"langchain-rag-dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d741c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini model initialized for reranking\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google embeddings model for vector representation\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Google's Gemini model for response generation\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    model=\"gemini-2.0-flash\",    # Latest fast Gemini model for quick reranking\n",
    "    temperature=0,               # Deterministic output for consistency in ranking\n",
    "    max_tokens=None,            # Use model default token limit\n",
    "    timeout=None,               # No timeout limit for ranking requests\n",
    "    max_retries=2,              # Retry failed requests twice\n",
    ")\n",
    "\n",
    "print(\"✓ Gemini model initialized for reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837f2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "\n",
    "### Step 3: Create MMR Retirever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3080b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Create MMR Retirever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb7aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prompt and LLM\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95999fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG pipeline created using LCEL with pipe operators\n"
     ]
    }
   ],
   "source": [
    "# Step 6: RAG Pipeline using LCEL (LangChain Expression Language)\n",
    "\"\"\"\n",
    "LCEL (LangChain Expression Language) Implementation:\n",
    "This creates a modern, streamlined RAG pipeline using the pipe operator (|) for chaining components.\n",
    "LCEL provides better readability, error handling, and supports async operations out of the box.\n",
    "\"\"\"\n",
    "\n",
    "# Import required LCEL components\n",
    "from langchain_core.output_parsers import StrOutputParser  # Parses LLM output to clean string\n",
    "from langchain_core.runnables import RunnablePassthrough    # Passes input through unchanged\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format retrieved documents into a single context string.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of retrieved Document objects from the vector store\n",
    "        \n",
    "    Returns:\n",
    "        str: Concatenated document content separated by double newlines\n",
    "        \n",
    "    Purpose:\n",
    "        - Combines multiple document chunks into a single context string\n",
    "        - Uses double newlines to clearly separate different document chunks\n",
    "        - Makes it easier for the LLM to understand document boundaries\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create LCEL pipeline using pipe operator (|)\n",
    "\"\"\"\n",
    "LCEL Pipeline Structure:\n",
    "1. Input Processing: {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "   - retriever: Uses MMR to find diverse, relevant documents\n",
    "   - format_docs: Converts retrieved docs to formatted string\n",
    "   - RunnablePassthrough(): Preserves original query unchanged\n",
    "   \n",
    "2. Prompt Formatting: | prompt\n",
    "   - Injects context and input into the predefined prompt template\n",
    "   \n",
    "3. LLM Processing: | llm\n",
    "   - Sends formatted prompt to Google's Gemini model for response generation\n",
    "   \n",
    "4. Output Parsing: | StrOutputParser()\n",
    "   - Converts LLM response object to clean string output\n",
    "\"\"\"\n",
    "rag_chain_lcel = (\n",
    "    # Step 1: Prepare inputs - retrieve docs and pass query through\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    # Step 2: Format the prompt with context and input\n",
    "    | prompt\n",
    "    # Step 3: Generate response using the language model\n",
    "    | llm\n",
    "    # Step 4: Parse output to clean string format\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ RAG pipeline created using LCEL with pipe operators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " LangChain supports agents by allowing LLMs to act as agents that can decide which tool to call and in what order during a task. It supports memory through components like `ConversationBufferMemory` and `ConversationSummaryMemory`, which help models retain previous interactions and maintain coherence in multi-turn conversations.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Query Execution and Testing\n",
    "\"\"\"\n",
    "Test the LCEL RAG pipeline with a sample query about LangChain's capabilities.\n",
    "This demonstrates how the pipeline retrieves relevant documents using MMR,\n",
    "formats them as context, and generates a comprehensive answer.\n",
    "\"\"\"\n",
    "\n",
    "# Define test query - asking about LangChain's advanced features\n",
    "query = \"How does LangChain support agents and memory?\"\n",
    "\n",
    "# Execute the LCEL pipeline\n",
    "# The pipeline will:\n",
    "# 1. Use MMR retriever to find diverse, relevant documents\n",
    "# 2. Format retrieved docs as context string\n",
    "# 3. Combine with query in the prompt template\n",
    "# 4. Generate response using Gemini model\n",
    "# 5. Parse output to clean string\n",
    "response = rag_chain_lcel.invoke(query)\n",
    "\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c6294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain supports agents by allowing LLMs to act as agents that can decide which tool to call and in what order during a task. It supports memory through components like `ConversationBufferMemory` and `ConversationSummaryMemory`, which help models retain previous interactions and maintain coherence in multi-turn conversations.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a29a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
