{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f2c3b5",
   "metadata": {},
   "source": [
    "# Document Reranking in RAG Systems\n",
    "\n",
    "Re-ranking is a second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "\n",
    "1. **First Stage**: Use a fast retriever (like BM25, FAISS, hybrid) to fetch top-k documents quickly.\n",
    "\n",
    "2. **Second Stage**: Use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.\n",
    "\n",
    "ðŸ‘‰ **Benefits**: Ensures that the most relevant documents appear at the top, improving the final answer quality from the LLM while maintaining retrieval speed.\n",
    "\n",
    "This notebook demonstrates how to implement LLM-based reranking using Google's Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129a2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjit/Desktop/Storage01/SelfDevelopment/Rag_Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import necessary libraries for document processing, retrieval, and LLM interaction.\n",
    "\n",
    "Libraries used:\n",
    "- langchain: For document processing and LLM integration\n",
    "- langchain_google_genai: For Google's Gemini models and embeddings\n",
    "\"\"\"\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4950cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Document Loading and Preprocessing\n",
    "Load the source document and split it into manageable chunks for retrieval.\n",
    "\"\"\"\n",
    "\n",
    "# Load text file containing LangChain documentation\n",
    "loader = TextLoader(\"langchain-sample.txt\")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Split text into smaller chunks for better retrieval performance\n",
    "# chunk_size=500: Each chunk contains ~500 characters\n",
    "# chunk_overlap=50: Adjacent chunks share 50 characters to maintain context\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "print(f\"âœ“ Loaded and split document into {len(docs)} chunks\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2: Define the User Query\n",
    "This is the question we want to find relevant documents for.\n",
    "\"\"\"\n",
    "\n",
    "# Example query about LangChain application development\n",
    "query = \"How can i use langchain to build an application with memory and tools?\"\n",
    "print(f\"User Query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316ef7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API keys loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Environment Setup\n",
    "Load API credentials securely from environment variables.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Google API key with error handling\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "\n",
    "print(\"âœ“ API keys loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f83114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 4: Initialize the Reranking LLM\n",
    "Set up Google's Gemini model that will be used for document reranking.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Google's Gemini model for response generation\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    model=\"gemini-2.0-flash\",    # Latest fast Gemini model for quick reranking\n",
    "    temperature=0,               # Deterministic output for consistency in ranking\n",
    "    max_tokens=None,            # Use model default token limit\n",
    "    timeout=None,               # No timeout limit for ranking requests\n",
    "    max_retries=2,              # Retry failed requests twice\n",
    ")\n",
    "\n",
    "print(\"âœ“ Gemini model initialized for reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 5: Create Vector Store and Initial Retriever\n",
    "Set up the first-stage retriever using Google embeddings and FAISS vector store.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Google embeddings model for vector representation\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Create FAISS vector store from document chunks\n",
    "vectorstore_google = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Configure retriever to fetch top 8 documents initially\n",
    "# This gives us more candidates for the reranking stage\n",
    "retriever_google = vectorstore_google.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "print(\"âœ“ Vector store created and retriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "759738c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7d012c043560>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the configured retriever object for verification.\n",
    "\"\"\"\n",
    "retriever_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 6: Create Reranking Prompt Template\n",
    "Define the prompt that will instruct the LLM how to rank documents by relevance.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template for document reranking\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Analyze each document's content and its relevance to the user's question\n",
    "- Consider semantic similarity, topic alignment, and information completeness\n",
    "- Return a list of document indices in ranked order, starting from the most relevant\n",
    "- Only include indices that correspond to actual documents\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ Reranking prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3076a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='32f87b94-2fc7-4950-9bc3-7fa11ca1ec0b', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='59dd29ba-04da-444e-ae07-ac0f037d29af', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='1eb5b4d9-5be9-4949-9233-44764a1dba8e', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='06305acc-1ec3-4c2d-9fb8-e0078b5f5dc4', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='d4093c23-28b7-4b01-a725-12f4da5363ad', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='356a55cf-ca94-49ab-bb2e-d8a5e7d09f79', metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 7: Perform Initial Retrieval\n",
    "Use the vector store retriever to get the initial set of candidate documents.\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve top-k documents using semantic similarity search\n",
    "retrieved_docs = retriever_google.invoke(query)\n",
    "\n",
    "print(f\"âœ“ Retrieved {len(retrieved_docs)} initial documents\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b709dd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7d012c068c80>, default_metadata=())\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the reranking chain by combining prompt, LLM, and output parser.\n",
    "\"\"\"\n",
    "\n",
    "# Chain: Prompt â†’ LLM â†’ String Output Parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ“ Reranking chain created\")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94763d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 8: Format Documents for Reranking\n",
    "Prepare the retrieved documents in a numbered format for the LLM to rank.\n",
    "\"\"\"\n",
    "\n",
    "# Format documents with indices for the reranking prompt\n",
    "# Each document gets a number starting from 1\n",
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)\n",
    "\n",
    "print(\"âœ“ Documents formatted for reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82a2b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.',\n",
       " '2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.',\n",
       " '3. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.',\n",
       " '4. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.',\n",
       " '5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.',\n",
       " '6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the formatted document lines for verification.\n",
    "\"\"\"\n",
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de025bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "3. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "4. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display the complete formatted documents string that will be sent to the LLM.\n",
    "\"\"\"\n",
    "print(\"ðŸ“„ Formatted Documents for Reranking:\")\n",
    "print(\"=\" * 50)\n",
    "print(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f73baa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2, 1, 3, 5, 4, 6'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 9: Execute Reranking\n",
    "Send the query and documents to the LLM for reranking.\n",
    "\"\"\"\n",
    "\n",
    "# Invoke the reranking chain with the query and formatted documents\n",
    "response = chain.invoke({\n",
    "    \"question\": query,\n",
    "    \"documents\": formatted_docs\n",
    "})\n",
    "\n",
    "print(\"âœ“ Reranking completed\")\n",
    "print(f\"LLM Response: {response}\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31ecb3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 4, 3, 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 10: Parse Reranking Response\n",
    "Extract and validate the document indices from the LLM's response.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the comma-separated indices and convert to 0-based indexing\n",
    "# LLM returns 1-based indices, we need 0-based for Python list access\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "\n",
    "print(f\"âœ“ Parsed reranked indices: {indices}\")\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05fab1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='32f87b94-2fc7-4950-9bc3-7fa11ca1ec0b', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='59dd29ba-04da-444e-ae07-ac0f037d29af', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='1eb5b4d9-5be9-4949-9233-44764a1dba8e', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='06305acc-1ec3-4c2d-9fb8-e0078b5f5dc4', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='d4093c23-28b7-4b01-a725-12f4da5363ad', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='356a55cf-ca94-49ab-bb2e-d8a5e7d09f79', metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display original retrieved documents for comparison.\n",
    "\"\"\"\n",
    "print(\"ðŸ“‹ Original Retrieved Documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i}: {doc.page_content[:100]}...\")\n",
    "    \n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "585f5d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='59dd29ba-04da-444e-ae07-ac0f037d29af', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='32f87b94-2fc7-4950-9bc3-7fa11ca1ec0b', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='1eb5b4d9-5be9-4949-9233-44764a1dba8e', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='d4093c23-28b7-4b01-a725-12f4da5363ad', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='06305acc-1ec3-4c2d-9fb8-e0078b5f5dc4', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='356a55cf-ca94-49ab-bb2e-d8a5e7d09f79', metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 11: Apply Reranking Results\n",
    "Reorder the documents based on the LLM's ranking with validation.\n",
    "\"\"\"\n",
    "\n",
    "# Reorder documents according to the LLM's ranking\n",
    "# Only include valid indices to avoid index errors\n",
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "\n",
    "print(f\"âœ“ Successfully reranked {len(reranked_docs)} documents\")\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f857c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Reranked Results:\n",
      "\n",
      "Rank 1:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 2:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 3:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 4:\n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "\n",
      "Rank 5:\n",
      "LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "\n",
      "Rank 6:\n",
      "Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 12: Display Final Results\n",
    "Show the reranked documents in their new relevance order.\n",
    "\"\"\"\n",
    "\n",
    "# Display the final reranked results\n",
    "print(\"\\nðŸŽ¯ RERANKING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Original documents retrieved: {len(retrieved_docs)}\")\n",
    "print(f\"Successfully reranked: {len(reranked_docs)}\")\n",
    "print(\"\\nðŸ“Š Final Reranked Documents (Most to Least Relevant):\")\n",
    "\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\n{'='*10} RANK {i} {'='*10}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    if hasattr(doc, 'metadata') and doc.metadata:\n",
    "        print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SUMMARY: Document Reranking Pipeline Complete\n",
    "\n",
    "This notebook demonstrates a complete document reranking pipeline:\n",
    "\n",
    "1. âœ… Document Loading & Chunking\n",
    "2. âœ… Vector Store Creation (FAISS + Google Embeddings)\n",
    "3. âœ… Initial Retrieval (Top-K semantic search)\n",
    "4. âœ… LLM-based Reranking (Gemini model)\n",
    "5. âœ… Result Validation & Reordering\n",
    "\n",
    "Key Benefits:\n",
    "- Improved relevance ranking using LLM understanding\n",
    "- Two-stage retrieval for speed + accuracy balance\n",
    "- Robust error handling for production use\n",
    "\n",
    "Next Steps:\n",
    "- Integrate reranked documents into your RAG pipeline\n",
    "- Experiment with different reranking models (cross-encoders)\n",
    "- Add evaluation metrics to measure ranking quality\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Document Reranking Pipeline Complete!\")\n",
    "print(\"Ready to integrate into your RAG system.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rag_Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
